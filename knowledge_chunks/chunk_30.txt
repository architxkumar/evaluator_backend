normally assume. In 1966, the US government appointed Automatic Language Processing Advisory Committee (ALPAC) produced a negative report leading to a decline in research funding for machine translation. Like other ideas, it revived only with availability of increased computing power in the latter half of the century. The task that computers were successful at from the word go, was logical reasoning. In the Dartmouth Conference in 1956, two (then) relatively unknown scientists from the West Coast of the US, Herbert Simon and Alan Newell, demonstrated a working theorem prover called LT (Logic Theorist), along with J C Shaw. The Logic Theorist proved many of the theorems in Russell and Whitehead s Prinicipia Mathematica, even finding shorter and elegant proofs for some of them. An attempt to publish a new proof in the Journal of Symbolic Logic however failed, apparently because a paper coauthored by a program was not acceptable! Another system that showed promise was a geometry theorem prover built by Gelernter in 1959. However, these theorem provers were based on search, and were faced with its nemesis an exponentially growing search space. Like many AI problems, geometry theorem proving too faced a revival many years later. In 1961, James Slage wrote the first symbolic integration program, SAINT, which formed the base for many symbolic mathematics tools. Perhaps the most significant contribution of Newell and Simon was their program called GPS (General Problem Solver) that addressed general purpose problem solving, based on human thought processes. Their strategy called Means End Analysis (MEA) embodied a goal directed search strategy in which the problem solver repeatedly looks for methods (means) to achieve the most significant partial goal (ends), till all goals are solved. Their work found a home in the Carnegie Mellon University (CMU). It was first implanted in a production system language, OPS5, that was used to build expert systems. Subsequently, John Laird a