any action in the goal state keeps it in the goal state, C(Sq a,Sg) 0, the cost of staying in the goal state is 0. There is a condition that the SSP should have a proper policy. A policy is called proper, if from any state s S the policy 77 will drive it toa goal state in a finite amount of time. In other words, the system cannot loop amongst nongoal states indefinitely. Dealing with SSPs, the optimality criterion now becomes the minimization of the total expected cost, as opposed to maximization of rewards collected. The discussion which follows applies to SSPs. The reader should however keep in mind that other MDPs generalize to SSPs, as a consequence the conclusions apply to all MDPs. We are interested in choosing between competing policies. A policy specifies an action in each state. The underlying state transition system specifies the actions that are possible in each state. Consider the following example state space depicted in Figure 17.29 which is an SSP MDP. One could think of it as climbing a slippery snow slope from point 1 to point sg. There are two possible routes, one via sgand the other via Ss. However, each action is fraught with the danger of slipping, shown by two arrows (state transitions) emerging from the same point in each node. For example, attempting to go from sjto ss has the danger that one might slip and end up in sq instead, from where the only option is to go back to s, (and try again). 6 , y ee ee A gat le KS) VA 6 - fon e 1 I5S mm Sg tr vA 7 10 x. 10 Sy Ys aX ea js Ly 3 Sy 1 ; Nut 7 af S,) ars a, Figure 17.29 A stochastic planning domain. The two arrows originating from the same point on the nodes represent the two possible state transitions given an action. The four arrows with solid lines represent the desired paths. The labels on the edges are costs of actions. The action set available to the agent is A a42, 443, a44, 815, 2G, 26, 45g, 457, 441, 474, Ag3, 431 . The stochastic behaviour of the actions is described in the Table 17.13.