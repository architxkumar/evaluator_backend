 naturally allow new kinds of algorithms to exploit a common pool of knowledge. Knowledge representation is thus still an exciting challenge in artificial intelligence. In the meantime, we will continue to build upon a set of general problem solving algorithms that will find a place in the arsenal of any self respecting, intelligent system of the future. In the next chapter, we look at optimization methods that employ randomness to avoid getting stuck on local maxima. 3.10 Discussion Heuristic functions give a sense of direction to search algorithms. Given a set of choices that the search process faces, the heuristic function estimates the closeness to the desired goal function. The functions introduced in this chapter are domain dependent heuristic functions. They look at a given state in the context of the goal, and return a value that is an estimate of closeness to the goal. In the Best First Search, the OPEN list is sorted on the heuristic value, so that the best looking nodes are examined first, yielding a complete algorithm. However, the space requirements are still large. The Hill Climbing algorithm burns its bridges as it goes along, discarding the unseen nodes altogether. It has constant space requirements, but is not complete. The Hill Climbing algorithm basically exploits the gradient defined by the heuristic function, and may get stuck at nodes that look locally optimum. Variable Neighbourhood Descent attempts to increase connectivity in the search space gradually, and to provide more actions as the search moves to better values. Beam Search is similar, but always keeps a fixed number of options open. Its space requirements are constant as well, but it has better chances of reaching the goal state. Tabu search continues beyond an optimum, keeping a small, constant memory of recent moves. The memory forces the search to explore new areas, often going against the heuristic function. Finally, we observe that knowledge based methods could be used to remember