ultilayer Perceptron That Solves The use of multilayer perceptrons, then, solves our the XOR Problem knowledge representation problem. However, it introduces a serious learning problem. The convergence theorem does not extend to multilayer perceptrons. The perceptron learning algorithm can correctly adjust weights between inputs and outputs, but it cannot adjust weights between perceptrons. In Fig. 18.13, the inhibitory weight 9.0 was hand-coded, not learned. At the time Perceptrons was published, no one knew how multilayer perceptions could be made to learn. In fact, Minsky and Papert speculated: The perceptron ... has many features that attract attention: its linearity, its intriguing learning theorem ... there is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgement that the extension is sterile. Connectionist Models 385 Despite the identification of this important research problem, actual research in perceptron learning came to a halt in the 1970s. The field saw little interest until the 1980s, when several learning procedures for mnultilayer perceptrons also called multilayer networks were proposed. The next few sections are devoted to such learning procedures. 18.2.2 Backpropagation Networks As suggested by Fig. 18.8 and the Perceptrons critique, the ability to train multilayer networks is an important step in the direction of building intelligent machines from neuronlike components. Let s reflect for a moment on why this is so. Our goal is to take a relatively amorphous mass of neuronlike elements and teach it to perform useful tasks. We would like it to be fast and resistant to damage. We would like it to generalize from the inputs it sees. We would like to build these neural masses on a very large scale, and we would like them to be able to learn efficiently. Perceptrons got us part of the way there, but we saw t