 exists a causal relation between them. For example consider the statement, A new toy makes a child happy . In a logic framework one may express this as (NewToy Happy). Then if we add the sentence NewToy Modus Ponens (see Chapter 12) allows us to add Happy as well. If one wants to cater to uncertainty, one might try some variation of default reasoning. In Bayesian reasoning, one expresses this relation as a high conditional probability, for example, P(Happy NewToy) 0.8. While one can think of the conditional probability as an equivalent rule, one cannot extend the analogy with logic further. This is because logic deals with sets of (true) sentences and the application of a rule like Modus Ponens changes the set by adding new sentences to the knowledge base , Logic employs an inference engine to add new statements. Given the fact that the child got a new toy , one can add the sentence the child is happy. One cannot do something similar in Bayesian reasoning because probability theory deals with beliefs and not facts. And, in fact, the conditional probability statement already states the conclusion that, given NewToy, the probability of Happy is 0.8. Consider the following joint probability distribution. Table 17.12 A small joint probability distribution NewToy yes NewToy no Happy yes 0.4 0.3 Happy no 0.1 0.2 The joint probability distribution captures all our beliefs pertaining to the set of random variables. There is nothing new to be discovered. Inspecting the joint probability distribution, one can make the following observations, P(NewToy yes 0.4 0.1 0.5 P(Happy yes) 0.4 0.3 0.7 P(NewToy yes, Happy yes) 0.4 P(Happy yes NewToy yes) 0.4 (0.4 0.1) 0.8 As one can see that computing conditional probabilities is done by restricting the counting to that part of the table that corresponds to the given values of the given variables. However, very often we do not have the complete joint distribution. We have already observed that this table can be prohibitively large in si