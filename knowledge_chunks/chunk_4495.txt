as recurrent and time-delay networks [Waibel et al., 1989], are being employed to overcome these difficulties. In our discussion of vision in Section 21.2.1, we saw that higher-level sources of knowledge can be used to manage uncertainty at lower levels. Speech recognition also has sources of higher-level knowledge. We have already studied some of these in Chapter 15. Syntactic knowledge can be used to identify constituent 438 Artificial Intelligence {Ase a ACIa AAt NLa L Oe ENOL ANNONA REREARL phrases, semantic knowledge to disambiguate word senses, discourse knowledge to dereference pronouns, and so forth. Early speech recognition systems sought to make use of this higher-level knowledge in order to constrain the interpretation at the lower levels. As we saw in Chapter 14, a speech system that cannot decide between the cat s cares are few and the cat scares are few can invoke high-level knowledge to choose one alternative over the other. However, modern speech systems perform fairly well without any sophisticated syntactic or semantic models of language. Instead, simple statistical models are used. For example, SPHINX uses a word-pair grammar, which tells it which words can legally appear adjacent to one another in the input. TANGORA uses atrigram grammar, which, given the previous two words in the input, yields the probability that a given word will occur next. Still, no speech system is 100 percent accurate. There has recently been renewed interest in integrating speech recognition and natural language processing in order to overcome the final hurdle. For example, ATNs and unification-based grammars can be used to constrain the hypotheses made by a speech system. Thus far, integration has proved difficult, because natural language grammars do not offer much in the way of constraints. In the speech recognition literature, there is a quantitative measure of grammar, called perplexity. Perplexity measures the number of words that can legally appear next in the inpu