endous amounts of knowledge. Where does this large amount of knowledge come from? The lazy answer is machine learning. In practice, one may need to encode much of such knowledge manually. This was attempted with mixed success in the seventies by groups led by Roger Schank, Robert Wilensky, Douglas Lenat and more recently in the Worlfram Alpha system which aims to garner enough knowledge to be able to answer general questions posed on the Web, a a the Turing Test. In our journey through this book, we have seen the need for knowledge emerge. We have explored various aspects and issues of knowledge representation. And yet we are only at a beginning. We still need to devise representation schemes that will cater to different kinds of reasoning in an integrated manner. This is going to be the key to building autonomous intelligent systems. And then we have to figure out how to get the knowledge in. And we need to find ways in which machines can acquire this knowledge themselves. Machine learning has been extremely successful in learning data that fits into predefined schemas: Learning parameters and classes, associations between entities, and learning grammars of languages in text. Success stories include applications like automatic face recognition, speech recognition and speaker recognition. Such applications have made tremendous impact in the turmoil filled current day world when governments grapple with security issues. Other examples of success in machine learning come from the medical domain where statistical methods have succeeding in finding hitherto unknown relations crucial to diagnosis. Statistical methods have also been overwhelmingly adopted by the natural language processing community, for example for machine translation with the aid of aligned annotated text corpora. The next step would be for machines to be able to learn new concepts. Admittedly, the earliest attempts at learning were directed towards this goal, but the availability of increasing amount o