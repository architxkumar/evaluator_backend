real world. After these rough weights are established, the whole network is trained using real-world feedback unti} it is able to perform accurately. Articulatory Units SS -O>0- ~~ > Hidden Target Units Units Hidden Units {output) Fig. 18.24 A Recurrent Network with a Mental Model Another type of recurrent network is described in Elman [1990]. In this model, activation levels are explicitly copied from hidden units to state units. Networks of this kind have been used in a number of applications, including natural language parsing. 18.5 DISTRIBUTED REPRESENTATIONS As we have seen, the long-term knowledge of a connectionist network is stored as a set of weights on connections between units. This general scheme admits many kinds of representations, just as the basic slot-and-filler structure left room for all the representations discussed in Chapters 9 and 10. Connectionist networks can be divided roughly into two classes: those that use /ocalist representations and those that use distributed representations. NETL [Fahlman, 1979] is a highly parallel system that employs,a localist representation. Each node in a NETL network stands for one concept in a semantic network. For example, there is a node for elephant, a node for gray, etc. When the network is considering an elephant, the elephant unit becomes active. This unit Connectionist Models 401 Set nce EL SSI OR MONEE then activates neighboring units, such as units for gray, large, and mammal. The reverse process works nicely as a content-addressable memory. Distributed representations [Hinton ef al., 1986}, on the other hand, do not use individual units to represent concepts; they use patterns of activations over many units. We have already seen one example of how this works: A Hopfield network provides a distributed representation for a content-addressable memory, in which each structure is stored as a collection of active units. One might be tempted to say that digital computers also use distributed representations.