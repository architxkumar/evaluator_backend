n ofreinforcement learning wasalsothefirstsignificant learn-
ing program of any kind the checkers program written by Arthur Samuel (1959, 1967).
Samuel first used a weighted linear function for the evaluation of positions, using up to 16
termsatanyonetime. Heappliedaversionof Equation(21.12)toupdatetheweights. There
weresomesignificantdifferences,however,betweenhisprogramandcurrentmethods. First,
heupdatedtheweightsusingthedifferencebetweenthecurrentstateandthebacked-upvalue
generated byfulllook-ahead inthesearchtree. Thisworksfine,because itamountstoview-
ing the state space at a different granularity. A second difference was that the program did
notuseanyobservedrewards! Thatis,thevaluesofterminalstatesreachedinself-playwere
ignored. Thismeansthatitistheoretically possiblefor Samuel sprogramnottoconverge, or
to converge on a strategy designed to lose rather than to win. Hemanaged to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably,
this was sufficient to direct the program into areas of weight space corresponding to good
checkers play.
Gerry Tesauro s backgammon program TD-GAMMON (1992) forcefully illustrates the
potential of reinforcement learning techniques. In earlier work (Tesauro and Sejnowski,
1989), Tesauro tried learning a neural network representation of Q(s,a) directly from ex-
5 Alsoknownastwenty-oneorpontoon.
Section21.6. Applications of Reinforcement Learning 851 x
Figure21.9 Setupfortheproblemofbalancingalongpoleontopofamovingcart. The
cartcanbejerkedleftorrightbyacontrollerthatobserves x, ,x ,and .
amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious forthe expert. Itresulted inaprogram, called NEUROGAMMON,thatwas
strong by computer standards, but not competitive with human experts. The TD-GAMMON
project was an attempt to learn from self-play alone. The only reward signal was given at
the end of each game. The evaluation function was represente