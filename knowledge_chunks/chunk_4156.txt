t et al. [1967] and Newell and Simon [1972]. Chess provides a well-defined laboratory for studying the trade-off between knowledge and search. The more knowledge a program has, the less searching it needs to do. On the other hand, the deeper the search, the less knowledge is required. Human chess players use a great deal of knowledge and very little search they 4 Recal g stands for the cost so far in reaching the current node, and A stands for the heuristic estimate of the distance from the node to the goal. Game Playing 245 typically investigate only 100 branches or so in deciding a move. A computer, on the other hand, is capable of evaluating millions of branches. Its chess knowledge is usually limited to a static evaluation function. Deepsearching chess programs have been calibrated on exercise problems in the chess literature and have even discovered errors in the official human analyses of the problems. A chess player, whether human or machine, carries a numerical rating that tells how well it has performed in competition with other players. This rating lets us evaluate in an absolute sense the relative trade-offs between search and knowledge in this domain. The recent trend in chess-playing programs is clearly away from knowledge and toward faster brute force search. It turns out that deep, full-width search (with pruning) is sufficient for competing at very high levels of chess. Two examples of highly rate chess machines are HITECH [Berliner and Ebeling, 1989] and DEEP THOUGHT [Anantharaman et ai., 1990], both of which have beaten human grandmasters and both of which use custom-built parallel hardware to speed up legal move generation and heuristic evaluation. Checkers Work on computer checkers began with Samuel [1963]. Samuel s program had an interesting learning component which allowed its performance to improve with experience. Ultimately, the program was able to beat its author. We look more closely at the leaning mechanisms used by Samuel in Chapter 17. 