7 = 0.35, and run for 6000 training epochs. (Each epoch consists of forward and backward propagation of each of the four training examples.) Modify your program to use the momentum factor a@ = 0.9. Did adding momentum significantly decrease the number of training epochs required for learning? Here is a toy problem for testing generalization in networks. Suppose that there are eight political issues on which every political party must decide, and suppose further that those decisions are binary (for example, to legalize gambling or not, to increase military spending or not, etc.). We can then represent the platform of a political party as a vector of eight ones and zeros. Individuals who belong to political parties may have beliefs that differ slightly from their party s platform. Your job is to train a backpropagation network to compute the political platform of the party that most closely matches a given individual s beliefs. Generate four random 8-bit vectors to represent the platforms of four political parties. For each party, generate nine individuals who belong to that party. The beliefs of an individual, like those of a party, are represented as an 8-bit vector. One of the nine individuals should agree entirely with the party platform, and the other eight should differ on exactly one issue (1 bit). Now generate 36 input-output pairs, by juxtaposing individuals with the platforms of their respective political parties. Each input is 8 bits, and each output is 8 bits. Next, remove five of the input-output pairs. These five will make up the testing set ; the other 31 will make up the training set. Create a backpropagation network with eight input units, eight hidden units, and eight output units, Train the network on the 31 vectors in the training set until performance is very high. Now test the network on the five input-output pairs it has never seen before. How well does it perform? Experiment witb the different sizes of testing and training sets, as well as hidd