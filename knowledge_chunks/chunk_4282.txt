compute the probabilities. Since we look at only word pairs (viz. a single word previous to the one we are searching) this model is called the bigram model. If we follow the bigram model of seeking the next word using the novel used as the corpus the word that would follow Scarlet would most aptly be Pimpernel. This is substantiated by the data on words that appear after the word Scarlet depicted in Table 15.2. Table 15.2 Frequencies of words following the word Scarlet Word following Scarlet Pimpernel geranium heels waistcoat flower device enigma It can thus be assumed that when we refer to a previous word and find the probability of the next word, a more apt sentence is created. This is called a Markov assumption. Based on this, for a bigram model, the probability P(w, | (w,.w 2... w,_,)) can be approximated to the product of all P(w;,lw,_,) for i varying from J to n (n is the number of words in the sentence) i.e. 324 Artificial Intelligence Pow) = T] rowley) i=l We could extend the concept from bigrams (taking into consideration only two words viz. the current and the previous) to trigrams (viz. taking the current word and the previous two words) and further on, to tetragrams (previous four words). The approximate probability of finding the next word in case of N-grams is given by POw, hep!) = POvgl inna) wel includes the words from w, to w,,,. Similarly, wea means words w,_ 41 t O W,_1, It may now be interesting to note that the probability of the sentence He never told her and she had never cared to ask. can be found using the bigram probability model as P(He never told her and she had never cared to ask.) P(Hel<nil>).P(neverl He).P(toldinever).P(herltold).P(and|her).P(sheland).P(hadlshe). P(neverlhad) P(cared \never).P(tolcared).P(asklto). (207/67675).(3/5 12).(1/60).(1 0/3 1).(6/1137).(34/2353).(168/935).(6/1077).(1/60).(3/11). (3/2267) Note that each probability term is calculated by finding the number of occurrences of the specific bigram and dividing it by 