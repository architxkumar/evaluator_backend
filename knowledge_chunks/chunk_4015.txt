ht have a second rule that says that we prefer to believe hotel registers rather than people. Using these two rules, a problem solver would conclude that the most likely suspect is Cabot. Backward rules work exactly as we have described if all of the required facts are present when the cules are invoked. But what if we begin with the situation shown in Fig. 7.2 and conclude that Abbott is our suspect. Later, we are told that he was registered at a hotel in Albany. Backward rules will never notice that anything has changed. To make our system data-driven, we need t6 use forward rules. Figure 7.3 shows how the same knowledge could be represented as forward rules. Of course, what we probably want is a system that can exploit both. In such a system, we could use a backward rule whose goal is to find a suspect, coupled with forward rules that fire as new facts that are relevant to finding a suspect appear. 160 Artificial Intelligence MATL ANE IETS if: Beneticiary(x), UNLESS Ajlibi(x), then Suspect{x) If: Somewhere Else(x), then Alibi(x) If: Registered Hotel{x, y), and Far Away(y), UNLESS Forged Register(y), then Somewhere E/se(x) if Defends(x,y), UNLESS Lies(y), By then Alibi(x) If Picture Of(x, y), and Far Away(y), then Somewhere E/se(x) If TRUE, UNLESS Sx. Suspect(x) then Contradiction() Beneficiary(Abbott) Beneficiary Babbith Beneficiary( Cabot) Fig. 7.3. Forward Rules Using UNLESS 7.5 IMPLEMENTATION: DEPTH-FIRST SEARCH 7.5.1 Dependency-Directed Backtracking If we take a depth-first approach to nonmonotonic reasoning, then the following scenario is likely to occur often: We need to know a fact, F, which cannot be derived monotonically from what we already know, but which can be derived by making some assumption A which seems plausible. So we make assumption A, derive F, and then derive some additional facts G and H from F, We later derive some other facts M and N, but they are completely independent of A and F. A little while later, a new fact comes in that invalidate