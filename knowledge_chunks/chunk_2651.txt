iscusses how
an agent can use inductive learning to learn much faster from its experiences. Section 21.5
covers methods forlearning direct policy representations in reflexagents. Anunderstanding
of Markovdecisionprocesses (Chapter17)isessentialforthischapter.
832 Chapter 21. Reinforcement Learning
21.2 PASSIVE REINFORCEMENT LEARNING
Tokeep things simple, westart with the case of a passive learning agent using a state-based
representation in a fully observable environment. In passive learning, the agent s policy is fixed: in state s, it always executes the action (s). Its goal is simply to learn how good
the policy is that is, to learn the utility function U (s). We will use as our example the
4 3 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the
corresponding utilities. Clearly, the passive learning task is similar to the policy evaluation
task, part of the policy iteration algorithm described in Section 17.3. The main difference
is that the passive learning agent does not know the transition model P(s
(cid:2) s,a),
which
(cid:2)
specifies the probability of reaching state s from state s after doing action a; nor does it
knowtherewardfunction R(s),whichspecifiestherewardforeachstate.
3 1
3 0.812 0.868 0.918 1
2 1 2 0.762 0.660 1
1 1 0.705 0.655 0.611 0.388
1 2 3 4 1 2 3 4
(a) (b)
Figure 21.1 (a) A policy for the 4 3 world; this policy happens to be optimal with
rewardsof R(s) 0.04inthenonterminalstatesandnodiscounting. (b)Theutilitiesof
thestatesinthe4 3world,givenpolicy .
Theagentexecutesasetoftrialsintheenvironmentusingitspolicy . Ineachtrial,the
TRIAL
agent starts in state (1,1) and experiences a sequence of state transitions until it reaches one
oftheterminal states, (4,2)or(4,3). Itspercepts supply boththecurrent stateandthereward
receivedinthatstate. Typicaltrialsmightlooklikethis:
(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(4,3)
-.04 -.04 -.04 -.04 -.04 -.04 -.04 1
(1,1) (cid:2)(1