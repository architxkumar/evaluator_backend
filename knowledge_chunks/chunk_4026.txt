than one + = justification as shown in Fig. 7.11. Not only does Cabot say he was at the ski slopes, but he was _ Cabot [IN] Alibi Cabot [IN] * seen there on television, and we have no reason to believe that this was an elaborate forgery. This new valid justification of Alibi Cabot causes it to he labeled IN (which also causes Tells Truth Cabot to come IN). This change in state propagates to Suspect Cabot, which goes OUT. Now we have a problem. The justification for the contradiction is now valid and the contradiction is IN. The job of the TMS at this point is to determine how the contradiction can be made OUT again. In a TMS network, a node can be made OUT by causing all of its justifications to become invalid. Monotonic justifications cannot be made invalid without retracting explicit assertions that have been made to the network. Nonmonotonic justifications can, however, be invalidated by asserting some fact whose absence is required by the justification. We call assertions with nonmonotonic justifications assumptions. An assumption can be retracted by making IN some element of its justification s OUT-list (or recursively in some element of the OUT-list of the justification of some element in its IN-list). Unfortunately, there may be many such assumptions in a large dependency network. Fortunately, the network gives us a way to identify those that are relevant to the contradiction at hand. Dependency-directed backtracking algorithms, of the sort we described in Section 7.5.1, can use the dependency links to determine an AND/OR tree of assumptions that might be retracted and ways to retract them by justifying other beliefs. In Fig. 7.10, we see that the contradiction itself is an assumption whenever its justification is valid. We might retract it by believing there were other suspects or by finding a way to believe again that either Abbott, Babbitt, or Cabot was a suspect. Each of the last three could be believed if we disbelieved their alibis, which in turn are as