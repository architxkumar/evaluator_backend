 If the process is continued till the partitions are completely homogenous then this element would have separated towards the end, and can be identified as being wrongly labelled. Some amount of post-processing can then prune the tree of such tiny spurious classes. Different attributes will induce different partitions on a given data set. How does one choose the attribute that will separate the different classes most? The approach used in many algorithms is to use the notion of entropy, which is a measure of information content in a set (Shannon, 1948). Entropy is a measure of diversity in a set. The more homogenous the set is, the less information it has; in the sense that it needs a smaller number of bits to describe it. The more heterogeneous it is, the greater the information content or entropy. Entropy is then a measure of predictability. If one were to choose a random element from the set then zero entropy would mean that it is entirely predictable. This would happen if all the elements in the set were of the same class. With equal elements from two classes, the entropy would be one. With more classes, the entropy goes up further. Given a set S of elements from K classes, the entropy of the set is defined as, K Entropy(S) x p; log. p; (18.45) where p; is the proportion of elements of the class in the set. The algorithm tries out partitioning the set of element using different attributes. Let A be an attribute and Values(A) the set of values for the attribute. Using the attribute A to partition the set S results in information gain Gain(S, A), as defined by the formula below. ISI Gain(S, A) Entropy(S) Entro (S,) (18.46 PY PI Wy vevalues(4) The information gain is computed by subtracting the weighted sum of the entropies of the partitions from the entropy of the original set S. The weight for each partition is the proportion of the elements in that partition. One can see that the more homogenous the partitions are, the greater is the information gain. This is il