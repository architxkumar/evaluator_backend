if one has a layered network with more than two sigmoid layers then the same back propagation process can be used to train neurons in each preceding layer. For the simple network architecture of Figure 18.26, the above equation can be written as, 5, 5; 0 (1-0,) Zhse omputs Ma" Oe (18.74) The training step specified in Eq. (18.71) can now be used even for the hidden neurons. The Backpropagation algorithm adapted from (Mitchell, 1997) is given in Figure 18.29. It takes as input the number of neurons N,, Nj, and Ny, respectively in the input, hidden and output layers; the set of training examples each with a set of target outputs for the output layer; and the learning rate parameter h. Lines 1 to 6 are used to initialize the weights of connections incident on the hidden layer and the output layer. In Lines 7 to 19, each training example is considered repeatedly till convergence. In Lines 9 to 14, the weights for the output layer are adjusted. We refer to the set of these weights as W,. Line 11 is an abbreviation for the feedforward process in which the values at the input layer are propagated to the output layer. Observe that in Line 14 the value of the j hidden neuron o has been referred to as xjin Eqs. (18.70) and (18.72). Likewise, the input value x; is the input from the i neuron to the j " neuron referred to as w;. For the sake of brevity and readability, the summation from Eq. (18.74) has been left as it is in Line 16. This will expand into a loop when implementing the algorithm. Backpropagation (Training Set: T, Learning rate: n, In: N,, Hidden: Nz, Out: No ) 1 for j 1 to XN, 2 for i 0 to 3 Wig The hidden layer set W; 4 for k 1 to N, 5 for j 0 to N, Woy E The output layer set W, 7 repeat 8 for each X x,, .., x, in T 9 for k 1 to N, Adjusting output layer weights 10 t, training class label 11 , computed output of the k neuron 12 5, ( 4704) 04 (1-0) 13 for j 0 to N, 14 Wy Wy 46,40; Note: 0, xj, is for j 1 to N; Adjusting hidden layer weights 16 6, 05 (1-05) Zaseou