deal with temporal processes and how distributed representations can be made efficient. 18.2. LEARNING IN NEURAL NETWORKS 18.2.1 Perceptrons The perceptron, an invention of Rosenblatt [1962], was one of the earliest neural network models, A perceptron models a neuron by taking a weighted sum of its inputs and sending the output | if the sum is greater than 2In Fig. 18.4, state B is depicted as being lower than state A because fewer constraints are violated. A constraint is violated, for example, when two active units are connected by a negatively weighted connection. 380 Artificial Inteiligence s R CERNE NICATION Y some adjustable threshold value (otherwise it sends 0). Fig. 18.5 shows the device. Notice that in a perceptron, unlike a Hopfield network, connections are unidirectional. dendrites electrical spike Fig. 18.5 A Neuron and a Perceptron The inputs (x), x2, ...,.x,,) and connection weights (w), #2, ..., W,) in the figure are typically real values, both positive and negative. If the presence of some feature x; tends to cause the perceptron to fire, the weight w,, will be positive; if the feature x, inhibits the perceptron, the weight w; will be negative. The perceptron itself consists of the weights, the summation processor, and the adjustable threshold processor. Learning is a process of modifying the values of the weights and the threshold. It is convenient to implement the threshold as just another weight wa, as in Fig. 18.6. This weight can be thought of as the propensity of the perceptron to fire irrespective of its inputs. The perceptron of Fig. 18.6 fires if the weighted sum is greater than zero. A perceptron computes a binary function of its input. Several perceptrons can be combined to compute more complex functions, as shown in Fig. 18.7. Such a group of perceptrons can be trained on sample input-output [fh pairs until it learns to compute the correct function. The amazing property ik at, . . . . : positive/negative of perceptron learning is this: W