 of m attribute values. An attribute A1 having a small number of discrete values ajj, aj2..... is selected and a tree node structure is formed to represent Ar The node has k, branches emanating from it where each branch corresponds to one of the a1, values (Figure 19.1). The set of training objects 0 are then partitioned into at most k1 subsets based on the object's attribute values. The same procedure is then repeated recursively for each of these subsets using the other m - I attributes to form lower level nodes and branches. The process stops when all of the training objects have been subdivided into single class entities which become labeled leaf nodes of the tree. The resulting discrimination treeor decision tree can then be used to classify new unknown objects given a description consisting of its m attribute values. The unknown is classified by moving down the learned tree branch by branch in concert with the values of the object's attributes until a leaf node is reached. The leaf node is labeled with the unknown's name or other identity. 1D3 is an implementation of the basic CLS algorithm with some modifications. In the 1D3 system, a relatively small number of training examples are randomly selected from a large set of objects 0 through a window. Using these training examples, a preliminary discrimination tree is constructed. The-tree is then tested by scanning all the objects in 0 to see if there are any exceptions to the tree. A new subset or window is formed using the original examples together with some of the exceptions found during the scan. This process is repeated until no exceptions are found. The resulting discrimination tree can then be used to classify new objects. Another important difference introduced in l D3 is the way in which the attributes are ordered for use in the classification process. Attributes which discriminate best are selected for evaluation first. This requires computing an estimate of the expected information gain using all ava