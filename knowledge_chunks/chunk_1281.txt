OX, x; dnet ; ow, OE . dnet; (18.68) Let us consider the simpler case for a neuron k in the output layer. We can write, ) ) doy dnet, do, dnet, 0.1 00, F) O; 2 Lre outputs G kk o,) an et; do; t (4 0 ) dnet; do (net, ) ) Dnet k Substituting the derivative for the sigmoid (h 04) 0, (1 0g) (18.69) Substituting this expression in Eq. (18.68) and the result in Eq. (18.67) we get, Wr Wye P(t On) FOL OR) (18.70) which is the computation one has to do in the iterative steps, as the algorithm inspects each training example and adjusts each weight that is an input to an output neuron. The above equation may be abbreviated as, Wie Wy t AWp (18.71) where Aw, is the change one is making in the weight w and Awa 16 5 (18.72) where Ox (ty-0,) 0, (1-0,) can be thought of the error term which depends upon the actual and the expected output. In Eq. ( 18.70) the term (t,-0,) measures the error in the output of the neuron k whose input weights we are trying to learn. In the second case of learning, the weights associated with hidden neurons this term is not available. However, any hidden neuron j sends its output 0; to the set of neurons downstream(j) where the error shows up. If one has an error term for the downstream neurons, then one can propagate the error term back to neuron j in proportion to the connecting weights. That is, for a hidden neuron j that feeds the set of neurons downstream(j), 5, 0, (1-0)) Z jee downstreamg) Mid 5a (18.73) Recall that 0, (1 0;), the common term, is the derivative of the sigmoid function. In the layered feedforward network in our illustration, the set downstream(j) is the same as the set outputs. The idea of using the term downstream(j) is useful because it generalizes the back propagation process to architectures that are not necessarily layered. Also, if one has a layered network with more than two sigmoid layers then the same back propagation process can be used to train neurons in each preceding layer. For the simple network architecture of Figur