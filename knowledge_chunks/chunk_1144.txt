h values holding. Then given an arbitrary sentence a in the logic, we could compute the probability of the sentence being true as the sum of the probabilities of all those interpretations, in which a is necessarily true (Brachman and Levesque, 2004). P(a) Lea PD) However, note that this approach is likely to be computationally very expensive. Consider a domain in which there are two random variables X and Y with domains Dy x;, ..., x and Dy yj, ..., p . The two fundamental rules of probability theory are the sum rule and the product rule. The sum rule is, P(x,) Licicp P(x;Y;) which we can also write as, P(X) x,P. ) This formula allows us to compute the probability P(X x;,) from the joint probability distribution. The rule says that, given the joint probability distribution for X and Y, the probability distribution for X, also called the marginal probability, can be computed by adding up the corresponding values for all values of Y. The product rule allows us to compute the joint probability, given the conditional properties and the marginal properties. P(x Y) PO; x)" PO) PO; ) PO): PCY, ) PCY .) PC) POY FH P) The product rule leads us to the well known Bayes theorm. POY; x) POE ID POD) PO) P(Y .) PCY Y) P(Y) PCY) Given a set of random variables, what do the conditional probabilities represent? Given the variables X and Y, the expression IT (Y X) defines the probability distribution of X, given values of the variables of Y. For example, we could say that P(Fruit Litchi Season Summer) 0.7 to assert that if the season is summer, the probabilities of one finding litchis in the market is 0.7. One could also say that P(Fruit Litchi Season Summer, City Dehradun) 0.9, if one is in Dehradun in the summer then the probability is even higher. If one had conditional probabilities for other cities and other seasons, then one could compute P(Fruit Litchi) by using the sum rule and marginalising the other variables. Let us look at a small contrived example. In the following exampl