ET N bot Fix the bugs Figure 16.8 Bottom-up parsing. Statistical Parsing It is next to impossible to come up with a perfect grammar that accepts all English sentences that people find acceptable, and rejects those that people find unacceptable. Also, parsing a single sentence using a run-ofthe-mill parser may lead to multiple parses, of which only one makes sense to people. These two observations lead us to speculate that it is perhaps more sensible to assign to a given parse of a sentence, a number between 0 and 1 (inclusive) that indicates the probability that the given parse is reckoned as a meaningful one by people. While Charniak did propose that syntactic analyses should be qualified by probabilities, the automata theorist Taylor L. Booth was the first to suggest that rules in CFGs (context free grammars) should be assigned probabilities, giving rise to Probabilistic Context Free Grammars (PCFGs). A simple example of PCFGs is shown in Figure 16.9 below. Note that the probabilities corresponding to the rules having the same nonterminal as antecedent add up to 1. Let us use the grammar rules above to parse the following ambiguous sentence: Deepa ate noodles with chopsticks. S NP VP (1.0) NP NPPP (0.2) V ate(1.0) VP VNP (0.5) NP Deepa (0.2) PREP - with (1.0) VP VP PP (0.5) NP noodles (0.3) PP PREP NP (1.0) NP chopsticks (0.15) NP spoons (0.15) Figure 16.9 PCFG rules. The two parse trees generated by the system are shown below. Using independence assumptions, the probability of a parse tree is simply estimated as the product of probabilities of all rules occurring in it. For the two parses Parse, and Parse shown in Figure 16.10, the probabilities are estimated as shown below: P(Parse,) P(S NP VP) x P(VP V NP) x P(NP NP PP) x P(PP PREP NP) x P(NP Deepa) x P(V ate) x P(NP noodles) X P(PREP with) x P(NP chopsticks) 0.0009 P(Parse,) P(S NP VP) x P(VP VP PP) x P(VP V NP) x P(PP PREP NP) x P(NP Deepa) x P(V ate) x P(NP noodles) x P(PREP with) x P(NP chopsticks) 0.00225 