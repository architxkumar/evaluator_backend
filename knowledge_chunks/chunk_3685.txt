 ordered for use in the classification process. Attributes which discriminate best are selected for evaluation first. This requires computing an estimate of the expected information gain using all available attributes and then selecting the attribute having the largest expected gain. This attribute is assigned to the root node. The attribute having the next largest gain is assigned to the next level of nodes in the tree and so on until the leaves of the tree have been reached. An example will help to illustrate this process. For simplicity, we assume here a single-class classification problem, one where all objects either belong to class C or U-C. Let h denote the fraction of objects A1 - Figure 19.1 Anode created for attribute A, (color) with k discrete values. o, u,, (red, Orange, ....While). a Sec. 19.2 The 1D3 System 403 that belong to class C in a sample of n objects front h is an estimate of the true fraction or probability of objects in U that belong to class C. Also let = number of objects that belong to C and have value a, = number of objects not belonging to C and having value a,, = + il,,) / it be the fraction of objects with value a,, i.se assume objects have all attribute values so that Y, p,4 = I) = e,, / te,, + c/, ) the traction of objects in C with attribute '. alue a , . and = I - /,. the fraction of objects not in C with value a,. Now define H,(h) = 1; * log/z (I /i) * log( I I,) with H(0) = 0. and H, = /,, * log/,, - * logg, as the information content for class C and attribute a,, respecti'.els Fhcii the expected value (mean) of H,k is just E(HA) = * We can now define the gain G1 for attribute A, as = H - Each G, is computed and ranked. The ones having the largest values determine the order in which the corresponding attributes are selected in building the discrimina ti On tree. In the above equation, quantities computed as H = p, * log p, with .p, = are known as the information theoretic entropy wherep, is the probability of occurrence of some e