taken. Very often however, policies are cyclic. That is, one may revisit the same state again. In fact, it would be quite likely that in a stochastic domain, an agent may need to try some actions repeatedly till it succeeds , The values of such cyclic policies can be computed by setting up a system of linear equations as follows. The long term cost reaching a goal from a given state s is the cost of making the first move, plus the cost of reaching the goal from the destination of the first move. Since the action may be stochastic, the first move may end up in different states with different probabilities, and a weighted average would have to be taken. V (s) 0 ifseG Qyes Pls, a, s ) C(s, a, s ) VAs ) otherwise Let us write down these equations for the policy 775g for the problem depicted in Figure 17.29. The policy graph is depicted in Figure 17.31. Observe that paths from all states eventually lead to the goal state sg. Figure 17.31 The policy graph for the cyclic policy m5. The equations are given below. We have written the expression only for the first one. VES) Psy. ays. 3)1CUSy, 245. 85) VOMss) Pls. ars. 4) CUS1. 15, 54) VE S4) VEG(s,) 0.9 15 VO s5) 0.1 10 P s,) VEC(s,) 0.7 10 VE (sg) 0.3 6 VF s,) V 5(s;) 0.6 7 V s,) 0.4 1 V s5) VECs,) 0.6(7 VE (s,) 0.4 1 V s,) VE(ss) 0.6 5 VO(sg) 0.413 V s,) VES) 0.7 7 VE s,) 0.3 1 VE s6) VPS s.) 0.6 7 VE s4) 0.3 1 V s,) VES) 0 Solving these equations we get the values listed for V (s,), ..., V5 (s7) as, VG (45.3351, 26.9291, 53.0018, 53.0018, 28.3721, 60.4303, 60.4303 One can see that the expected costs are pretty high compared to the costs of deterministic solutions. The highest costs are for V (sg) and VG (s7), which are symmetrically placed in the state space. They have to get all the way back to s, and try going through ss again. The expected costs are lowest for sz and ss, since they are closest to the goal node and the policy picks the action taking them to the goal node. The policy 79g is the other interesting policy (t