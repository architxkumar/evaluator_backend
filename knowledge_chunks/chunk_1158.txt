 conditional dependence between the two nodes, which stand for random variables. Bayesian belief networks are directed acyclic graphs in which a directed edge captures a causal relation between two random variables. Observe that this would still allow for abductive inferences. Given a set of random variables, there could be varying degrees of conditional dependence between them. If one can identify the dependence relations then one could take recourse to probabilistic graphical models. An example of such models is Bayesian networks, also known as Bayesian Belief Networks (BBNs). Bayesian networks are directed graphs in which edges capture the causal relations between the nodes (random variables). Importantly, Bayesian networks allow us to get an insight into the conditional dependencies between random variables. Consider the randomly generated graph of seven variables X, ... X7 shown in Figure 17.27. The conditional dependencies between the variables can be determined by looking at the graph. Each variable is causally influenced by its parent. Consequently, the joint distribution of the seven variables can be written as, P(X, ...,X5) POX) PCX,) PCX; 4X, PCY, X) P(Xs -X3,) PX Xs, X4) POG X) Observe that this equation says that variables X, and Xp are independent of other variables, X3 is conditionally dependent on X, and X2 only, and so on. This is in lieu of the general expansion for conditional probabilities, P(X), ..-,X5) PX, X4..-X5) PCY, X, ....X5) ... P(X6 3) PX) Figure 17.27 A Bayesian network of seven variables. The latter is simply derived from the product rule blindly without paying any attention to which variables are causally influencing which other variables. The Bayesian network, on the other hand, clearly marks specific direct influences and is presumably constructed by exploiting domain knowledge. In general, a Bayesian belief network over N variables is characterized by its joint distribution, defined as the product of the probabilities of all its va