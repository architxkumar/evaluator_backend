ch a saddle point or even a local
minimum.) Inthis sense, EMresembles agradient-based hill-climbing algorithm, butnotice
thatithasno stepsize parameter.
700
600
500
400
300
200
100
0
-100
-200
0 5 10 15 20
Ldoohilekil-go L
-1975
-1980
-1985
-1990
-1995
-2000
-2005
-2010
-2015
-2020
-2025
0 20 40 60 80 100 120
Iteration number
Ldoohilekil-go L
Iteration number
(a) (b)
Figure20.12 Graphsshowingtheloglikelihoodofthedata, L, asafunctionofthe EM
iteration.Thehorizontallineshowstheloglikelihoodaccordingtothetruemodel.(a)Graph
for the Gaussian mixture model in Figure 20.11. (b) Graph for the Bayesian network in
Figure20.13(a).
820 Chapter 20. Learning Probabilistic Models
P(Bag 1) Bag C
Bag P(F cherry B) 1 F1 2 F2
Flavor Wrapper Hole X
(a) (b)
Figure20.13 (a)Amixturemodelforcandy. Theproportionsofdifferentflavors,wrap-
pers,presenceofholesdependonthebag,whichisnotobserved. (b)Bayesiannetworkfor
a Gaussianmixture. Themeanandcovarianceoftheobservablevariables Xdependonthe
component C.
Things do not always go as well as Figure 20.12(a) might suggest. It can happen, for
example,thatone Gaussiancomponentshrinkssothatitcoversjustasingledatapoint. Then
its variance will go to zero and its likelihood will go to infinity! Another problem is that
twocomponents can merge, acquiringidenticalmeansandvariancesandsharingtheirdata
points. These kinds of degenerate local maxima are serious problems, especially in high
dimensions. One solution is to place priors on the model parameters and to apply the MAP
version of EM. Another is to restart a component with new random parameters if it gets too
smallortooclosetoanothercomponent. Sensibleinitialization alsohelps.
20.3.2 Learning Bayesiannetworks withhidden variables
To learn a Bayesian network with hidden variables, we apply the same insights that worked
formixturesof Gaussians. Figure20.13represents asituation inwhichtherearetwobagsof
candies that have been mixed together. Candies are described by three features: in addition
to the Flavor 