ackpropagation, because we only calculate activation levels for the purpose of singling out the most highly activated output unit. 11 Ag mentioned earlier, any, method for determining the most highly activated output unit is sufficient. Simulators written in a serial programming language may dispense with the neural circuitry and simp!y compare activations levels to find the maximum. 394 Artificial Intelligence where w; is the weight on the connection from input unit j to the active output unit, x, is the value of the jth input bit, m is the number of input units that are active in the input vector that was chosen in step 1. and 77 is the learning rate (some small constant). It is easy to show that if the weights on the connections feeding into an output unit sum to | before the weight change, then they will still sum to 1 afterward. 5. Repeat steps 1 to 4 for all input patterns for many epochs. The weight update rule in step 4 makes the output unit more prone to fire when it sees the same input again. If the same input is presented over and over, the output unit will eventually adjust its weights for maximum activation on that input. Because input vectors arrive in a mixed fashion, however, output units never settle on a perfect set of weights. The hope is that each will find a natural group of input vectors and gravitate toward it, that is, toward high activations when presented with those inputs. The algorithm halts when the weight changes become very small. The competitive learning algorithm works well in many cases, but it has some problems. Sometimes, one output unit will always win, despite the existence of more than one cluster uf input vectors. If two clusters are close together, one output unit may learn weights that give it a high level of activation when presented with an input from either cluster. In other words, it may oscillate between the two clusters. Normally, another output unit will win occasionally and move to claim one of the two clusters. Howe