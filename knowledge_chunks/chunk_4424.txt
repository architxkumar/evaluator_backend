network achieves some of the same results as does simulated annealing. Connectionist Models 399 18.4 RECURRENT NETWORKS One clear deficiency of neural network models compared to symbolic models is the difficulty in getting neural network models to deal with temporal AI tasks such as planning and natural language parsing. Recurrent networks, or networks with loops, are an attempt to remedy this situation. Consider trying to teach a network how to shoot a basketball through a hoop. We can present the network with an input situation (distance and height of hoop, initial position of muscles), but we need more than a single output vector. We need a series of output vectors first move the muscles this way, then this way, then this way, etc. Jordan [1986] has invented a network that can do something like this. It is shown in Fig. 18.23. The network s plan units stay constant. They correspond to an instruction like shoot a basket, The state units encode the current state of the network. The output units simultaneously give commands (e.g., move arm x to position y) and update the state units. The network never settles into a stable state; instead it changes at each time step. Output Hidden Units Units Fig. 18.23 A Jordan Network Recurrent networks can be trained with the backpropagation algorithm. At each step, we compare the activations of the.output units with the desired.activations and propagate errors backward through the network. When training, is completed, the network will be capable of performing a sequence of actions. Features of backpropagation, such as automatic generalization, also hold for recurrent networks. A few modifications are useful, however. First of all, we would like the state units to change smoothly. For example, we would not like to move from a crouched position to a jumping position instantaneously. Smoothness can be implemented as a change in the weight update rule; essentially, the error of an output becomes a combination of real error and the m