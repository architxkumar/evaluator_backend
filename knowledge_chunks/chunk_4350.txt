tions in the version space that cover the example and the current elements of the S set. That is, generalize the elements of S as little as possible so that they cover the new training example. If it is a negative example, first remove from S any descriptions that cover the example. Then, update the G set to contain the most general set of descriptions in the version space that do not cover the example, That is, specialize the elements of G as little as possible so that the negative example is no longer covered by any of the elements of G. 4. If Sand Gare both singleton sets, then if they are identical, output their value and halt. If they are both singleton sets but they are different, then the training cases were inconsistent. Output this result and halt. Otherwise, go to step 3. Let us trace the operation of the candidate elimination algorithm. Suppose we want to Jearn the concept ut Japanese economy car from the examples in Fig. 17.12. G and S both start out as singleton sets. G contains the nul! description (see Fig. 17.1]), and S contains the first positive training example. The version space now contains all descriptions that are consistent with this first example:* origin: Japan origin: = Japan origin: Japan mir: Honda mir: Toyota mir: Toyota color; Blue color: Green color: Blue decade. 1980 decade. 1970 decade: 1990 type: Economy type. Sports type: Economy (+) (-) (+) origin: USA origin. = Japan mfr: Chrysler mir. Honda color; Red color: White decade: 1980 decade: 1980 type: Economy type: Economy (-) (+) Fig. 17.12 Positive and Negotive Examples of the Concept Japanese economy car G =U (xy, Xa, X55 Uy 15) S = {(Vapan, Honda, Blue, 1980, Economy)} Now we are ready to process the second example. The G set must be specialized in such a way that the negative example is no longer in the version space. In our representation language, specialization involves replacing variables with constants. (Note: The G set must be specialized only to descriptions that are wit/