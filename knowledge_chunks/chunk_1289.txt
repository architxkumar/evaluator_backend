pace of patterns of four values each, captured in the input layer. The basic idea is the neurons in the output layer compete for activation. Instead of assigning a supervisor designated class label, the neuron with the highest activation is declared as winner. The weights of edges connecting the input layer to the winner neuron are adjusted to become closer to the input vector (pattern). i i Output FIGURE 18.30 In a Kohonen Network, the neuron with the highest activation is the winner. Learning involves weights of the winner to decrease the error. Such networks are said to be self organising. In practice, the competitive layer may contain many neurons, and the neurons may satisfy a similarity property, wherein neurons that are topologically near each other in the output layer have similar weight vectors. This can be ensured by establishing lateral connections amongst the output layer neurons, like in the feedback networks (see discussion on Hopfield networks below). The weights of the lateral connections are positive for neurons closer to each other, and negative for neurons further away. That is, they are excitatory or inhibitory connections. In this way, the activation value of a neuron is also propped by the activation of neighbouring neurons, and vice versa. The spatial location of a neuron map corresponds to a particular pattern or feature in the input space. Each input can be seen as a vector of N values (four for the example in the figure). The corresponding weights of the output neurons are also vectors of N values. In the training step, the weights of the winner neuron (and perhaps a few neighbours) are adjusted so as to come closer to the input vector. If the input data set is clustered in M clusters, all the inputs from the same cluster will have similar vectors. That is, the points in N-dimensional space will be clustered close to each other, and the weight vectors will be aligned closer to each other. If there are more than M neurons in the output layer