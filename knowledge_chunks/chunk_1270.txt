ent is predicted to be from Class-2. Else it is from Class-1. This is equivalent to saying that the output is 1 if 2j 1,, w V; -Wo. This shows that the role of the bias b wo value is to position the line appropriately, whereas w, determines the slope. The equation jo, wx; 0 defines an (n 1) dimensional hyperplane in n dimensional space that aims to separate the two classes. Since this is a linear equation, if it succeeds in separating the two classes, we say that the two classes are linearly separable. This situation is depicted in Figure 8.21. If the two figures representing Class-1 and Class-2 were closer together then one can see that they would not be linearly separable. Like all learning systems, the Perceptron learns its parameters by looking at a subset of the population known as the training set. Based on the weights learnt, the Perceptron can classify previously unseen instances. This classification will be correct, if the training set is a representative of the entire population, and the two classes are linearly separable. The Perceptron Convergence theorem states that if the training data is linearly separable, the Perceptron training algorithm described above will converge to a classifier in a finite number of steps. Readers interested in the proof are recommended to (Rosenblatt, 1962), (Nilsson, 1965) or (Haykin, 2009). Thus, if the training set is linearly separable, the convergence criteria can simply be that no new updates are taking place, or when no training instances are misclassified. If the training data is not separable, convergence is not guaranteed. This is not surprising because the data is not linearly separable and any hyperplane would misclassify some instances, and the algorithm will keep moving the hyperplane about, trying to correctly classify them. In such a situation, an alternate weight update rule discussed below is more useful. FIGURE 18.21 The Perceptron finds a linear discriminator for the two classes. As long as the training da