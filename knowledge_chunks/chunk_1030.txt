fined as follows: sim(D,, Dy) idf log ( ) where N is the total number of documents in the collection and n is the number of documents in which the word occurs. The logarithm is used for scaling, since for many words, n is much smaller than N. Note that in the three-document collection shown above, the inverse document frequency of the word cricket is 0, symbolizing the fact that it has very poor discriminating power between the documents. The product of term frequency and inverse document frequency, referred to as the tf-idf score is often used to capture the overall relevance of a term to a document. The steps involved in processing a document are as follows: 1. Tokenization The document is broken down into tokens after removing any irrelevant markups or metadata. A conscious choice needs to be made about handling punctuation marks and special symbols. 2. Stopword Removal Certain words, sometimes referred to as function words, play no important role in retrieval. Examples are articles like a or the , and prepositions like on and in . Typically, a stopword list is used to identify and filter out such words. Additionally, a domain specific stopword list may also be used. 3. Stemming This is used to reduce each occurrence of a word into its canonical representation, so that different variants of the same word (say storing , stored ) reduce to their root form ( store ). Ideally, FSTs realizing a two-level morphology, as described in Section 16.2.2, should be used for this. In practice, however, simpler algorithms like Porter s stemmer are often used. 4. Term Weighting The relevance of each term to a document is calculated using the f-idf score described above. This gives us a vector corresponding to every document. The same steps are repeated for the query as well. The query vector is then compared against each document vector using the cosine similarity score, and the documents are ranked in descending order and presented to the user. The retrieval scheme described ab