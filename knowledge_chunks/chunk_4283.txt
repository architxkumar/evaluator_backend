0).(1 0/3 1).(6/1137).(34/2353).(168/935).(6/1077).(1/60).(3/11). (3/2267) Note that each probability term is calculated by finding the number of occurrences of the specific bigram and dividing it by the frequency of the previous word. Thus, P(sheland) = (Number of occurrence of the bigram and she)/(Number of occurrences of the word and) Observe that the denominator could also be interpreted as the number of bigrams that start with the word and, When we wish to predict the next word given a word, we may find all the bigram frequencies starting with the given word and use the next word of that bigram that has highest frequency. The concept can be extended to higher grams viz. tri, tetra and finally N-grams. So, what can we do with these grams? If we have a large corpus from which the related probabilities can be calculated, we could generate sentences and verify their correctness. Starting with on word we could predict what could be the next, and then do the same for the next word; always using the maximum probability to select the next word in the sequence. Table 15.3 shows the bigram counts from our corpora for the sentence. Table 15.3. Bigram counts (The number in the bracket indicates the probability.) He never told her and she had never cared to ask He [207]) 0(0) 30.014) 00) -0(0) 40,019) 0(0) 1140.55) 3 0.0144) 00) 200.01) 0) never [60] O(0) 00) 1(0.02) 00) 00) 00) 300.05) 00) 1(0.02) 00) 00) told [31] (0) 000) 0) 10(0.323) 0(0) 00) 00) 00) 00) 00) 00) her [1137] 1(0.00008) 0(0) 00) = 00) 6(0.005) 0(0) 1(0.00008) 0(0) 0(0) 19(0.0167) (0) and [2353} 34(0.0144) 1(0.00004) 00) -26(0.011) 0) 34(0.014) 2300.01) 1(0.00004) 0(0) 38(0.016) (0) she [935] (0) 1.001) (0.001) 0(0) 30.003) _0(0) 168(0.18) 1(0.001) 20.002) (0) 0) had [1077] 8(0.007) 6(0.006) _3(0.003) _0(0) 010) 9(0.008) 12(0.111) 6(0.006) 1(0.0001) 8(0.007) 0f0) never [60] 0(0) 0) 10.02) 000) (0) 0(0) 30.05) (0) 1(0.02) 00) 00) cared [11] 0(0) 0(0) 0) 00) 10.090) 0 0) 00) 00) 0(0) 30.273) 0(0) to [2267] 0(