 must become zero, and that would be a rare phenomenon. What might look like a minimum on first glance may have an escape route in some dimension. Even when plateaus and minima with a small dip in error occurs, one technique that can add some momentum to the gradient descent movement is by carrying forward some movement from the previous iteration. We modify Eq. (18.72) to add a momentum term Aryl) 7 Opry FAW, (2-1) (18.75) where Aw;(n-1) refers to the step taken in the previous or (n 1)th iteration and a is a constant from the range 0, 1) called momentum. Another approach would be to add a degree of randomness along the lines discussed in Chapter 4. The size of the network will obviously have an impact on the convergence rate, since the number of dimensions of the space in which gradient descent is done increases with the number of neurons. On the other hand, the size of the network will also have a bearing on the representational power of the network. Ideally, we would like to have a network that has just enough neurons to be able to generalize over unseen examples effectively. If there are too many neurons then there is a danger of overfitting (see Section 18.4.2). The question of overfitting also arises in deciding the termination criterion for Backpropagation. A simple termination criterion is to stop when the decrease in error has become sufficiently small. If the algorithm is run too long then it will overfit the weights to the training data. Recall that overfitting happens in a supervised learning situation when the learning algorithm has a very low error on the training data but a disproportionately large error on unseen data. This is particularly a danger in using Backpropagation because the same data is seen again and again. One way to avoid overfitting is to do a fixed number of iterations, but then one should have an idea of how many. A method that is often used is to separate the training data from the test data or the validation data. Performance on t