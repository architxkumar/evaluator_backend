ositive increment and reducing all other probabilities proportionately to maintain the sum equal to I. If the response is bad, the automaton will penalize the selected setting by reducing the probability corresponding to the bad setting and increasing all other values proportionately. This process is repeated each day until the good selections have high probability values and all bad choices have values near zero. Thereafter, the system will always choose the good settings. If, at some point, in the future your temperature preferences change, the automaton can easily readapt. Learning automata have been generalized and studied in various ways. One wons. $Ofl WIui w Eniironmont Oi Ofl Figure 17.4 Learning automaton model. Sec. 17.4 Learning Automata 373 Initial pr Obabihty cilues 1110 1/10 1/10 1/10 I/10 I .I I I Control neleeoon Jr 50 55 60 65 70 75 80 85 90 95 100 II I I I I Temperature range settings Figure 17.5 Temperature control model. such generalization has been given the special name of collective learning automata (CLA). CLAs are standard learning automata systems except that feedback is not provided to the automaton after each response. In this case, several collective stimulusresponse .actions occur before feedback is passed to the automaton. It has been argued (Bock. 1976) that this type of learning more closely resembles that of human beings in that we usually perform a number or group of primitive actions before receiving feedback on the performance of such actions, such as solving a complete problem on a test or parking a car. We illustrate the operation of CLAs with an example of learning to play the game of Nim in an optimal way. Nirn is a two-person zero-sum game in which the players alternate in removing tokens from an array which initially has nine tokens. The tokens are arranged into three rows with one token in the first row, three in the second row, and five in the third row (Figure 17.6). The first player must remove at least one token but no