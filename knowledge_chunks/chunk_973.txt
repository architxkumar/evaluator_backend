se for distance. Once we think of cases as points in some space, we are essentially relating distance (inversely) to similarity. For that, one needs to find the maximum difference between values, to be equated to zero similarity value. As a corollary, a difference of say d in two different numeric attributes may have different impact. An extreme example is Boolean attributes. Here, a difference of 1 corresponds to (local) similarity of 0. In the example above of Figure 15.16, the Xp axis has a maximum difference 70 180 - 110, while the X, attribute has a maximum difference 140 160 20. One way to address this is to normalize all distances to be in the range 0,1 . While doing so, one can also incorporate the weights of the different attributes. The kd-tree and the Inreca tree employ a tree structure to index the cases in the case space. The objective is to group together cases similar to each other in compartments that are indexed. Other approaches that have employed indexing schemes are the structure GNAT (Brin, 1995), and the Bubbleworld (Efros, 1998). 15.2.6 Fish and Shrink Algorithm The complexity of retrieval is dependent, both on the number of cases one has to search through, and the complexity of each similarity computation. The kd-tree and similar algorithms address the issue of inspecting a small number of cases from a large case base. The algorithm Fish and Shrink, on the other hand, addresses the issue when similarity function is very complex (Schaaf, 1995, 1996). The similarity function can be complex when the case structure is complex. This can be when the cases are themselves structured. We give a brief idea here of when that can be. The interested reader is referred to (Bergmann, 2002) for a more detailed discussion. Consider, for example, a recommender system for computer systems. A case in this system is the description of a specific computer model. This can be an object oriented (OO) representation. In an object oriented representation, the relation 