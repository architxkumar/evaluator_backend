x x x x x x where an x represents missing or unspecified data, it would converge to the second pattern P2, and given a pattern like 10 111001 1 probably converge to P;. This can be seen as similarity based retrieval, in which the most similar pattern is retrieved from memory. Recall that we studied other approaches to similarity based retrieval in Chapters 15 and 16 when we looked at memory based reasoning systems that store and reuse problem solving experience, possibly in free text form. Based on his experiments, Hopfield hypothesized that the number of patterns that can be stored reliably in a network is about fifteen percent of the number of neurons. Thus to store 150 patterns, one would need a network of about a thousand neurons. One can think of such networks as content addressable memories or associative memories, in which patterns are stored and recalled on partial or even erroneous description of the pattern as input. This has been claimed to be closer to the way our own memory systems work. Observe that this is starkly different from computer memory systems that retrieve information, based on the address of the memory location. This has also been put forth as a more biologically feasible network model than the Backpropagation based feedforward network. However, much of the work done in ANNs is focused towards solving problems that are of immediate interest, and the feedforward networks have been immensely successful in the task of pattern classification, such as handwritten character recognition. 18.8.5 Subsymbolic Representations The success of neural networks can be attributed to the ability of ANNs to generalize, and hence tolerate erroneous (previously unseen) input, and the fact that they learn inductively from a set of samples. In contrast, it has been much more difficult to directly implement algorithms for the task of pattern recognition, since it is difficult to articulate the precise rules that define the patterns. Such articulation is not necess