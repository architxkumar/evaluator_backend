 examining the properties of words. The first property of a word is that it has a meaning; a word is a surrogate for something in the material or the abstract world. Letters that constitute a word don t have meanings of their own (though some words are just single letters); so in that sense, a word is the smallest linguistic element with a meaning. In NLP, the study of word meanings is called lexical semantics. The word lexical is used whenever we want to refer to processing at the level of a word. One central question is: how do we make machines understand the meanings of words? Humans use dictionaries which explain the meanings of complex words in terms of simple ones. For machines to use dictionaries, we have two problems. The first is, how do we communicate the meaning of simple words (like red or sad )? We have also discussed the problem of defining the semantics of such words in FOL in Chapter 13. The second is, to understand the meanings of complex words out of simple ones, we would need the machine to understand English in the first place. The first problem has no easy solution; there are words whose meanings are expressed better in the form of images or when contrasted with other words ( orange versus yellow ). The second problem of defining words in terms of others can be addressed using a knowledge-representation formalism like a semantic network. The WordNet (Miller, 1995) is a massive network of words compiled manually over 10-15 years, where each word is extensively annotated and its inter-relationships to other words are also specified. A fragment of the WordNet network is shown in Figure 16.1. Object Artifact Instrumentality Arucle Conveyance Ma Tableware Vehicle Cutlery Motor Vehicke Wheeled Vehicle kd rk Car Cycle Bicycle Figure 16.1 A fragment of the WordNet Noun Network showing a concept hierarchy (an adapted version of an example in (Jiang et al., 1997)). In the terminology of WordNet, a word has a form (the sequence of letters that comprise it)