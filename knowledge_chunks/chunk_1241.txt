s as conjunctions of constraints on attributes. What would happen if instead we allow the use of disjunction and negation as well? Then we could say things like Experience should be Medium or High or equivalently Experience should not be Low . Then the algorithm would be able to find a concept that is consistent with the entire training set, but there would be a danger that the algorithm would be unable to say anything about the unseen elements of the domain. Consider the situation when such a learner has input a set of positive instances for a concept P1, Po, ..., Pn and a set of negative instances n1, no, ..., n then it could simply use the instances to form the two boundaries as follows. G -(nv nz... V n,) S (pV P2 --- V Pp) That is, it simply memorizes the training set. As a consequence, it is unable to decide upon the class of the unseen instances, even while it is flawless on the training set itself. This is an example of overfitting. It is unable to generalize from the examples. Given a hypothesis space H, a hypothesis heH is said to overfit the training data, if there exists some alternative hypothesis h'eH, such that h has a smaller error over the training examples, but h has a smaller error than h over the entire distribution of instances (Mitchell, 1997). On the other hand, with the conjunctive hypotheses schema the system is learning from training data and is able to generalize. But there is a caveat, the inductive bias. The bias is that the system works only when the target function is contained in the space of conjunctive hypotheses. In a sense, the bias in the system is able to learn and generalize, only when there is a restricted form for the hypotheses which it tunes to the training examples. The fundamental property of inductive learning is that a learner that makes no a priori assumptions regarding the identity of the target concept has no rational basis for classifying unseen instances (Mitchell, 1997). Because inductive learning requires some fo