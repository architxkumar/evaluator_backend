ne in h (s), the h2(s) function looks at the costliest pair of propositions that are achieved at the same time. Let us say that the two propositions are called p and q. Then the heuristic function h2(s) is defined as (Ghallab et al., 2004), e(s, p) 0 ifpes minje oy 1 g (s, precond(a) otherwise gs, tp, gq) 0 ifp,qes min min, 1 9 (s, precond(a)) p, g GC add(a) , min, 1 g (s, gq Uprecond(a)) pe add(a) , min, 1 g (s, p Uprecond(a)) q add(a) 2(s, G) max, , (2s, Pg) g SG and, h (s) g(s, G) Clearly, h2(s) is more informed than h (s) which can be thought of as h (s). In fact, we can define a family of heuristic functions h1(s), h2(s), ..., h s)which are increasingly more informed, as well as increasingly more expensive to compute. One can observe that when for G j, the function hi(s) in fact solves the relaxed planning problem, which is NPhard. Another version HSPr, a regression planner, searches backward from the goal propositions (see Chapter 7 for a discussion on backward state space search). According to Bonet and Geffner, heuristic computation in HSP and HSP2 takes up about 80 of the total computation time, and this results in fewer nodes being explored by the search algorithm. The problem becomes acute because as the forward state-space search generates new candidate nodes as it progresses, the heuristic value for every new state has to be computed. The algorithm HSPr instead searches in the regression space, searching backwards from the goal propositions. A major advantage of doing so is that the heuristic computation is drastically reduced. Figure 10.17 illustrates the situation. The HSPr algorithm initially computes the value g(So, p) for every ground predicate p, where Sq is the start state. This is illustrated in the figure by the dashed lines connecting the nodes, which are drawn when they first appear as time increases from left to right (as in the planning graph). Since we are ignoring the delete effects, once a proposition has been generated it is always a pa