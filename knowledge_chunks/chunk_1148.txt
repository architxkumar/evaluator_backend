 2, ---1 n) P(e2 3, wees On) ... P( p) This leads us towards the Naive Bayes Classifier that we look again in Chapter 18. It is claimed that probability theory was devised to address problems in gambling in which players are concerned with odds (Stewart, 2002). Odds represent the amount the player is willing to wager, based on her belief of the probability of the event being bet on. For example, one would give 1:1 odds that a fair coin will come up heads, and give 1:3 odds that a random card drawn from a standard pack would be a spade. When we say that the odds are m:n, this corresponds to the probability m (m n) (Jeffrey, 2004). Thus, the odds 1:3 for drawing a spade card correspond to the probability 1 (1 3) , which is the probability of drawing a spade. The odds represent a fair bet, which would even out in the long run. Judea Pearl (1988) portrays the Bayes rule using odds as follows. Let H be the hypothesis we are interested in and let H be the case that H is not true. That is P(H) 1 P(H). The prior odds O(H) that H is true are given by, O(H) P(H) P( H) P(A) (1 P(A)) The posterior odds given the evidence e are defined as O(H e) where, O(Hle) P(H e) P( Hle) which by applying the Bayes rule becomes, P(e H) PH) P(elH) PH) P(el-H) PCH) -P(e -H) (1 - P(A) If we define the likelihood ratio L(e H) as, L(e H) PlelH) P(e 7H) ole) then we get the posterior odds, O(Hle) L(e H) O(H) Thus, the posterior odds of the hypothesis H being true is the product of the likelihood ratio of the evidence being seen if H were true versus if H were not true, and the prior odds of H. The likelihood ratio provides diagnostic support given the evidence, and the prior odds are a predictive support based on previous knowledge. Very often when two random variables or propositions are related there exists a causal relation between them. For example consider the statement, A new toy makes a child happy . In a logic framework one may express this as (NewToy Happy). Then if we add the sentence New