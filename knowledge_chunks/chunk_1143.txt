Y) PCo P() But we also know that by the product rule, PCX, Y) PAY) PY) POX) PX) From the above two equations, we can derive, POY) PO) which can be seen as an equation characterizing the independence of X from Y. The value that Y takes, if tossed earlier, does not influence the probability distribution of X. In a similar manner, we can observe that Y is independent of X. Given N variables, one can talk of a joint probability distribution that assigns a probability to each combination of values the N variables can take. We denote this joint probability distribution by Px, Xo, ..., Xp). An element of this probability distribution would be of the form P(X, V13, Xo Voz, .... Xp Vn4) 0.06, which says that the probability that X, takes the third value V;3 from its domain, and X2 takes its seventh value V27, and so on till X, is 0.06. Wherever there is no ambiguity, we may write this as P(V43, Vo07, Vna) 0.06. Observe that the joint probability distribution in the general case could be very large. If each of the N random variables can take K values, then there are K different elements in the joint probability distribution. Obliviously, filling up these values would be a huge task. In the special case when each random variable corresponds to a sentence in propositional logic, taking the value true or false, the joint probability distribution will have 2" entries. Each of these 2" entries will correspond to an interpretation of the set of sentences (see Chapter 12). Except that the interpretation assigns true or false values to each proposition, and the joint probability distribution assigns a measure of belief to each combination of such values. We could think of the probability of each interpretation being true as P( ), which is the measure of belief of that combination of truth values holding. Then given an arbitrary sentence a in the logic, we could compute the probability of the sentence being true as the sum of the probabilities of all those interpretations, in which a 