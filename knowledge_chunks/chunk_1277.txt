how it can be represented with more than one neuron connected together. We next look at multilayer neural networks and the algorithms to train them. 18.8.2 Feedforward Networks Network structures have different kinds of flow of information between neurons. In a feedforward network, there are at least three ayers of neurons, known as the input layer, the hidden layer, and the output layer (see Figure 18.26). Information flows in one direction from the input layer towards the output layer. The figure shows a network that takes in a pattern of four values as input and produces an output pattern of three values. These could, for example, be three classes that patterns fall into, if the output is unary, or eight classes if the output is binary (treated as three bits). We assume that each output layer neuron is connected to all hidden neurons, and each hidden neuron is connected to all input layer neurons. Observe that in the training process, some of these connections could be broken if they are assigned values close to zero in the training process. Input Hidden Quput FIGURE 18.26 A feedforward artificial neural network. Each neuron in the hidden and output layers generates an output that is a function of the inputs it receives. This function should be a nonlinear function for the hidden layer to be meaningful. Otherwise, the network could be collapsed into a two layer network called a Linear Association Classifier (Yegnanarayana, 1999). This function should also be a differentiable function if one is to use the gradient descent algorithm. In Section 18.8.1, we have looked at the use of gradient descent weight adjustment rule. This rule was applied when only the linear weighted sum of the inputs was considered. But as mentioned above, cascading neurons that compute a linear sum does not give us any additional power. We need a function that is both non-linear and differentiable. Typically, the function that is chosen is the sigmoid function, reproduced below to contrast i