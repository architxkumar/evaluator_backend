know that J(w,) = 0. Suppose we begin with a random weight vector W that is not a solution vector. We want to slide down the / surface. There is a mathematical method for doing this we compute the gradient of the function J(). Before we derive the gradient function, we reformulate the perceptron criterion functioin to remove the absolute value sign: 4) W2 solution space Fig. 18.10 Adjusting the Weights by Gradient Descent, Minimizing j(w) ~ X if X is misclassified as a negative example I (i) = y a $s 2 P? sox (- if X is misclassified as a positive example Recall that X is the set of misclassified input vectors. Now, here is VJ, the gradient of J(w) with respect to the weight space: J( ) = rex The gradient is a vector that tells us the direction to move in the weight space in order to reduce J (iv). [n order | X if X is misclassified as a negative example ~X if X is misclassified as a positive example to find a solution weight vector, we simply change the weights in the direction of the gradient, recompute J(W), recompute the new gradient, and iterate until J(w) = 0. The rule for updating the weights at time t+1 is: Way = Ww, + VI Or in expanded form: . x if X is misclassified as a negative example Fa = * 7 ~ if X is misclassified as a positive example 7718 a scale factor that tells us how far to move in the direction of the gradient. A small 7) will lead to slower learning, but a large 7) may cause a move through weight space that overshoots the solution vector. Taking 7 to be a constant gives us what is usually called the fixed-increment perceptron learning algorithm : Connectionist Models 383 asco RATERS RSSICON, Algorithm: Fixed-Increment Perceptron Learning Given: A classification problem with 1 input features (x,,%2,....x,,) and two output classes. Compute: A set of weights (wo, 1, W2, .... W,) that will cause a perceptron to fire whenever the input falls into the first output class. Create a perceptron with n + 1 inputs and n + | weights, where the x) is alway