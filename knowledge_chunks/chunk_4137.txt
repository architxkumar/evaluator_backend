. Here we assume that the static evaluation functicn returns large values to indicate good situations for us, so our goal is to maximize the value of the static evaluation function of the next board position. 234 Artificial Intelligence SANA An example of this operation is shown in Fig. 12.1. It assumes a static A evaluation function that returns values ranging from 10 to 10, with 10 indicating a win for us, -10 a win for the opponent, and 0 an even match. Since our goal is to maximize the value of the heuristic function, we choose to move to B. Backing B s value up to A, we can conclude that A s value is 8, since we know we can move to a position with a value of 8. 8) 3) 2) But since we know that the static evaluation function is not completely Fig. 12.1. One-Ply Search accurate, we would like to carry the search farther ahead than one ply. This could be very important, for example, in a chess game in which we are in the middle of a piece exchange. After our move, the A situation would appear to be very good, but, if we look one move ahead, we will see that one of our pieces also gets captured and so the situation is not as favorable as it seemed. So we would like to look ahead to see what will happen to each of the new game positions at-the next move which will be made by the opponent, Instead of applying the static evaluation function to each of the positions that we just generated, we apply the plausible-move generator, generating a set of successor positions for each position. lf we wanted to stop here, at two-ply lookahead, we could apply the static evaluation function to each of these positions, as shown in Fig. 12.2. But now we must take into account that the opponent gets to choose which successor moves to make and thus which terminal value should be backed up to the next level. Suppose we made move B. Then the opponent must choose among moves E, F, and G. The opponent s goal is to minimize the value of the evaluation function, so he or she can be expected 