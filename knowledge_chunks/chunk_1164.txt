to a new state s' with a certain probability given by P(s, a, s ), where s, s S. The probability distribution must satisfy the condition for a given state s S and an action acA, X ves P(s, a, ) 1 That is, the action does result in a state. The agent could have several types of goals that it wants to achieve. These could be the classical goal of reaching a final goal state and it could have some trajectory preferences of going through or avoiding certain states. However, since the agent cannot deterministically control its actions but works with a policy that is defined over the entire state space, one needs to specify its performance in terms of evaluating the entire policy. This is done by specifying a reward function R: S -Large, Large that denotes the reward obtained by being in a given state. Here, Large is a suitably large number. In addition, one could also consider the cost of doing an action in a given state given by a function C: S x A 0, MaxCost , where MaxCost is the maximum possible cost of doing an action. We can now define the utility of being in a state s S and applying an action a A as, U(s, a) R(s) - C(s, a) If we are considering a policy 77 then the utility of applying the policy in state s Sis given by, U(s 17) R(s) - C(s, m(s)) where a 77(s) is the action chosen by the policy in that state. Given the notion of utility of being in a state, one can define a value function of a state given a policy p as a function of the total reward, accumulated over the history h sg, s7, ... minus the total cost involved, when the policy is applied. That is, Vso) u(U (sols), Uls, as,)), --- ) where u is some utility accumulating function. In the general case when the system may run for ever, one would like to measure the current worth of a state using a function that gives lesser importance to states that occur farther into the future. This is akin to discounting the future value of money now. It is not surprising that the idea of measuring current worth by discou