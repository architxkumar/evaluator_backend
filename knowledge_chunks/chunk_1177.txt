nto the policy. In this way, it moves to a new policy which has a strictly lower cost. The process continues till a better policy cannot be constructed. The algorithm in the Figure 17.34 below is adapted from (Mausam and Kolobov, 2012). We use the notation QY,, , to represent the Q-value at the end of the (n - 1)" iteration generated by the policy 77, 1. The expression V7,, , represents the complete value function and the end of the (n 1)" iteration, and V,(sx) is the lowest value for state s, amongst the value generated by all possible actions applicable to the state in the current iteration. As can be seen from lines 11-12, when this value is lower than the earlier value in which action 7, 1 (Sx) was applied then the action is replaced by the action that yields the lowest value. Policy-Iteration (state space S, action set A, costs C) laeo 2 for k 1 to ISI 3 do ,(3,) ay initialize the policy randomly 4 repeat 5 naAentil Compute V ,. Solve the set of linear equations 7 for k 1 to ISI 8 do (Sx) Mani (Sx) 9 WaeA Compute O . , (s,,a) 10 Vi(S ) mings ONn-1 (Se ) 11 if OM. (Sy, My (S,)) Va (5) 12 then ,(s,) argmings O",-1 (5,2) 13 until m, m, 14 return 7, Figure 17.34 The algorithm Policy Iteration starts with a random policy, in this version, choosing the first applicable action in each state. It then goes through a loop looking for better actions for each state s,. An action is better than the previous one, if it results in a lower Qvalue for that state. The point to note is that the Policy Iteration algorithm refines policies to strictly better policies. In each cycle of the Repeat loop, the policy is guaranteed to improve. It is doing Hill Climbing in the policy state (see Chapter 3). However, there is no danger of getting stuck in a local minimum. It has been proved that if the algorithm is initialized with a proper policy 779 then it is guaranteed to find the optimal policy. One has to be careful that the initial policy is a proper one, if one is to use Policy Iter