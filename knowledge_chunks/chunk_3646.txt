ng algorithm. Given training objects from two distinct classes, class-I and class-2 1. Choose an arbitrary initial value for w 2. After the rn" training step set = w,,, + d * x where d= +1 if r = w * x <0 and x,,, is type class-I d = 1 if r >0 and x,, is type class-2 Otherwise set w,,,4.1 = w,,,(d = 0) 3. When w,. = w,,,+1, (for all >= I) stop, the optimum w has been found. 25= 370 Early Work in Machine Learning Chap. 17 It should be recognized that the above learning algorithm is just a method for finding a linear two-class decision function which separates the feature space into two regions. For a generalized perceptron, we could just as well have found multiclass decision functions which separate the feature space into c regions'. This could be done by terminating each of the ATU outputs at c > 2 comparator units. In this case, class j would be selected as the object type whenever the response at the jth comparator was greater than the response at all otherj - I comparators. Perceptrons were studied intensely at first but later were found to have severe limitations. Therefore, active research in this area faded during the late 1960s. However. the findings related to this work later proved to be most valuable, especially in the area of pattern recognition. More recently. there has been renewed interest in similar architectures which attempt to model neural networks (Chapter 15). This is partly due to a better understanding of the brain and significant advances realized in network dynamics as well as in hardware over the past ten years. These advances have made it possible to more closely model large networks of neurons. 17.3 CHECKERS PLAYING EXAMPLE During the 1950s and 1960s Samuel (1959, 1967) developed a program which could learn to play checkers at a master's level. This system remembered thousands of board states and their estimated values. They provided the means to determine the best move to make at any point in the game. Samuel's system learns while playin