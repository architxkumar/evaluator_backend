nst the entire data set, and choose the one that classifies the instances most accurately. Another problem with the candidate elimination algorithm is the learning of disjunctive concepts. Suppose we wanted to learn the concept of European car, which, in our representation, means either a German, British, or Italian car. Given positive examples of each, the candidate elimination algorithm will generalize to cars of any origin, Given such a generalization, a negative instance (say, a Japanese car) will only cause an inconsistency of the type mentioned above. Of course, we could simply extend the representation language to include disjunctions. Thus, the concept space would hold descriptions such as Blue car of German or British origin and Jtalian sports car or German luxury car. This approach has two drawbacks. First, the concept space becomes much larger and specialization hecomes intractable. Second, generalization can easily degenerate to the point where the S set contains simply one large disjunction of al! positive instances. We must somehow force generalization while allowing for the introduction of disjunctive descriptions. Mitchell [1978] gives an iterative approach that involves several passes through the training data. On each pass, the algorithm builds a concept that covers the largest number of posiiive training instances without covering any negative training instances. At the end of the pass, the positive training instances covered by the new concept are removed from the training set, and the new concept then becomes one disjunct in the eventual disjunctive concept description. When all positive training instances have been removed, we are Jeft with a disjunctive concept that covers all of them without covering any negative instances. There are a number of other complexities, including the way in which features interact with one another. For example, if the origin of a car is Japan, then the manufacturer cannot be Chrysler. The version space algorithm a