ted with the " input, or the connection from the neuron. One often views the bias as a constant 0" input with value 1 and weight Wo b. The output of the first stage is, 2 Ziny np WX O z. 0. nn Wi X; where wy bandxy 1 (18.54) In the second stage, the signum function sgn is applied. This function essentially looks at the sign of the above expression. y sen(c) 1 if D9, wekx; 0 (18.55) -1 if Dio, wx; 0 FIGURE 18.19 The input to the Perceptron are n input signals x;...x, attenuated by weights W4... W, and a constant bias with weight b. The Perceptron first computes the weighted sum of its n 1 inputs and then applies a signum function sgn that returns 1, if the sum is greater than Zero, and 1 otherwise. A Perceptron can be seen as a classifier for a two class problem. Each element from the domain is represented by a set of n real feature or attribute values x 4...x . The Perceptron computes a (n - 1) dimensional hyperplane that separates the space into two regions, each containing one class. The hyperplane is defined by the n 1 weights Wo...W, . The learning task that the Perceptron solves is finding these weights, such that the Perceptron returns a value 1 for all elements of one class, and -1 for all elements of the other class. In a two dimensional space, where there are two features, the hyperplane is a line. We will use a two dimensional space for illustration. Error Correcting Weight Adjustment Rule The Perceptron stores the knowledge that discriminates between the two classes in the form of weights wo...w, The values of this weight vector are acquired through a process of supervised learning during which a training sequence is presented to the Perceptron along with the class labels, and the training algorithm adjusts the weights by a small amount, if there is a discrepancy between the class label predicted by the Perceptron and the one specified with the training example. The Perceptron training rule or the error correcting rule adjusts each weight w; as follows, W