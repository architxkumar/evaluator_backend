t, training class label 11 , computed output of the k neuron 12 5, ( 4704) 04 (1-0) 13 for j 0 to N, 14 Wy Wy 46,40; Note: 0, xj, is for j 1 to N; Adjusting hidden layer weights 16 6, 05 (1-05) Zaseourpuce 5x Oe 17 for it 0 to 18 Wig Wey 40.4 , Note: x, x5 19 until convergence 20 return j, W, FIGURE 18.29 The Backpropagation algorithm initializes all weights to small values denoted here by . It first adjusts the weights of the output layer W; and then propagates the error backward to adjust the weights in the hidden layer Wj, In line 16, we have used the summation as a shortcut for a loop in the interest of readability. The Backpropagation algorithm is an instance of gradient descent in the weight space. The loop in Lines 7-19 is repeated for a large number of training instances. See (McClelland and Rumelhart, 1986) for a detailed description and analysis. A large number of neural networks implemented use the Backprop algorithm as it is popularly known. Convergence and Representation Power How much more representation power do multilayer networks have than Perceptrons, and how does the training algorithm perform? We look at the second question first. The error surface for the Perceptron is smooth and parabolic with one global minimum that gradient descent ends up in. Unfortunately, the error surface for the multilayer Perceptron, as feedforward networks are also known, may have multiple local minima. It is possible that the Backpropagation algorithm could get stuck in one of the local minima. But in practice, the algorithm performs quite well, and is popularly used in many kinds of applications. It has been suggested that the occurrence of local minima may not be very prevalent. The reason for this is that for a local minimum to occur, the gradient in all the dimensions must become zero, and that would be a rare phenomenon. What might look like a minimum on first glance may have an escape route in some dimension. Even when plateaus and minima with a small dip in err