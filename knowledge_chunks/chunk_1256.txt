ged, as does the centroid for each cluster, the iterative procedure has converged. Genes 1 to 5 form one cluster with centroid y, (1.8, 1.6) and genes 6 10 for the second cluster with centroid pp (4.8, 4.8). Table 18.8 The Euclidean distance of each point to the two centroids, after iteration 2 Gene by (1.8,1.6) Uz (4.8,4.8) gene-1 1.0 5.4 gene-2 0.9 47 gene-3 0.4 4.0 gene-4 0.6 47 gene-5 1.3 3.3 gene-6 3.3 1.1 gene-7 4.0 0.8 gene-8 47 0.3 gene-9 5.4 1.2 gene-10 4.8 1.4 18.7 Learning from Outcomes We began this chapter with supervised learning, in which a learning system works with labelled data in a supervised learning process. The learning system then learns to assign the label for similar unseen data. We then saw an unsupervised, learning algorithm which observes a given data set and partitions it into subsets of similar elements, based on some measure of distance or similarity.? An autonomous agent operating a world has other opportunities to learn as well. The agent can observe what happens after it carries out an action or a set of actions in the world, and learn to identify those actions that are more rewarding. For actions that yield an immediate reward, the learning task is relatively straightforward, as will be corroborated vouched by anyone who plonks down in front of a television set to watch a football game, where finishing the class assignment would have yielded a reward somewhat in the future. We had briefly touched upon the problem of such goal conflicts in Section 14.5. However, this is not the topic of our focus here. Instead, we look briefly at how an agent can learn to identify actions when the reward from them is given sometime in the future, and in the interim period, the agent might have carried out other actions. The problem, as first articulated by Marvin Minsky, is of temporal credit assignment. When a sequence of actions leads to some eventual reward, how does one assign the credit of the final result to individual actions? This has been s