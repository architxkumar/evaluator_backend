thmontherestaurant
data. Thecurvefor DECISION-TREE-LEARNIN Gisshownforcomparison.
test that agrees exactly with some subset of the training set. Once it finds such a test, it
adds it to the decision list under construction and removes the corresponding examples. It
thenconstructs theremainderofthedecision list, using justtheremaining examples. Thisis
repeated untiltherearenoexamplesleft. Thealgorithm isshownin Figure18.11.
This algorithm does not specify the method for selecting the next test to add to the
decisionlist. Althoughtheformalresultsgivenearlierdonotdependontheselectionmethod,
it would seem reasonable to prefer small tests that match large sets of uniformly classified
examples,sothattheoveralldecisionlistwillbeascompactaspossible. Thesimpleststrategy
istofindthesmallesttesttthatmatchesanyuniformlyclassifiedsubset,regardlessofthesize
ofthesubset. Eventhisapproach worksquitewell,as Figure 18.12suggests.
18.6 REGRESSION AND CLASSIFICATION WITH LINEAR MODELS
Now it is time to move on from decision trees and lists to a different hypothesis space, one
that has been used for hundred of years: the class of linear functions of continuous-valued
LINEARFUNCTION
718 Chapter 18. Learningfrom Examples
1000
900
800
700
600
500
400
300
500 1000 1500 2000 2500 3000 3500
0001 ni
ecirp
esuo H
Loss
w
0
w
1
House size in square feet
(a) (b)
Figure 18.13 (a) Data points of price versus floor space of houses for sale in Berkeley,
CA, in July 2009, along with the linear function hypot(cid:2)hesis that minimizes squared error
loss: y 0.232x 246. (b) Plot of the loss function
j
(w
1
xj w
0 yj)2 for various
valuesofw ,w . Notethatthelossfunctionisconvex,withasingleglobalminimum.
0 1
inputs. We ll start with the simplest case: regression with a univariate linear function, oth-
erwise known as fitting a straight line. Section 18.6.2 covers the multivariate case. Sec-
tions 18.6.3 and 18.6.4 show how to turn linear functions into classifiers by applying hard
andsoftthresholds.
18.6.1 