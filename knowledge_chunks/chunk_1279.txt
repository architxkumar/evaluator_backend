s, the output could be the ASCII code for the character. The Backpropagation Algorithm The Backpropagation algorithm was first devised by Werber (see (Werber, 1994) for more information) and published in the well known book by Rumelhart and McClell and. The Backpropagation algorithm also adopts a gradient descent algorithm for training, and is essentially an extension of the PerceptronDeltaRule algorithm described in Figure 8.23. Each weight being learnt is moved a little bit in the direction opposite to the gradient of the Error hypersurface in the weight space. The magnitude of this move is dependent upon the error in the output from the neuron. For the neurons in the output layer, the error value can be readily computed because the actual output and the target output are known. For a neuron in the hidden layer, this is not the case since the target output is not available. However, since the neurons in the output layer receive their inputs from the hidden layer neurons, the error in their output can be attributed to an error in their inputs, which are the outputs of the various neurons in the hidden layer they are connected to. If one can propagate the error from the output layer to the neurons in the hidden layer then we have a basis for training the hidden-layer neurons. This is the key feature of the Backpropagation algorithm, and from which it derives its name. The basic idea behind the gradient descent is still the same. For each weight, take a step in the direction opposite to the gradient. The magnitude of the step depends upon the magnitude of the derivative of the error and upon the learning rate n, as described in Eq. (18.61). This is reproduced below. Observe that now there are many neurons for which we need to train the weights and hence there are two indices. In the equation below, the weight of the link from the " neuron to the j neuron is being adjusted, along the direction opposite to the gradient of the error E (see Eq. 18.59) with respect to the