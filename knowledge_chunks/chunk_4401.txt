ace we used to explain perceptron learning (Fig. 18.10). As we modified weights, we moved in the direction of the bottom of the bowl; eventually, we reached it. A backpropagation network, however, may slide down the error surface into a set of weights that does not solve the problem it is being trained on. If that set of weights is at a local minimum, the network will never reach the optimal set of weights. Thus, we have no analogue of the perceptron convergence theorem for backpropagation networks. There are several methods of overcoming the problem of local minima. The momentum factor a, which tends to keep the weight changes moving in the same direction, allows the algorithm to skip over small minima. Simulated annealing, discussed later in Section 18.2.4, is also useful. Finally, adjusting the shape of a unit s activation function can have an effect on the network's susceptibility to local minima. Fortunately, backpropagation networks rarely slip into local minima. It turns out that, especially in larger networks, the high-dimensional weight space provides plenty of degrees of freedom for the algorithm. The lack of a convergence theorem is not a problem in practice. However, this pleasant feature of backpropagation was not discovered until recently, when digital computers became fast enough to support large-scale simulations of neural networks. The backpropagation algorithm was actually derived independently by a number of researchers in the past, but it was discarded as many times because of the potential problems with local minima. In the days before fast digital computers, researchers could only judge their ideas by proving theorems about them, and they had no idea that local minima would tum out to be rare in practice. The modern form of backpropagation is often credited to Werbos [1974], Le Cun [1985], Parker [1985], and Rumelhart e a/. [1986]. Backpropagation networks are not without real problems, however, with the most serious being the slow speed of lea