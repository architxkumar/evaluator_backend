nctions are defined as measures ol likelihood of class inclusion. For example, using Bayes' rule, one can compute the v conditional probability P(C,IX) that the class of an object o is C, gi en the ubsersed value of X for ,. This approach requires a knowledge of the prior prabahiltt: P(C,), the probability of the occurrence of samples from C, as shell as !i X ( C xl . +4 + 4. 4- *4. ++ + 000+ * + 400. 0++0000 4+ 4*000 4-.- o o + + + 0 0 0 0 +.+ * +O000*** 000+4.4000+4+0 + 4-400004+4. 000+4+000+ #00 44+000 * + o 0 0 0 +1 + 0 0 0 4 0 0 44*4.4+4+ 0 0 0 01* + 0 0 0 0 0 4*4*44o olo + + * + + 0 000o00 jpeg I 00 Figure 13.3 Examples of nsn Irnearty separable classes. 276 Pattern Recognition Chap. 13 (Note that the C, are treated like random variables here. This is equivalent to the assumption made in Bayesian classification where the distribution parameter 0 is assumed to be a random variable since C, may be regarded as a function of 0). A decision rule for this case is to choose class C1 if X) > PC, I X) for all i 7^ j. A more comprehensive probabilistic approach is one which is based on the use of a loss or risk Bayesian function where the class is chosen on the basis of minimum loss or risk. Let the loss function L, denote the loss incurred by incorrectly classifying air actually belonging to class C, as belonging to C1. When I.,, is a constant for all i. I. I j. a decision rule can be formulated using the likelihood ratio defined as (see Chapter 6) P(XICk) PXIC, The rule is to choose class Co whenever the relation P(X Ck) > holds for all j ^ k P(XIc J) P((-1,) Probabilistic decision rules may be constructed as either parametric or nonparametric depending on knowledge of the distribution forms, respectively. For a comprehensive treatment of these methods see (Duda and Hart, 1973) or (lou and Gonzales. 1974). Syntactic Classification The s)ntactic recognition approach is based on the uniqueness of syntactic "structure'' among the object classes. With this approach, a gra