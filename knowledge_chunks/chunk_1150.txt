at corresponds to the given values of the given variables. However, very often we do not have the complete joint distribution. We have already observed that this table can be prohibitively large in size. Instead, we have fragments of the table captured in terms of some known prior probabilities, and some conditional properties. Computation is needed to determine the posterior probabilities of some variables of interest. Note that the probability of getting a new toy P(NewToy yes) is 0.5. Bayesian reasoning does not allow us to change this and assert a fact like NewToy yes and then make an inference about the probability of the child being happy. This would amount to changing our belief about the probability of getting a new toy. And these beliefs would have been formed via experience either subjectively, or via conducting a set of experiments from a frequentist perspective. The inference that we can make with a rule like Modus Ponens is latently captured in the joint probability statement, P(Happy yes NewToy yes) 0.8 The statement itself captures the essence of reasoning done with Modus Ponens and says the given that NewToy yes, the probability of Happy yes being true is 0.8. The analogy is applicable because P(Happy NewToy) is an (almost) true statement. Here, we are making a prediction that if a new toy is given to a child, then the child will be happy because new toys make children happy. On the other hand, one might know that the child is happy, and might be curious about what has made the child happy. This is the problem of diagnosis. Let us say that there are several reasons a child could be happy it could be an inherently happy nature the child has, or due to the joy of visiting an uncle, or having read a new book, eating an ice cream, etc. (and this would need an appropriately sized, multidimensional joint probability table). Each such case becomes a hypothesis for our investigating agent, and the application of the Bayes rule as described earlier comes to t