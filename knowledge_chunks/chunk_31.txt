solved. Their work found a home in the Carnegie Mellon University (CMU). It was first implanted in a production system language, OPS5, that was used to build expert systems. Subsequently, John Laird and Paul Rosenbloom built a general symbolic problem solving architecture known as SOAR, which is a popular tool now. Neural networks, the emergent systems approach to problem solving that believes that the sum of interconnected simple processing elements is more than its part, too made their first appearance in the early years. Unlike classical Al systems that are designed and implemented in a topdown manner, neural networks are built by connecting the neurons according to a certain architecture, and then learning the weights of these connections by a process of training. It is in these weights, that all the knowledge gets captured, and it is generally not straightforward to interpret the weights in a meaningful manner. That is why we often call them subsymbolic systems. The first of these was a system called Perceptron built in 1957 by Frank Rosenblatt (1928-1971) at Cornell University. The Perceptron was essentially a single layer feed-forward neural network, and could learn to classify certain patterns. However, Minsky and Papert (1969) gave a mathematical proof that the Perceptron could handle only a simpler class of patterns, which proved to be a dampener for neural networks. It was only in the mid-eighties, with the revival of the Backpropagation algorithm for training multilayer neural networks by Rumelhart and Co. that research in neural network came again to the fore, and with bigger and faster machines available, was quite a rage amongst researchers in the nineties. Meanwhile, John McCarthy, focused on logic in computer science, and proposed a system called Advice Taker in 1958, which was to use logic as a vehicle for knowledge representation and commonsense reasoning. He also invented Lisp, the programming language of choice for Al practitioners, based on Alo