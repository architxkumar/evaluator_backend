at grows only as quadratic. The reader must keep in mind that in many search spaces, the sizes of successive layers are multiples of preceding layers, leading to exponential growth. However, figures like Figure 5.18 above are useful for visualizing algorithms and we will continue to use them. We will also see problems where the growth in search space is quadratic, which are better illustrated by these figures, and in which there is a combinatorial growth of choices. Time complexity by itself can only improve with a better heuristic function, or at the expense of admissibility. For example, many researchers experiment with a variation known as Weighted A which uses the following function to order the search nodes, fin) g(n) k h(n) The factor k is used to control the pull of the heuristic function. Observe that as k tends to zero, the algorithm is controlled by g(n) and it tends to behave like Branch Bound. On the other hand, as we choose larger and larger values of k, the influence of h(n) on the search increases more and more, and the algorithm tends to behave more like Best First Search. With values of k greater than one, the guarantee of finding the optimal solution goes, but the algorithm explores a smaller portion of the search space. The issue of space complexity can be addressed, though at the cost of additional time. We look at some algorithms that require much lower space in the following sections. 5.7 Iterative Deepening A (IDA ) Algorithm DA (Korf, 1985a) is basically an extension of DFID algorithm seen earlier in Chapter 2. IDA is to A what DFID was to DFS. It converts the algorithm to a linear space algorithm, though at the expense of an increased time complexity. It capitalizes on the fact that the space requirements of Depth First Search are linear. Further, it is amenable to parallel implementations, which would reduce execution time further. A simple way to do that would be to assign the different successors to different machines, each extending diff