 space. If we had chosen k 2 then each candidate would have ten new neighbours (we can choose two bits in 5C2 ways). For 01010, they are: 10010, 11110, 11000, 11011, 00110, 00000, 00011, 01100, 01110, and 01001. The above example gives us an interesting insight into designing search spaces. For the same search space, or the set of all possible candidate solutions, different neighbourhood functions can be defined by choosing different operators. This would obviously affect the performance of the search algorithm, because all algorithms consider the set of neighbours to select a move. A sparse neighbourhood function would imply fewer choices at each point, while a dense function would mean more choices. The more dense the neighbourhood, the more expensive it is to inspect the neighbours of a given node. As an extreme in the SAT problem, one could choose an operator that changes all subsets of bits. This would mean that al nodes in the search space would become neighbours of the given node, and the search would then reduce to an inspection of all the candidates. Notice that with this all-subsets exchange, there is no notion of a local optimum. When ail the candidates are neighbours, the best amongst them is the optimum, and that is the global optimum. Conversely, the more sparse the neighbourhood function, the more likelihood of there being a local optima in the search space. The local optima arise because the node (the local optimum) does not have a better neighbour. That is, better nodes exist in the search space; but the local optimum is not connected to any of them. The above realization leads to a simple extension of the Hill Climbing algorithm, known as the Variable Neighbourhood Descent. 3.6 Variable Neighbourhood Descent In the previous section, we saw that one can define different neighbourhood functions for a given problem. Neighbourhood functions that are sparse lead to quicker movement during search, because the algorithm has to inspect fewer neighbours. Bu