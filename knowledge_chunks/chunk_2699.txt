everseen http before doesnotmeanthatourmodelshould claimthatit isimpossi-
ble. Thus, we will adjust our language model so that sequences that have a count of zero in
thetraining corpuswillbeassignedasmallnonzeroprobability (andtheothercountswillbe
adjusted downward slightly sothat theprobability still sumsto1). Theprocess od adjusting
theprobability oflow-frequency countsiscalled smoothing.
SMOOTHING
Thesimplesttypeofsmoothingwassuggestedby Pierre-Simon Laplaceinthe18thcen-
tury: hesaidthat,inthelackoffurtherinformation,ifarandom Booleanvariable X hasbeen
falseinallnobservationssofarthentheestimatefor P(X true)shouldbe1 (n 2). That
is,heassumesthatwithtwomoretrials, onemightbetrueand onefalse. Laplacesmoothing
(alsocalledadd-onesmoothing)isastepintherightdirection,butperformsrelativelypoorly.
Abetterapproachisabackoffmodel,inwhichwestartbyestimatingn-gramcounts,butfor
BACKOFFMODEL
anyparticularsequencethathasalow(orzero)count,webackoffto(n 1)-grams. Linear
LINEAR
interpolation smoothing is a backoff model that combines trigram, bigram, and unigram
INTERPOLATION
SMOOTHING
modelsbylinearinterpolation. Itdefinestheprobability estimateas
P (c i c i 2:i 1 ) 3 P(c i c i 2:i 1 ) 2 P(c i c i 1 ) 1 P(c i ),
where 1. The parameter values can be fixed, or they can be trained with
3 2 1 i
an expectation maximization algorithm. It is also possible to have the values of depend
i
on the counts: if we have a high count of trigrams, then we weigh them relatively more; if
onlyalowcount,thenweputmoreweightonthebigramandunigrammodels. Onecampof
researchers has developed evermore sophisticated smoothing models, while the other camp
suggestsgatheringalargercorpussothatevensimplesmoothingmodelsworkwell. Bothare
gettingatthesamegoal: reducing thevarianceinthelanguage model.
Onecomplication: note that the expression P(c i c i 2:i 1 ) asks for P(c 1 c-1:0 ) when
i 1, but there are no characters before c . We can introduce artificial characters, for
1
example, defining c to be a space charact