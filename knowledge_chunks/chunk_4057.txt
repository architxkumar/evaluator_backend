ting all of the combinations of conditional probabilities. But our goal is not to have to do that. Dempster-Shafer theory lets us handle interactions by manipulating sets of hypotheses directly. The key function we use is a probability density function, which we denote as m. The function m is defined not just for elements of but for all subsets of it (including singleton subsets, which correspond to individual elements). The quantity m(p) measures the amount of belief that is currently assigned to exactly the set p of hypotheses. If contains n elements, then there are 2 subsets of . We must assign m so that the sum of all the m values assigned to the subsets of @ is 1. Although dealing with 2" values may appear intractable, it Usually turns out that many of the subsets will never need to be considered because they have no significance in the problem domain (and so their associated value of m will be 0). Let us see how m works for our diagnosis problem. Assume that we have no information about how to choose among the four hypotheses when we start the diagnosis task. Then we define m as: {O} (1.0) All other values of m are thus 0. Although this means that the actual value must be some one element All, Flu, Cold, or Pneu, we do not have any information that allows us to assign belief in any other way than to say that we are sure the answer is somewhere in the whole set. Now suppose we acquire a piece of evidence that suggests (at a level of 0.6) that the correct diagnosis is in the set { Flu, Cold,Pneu}. Fever might be such a piece of evidence. We update m as follows: { Flu, Cold, Pneu} (0.6) {9} (0.4) At this point, we have assigned to the set { Flu, Cold,Pneu} the appropriate belief. The remainder of our belief still resides in the larger set . Notice that we do not make the commitment that the remainder must be assigned to the complement of {Flu,Cold, Pneu}. Having defined m, we can now define Bel(p) for a set p as the sum of the values of m for p and for all of its