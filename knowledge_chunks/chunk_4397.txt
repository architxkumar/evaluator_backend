 weights each time it sees an input-output pair. Each pair requires two stages: a forward pass and a backward pass. The forward pass involves presenting a sample input to the network and letting activations flow until they reach (he output layer. During the backward pass, the network s actual output (from the forward pass) is compared with the target output and error estimates are computed for the output units. The weights connected to the output units can be adjusted in order to reduce those errors. We can then use the error estimates of the output units to derive error estimates for the units in the hidden layers. Finally, errors are propagated back to the connections stemming from the input units. Unlike the perceptron learning algorithm of the last section, the backpropagation algorithm usually updates its weights incrementally, after seeing each input-output pair. After it has seen all the input-output pairs (and adjusted its weights that many times), we say that one epoch has been completed. Training a backpropagation network usually requires many epochs. Refer back to Fig, 18.14 for the basic structure on which the following algorithm is based. Algorithm: Backpropagation Given: A set of input-output vector pairs. Compute: A set of weights for a three-layer network that maps inputs onto corresponding outputs. 1. Let A be the number of units in the input layer, as determined by the training input vectors. Let C be the number of units in the output. Now choose B, the number of units in the hidden layer.* As the input and hidden layers each have an extra unit used for thresholding therefore, the units in these layers will sometimes be indexed by ranges (0, ..., A) and (0, ..., B). We denote the activation levels of the units in the input layer by .;, in the hidden layer by #;, and in the output layer by o, Weights the input layer to the hidden layer are denoted by w1,, where the subscript i indexes the input units and j indexes the hidden units. Likewise, weights