ner that makes no a priori assumptions regarding the identity of the target concept has no rational basis for classifying unseen instances (Mitchell, 1997). Because inductive learning requires some form of prior assumption (expectation) or inductive bias, we will find it useful to characterise different learning approaches by the inductive bias they employ. Does that mean that it is not possible to learn more complex target concepts? No, one can, but with a different kind of bias, as we see in the next section. 18.5 Decision Trees Decision trees, also known as discrimination trees, or many sorted trees adopt a different approach to concept learning. Instead of trying to learn a specific concept, they accept a training set labelled with different class labels (concept names) and build a discrimination structure that separates the different classes or concepts. Given that the elements of the domain are described by attributes, the objective is to uncover what combination of attributes defines a given concept. Another way of looking at this is to ask which attributes are characteristic of a given class, and which discriminate it from other classes. Decision trees, when constructed, ask a series of questions of the given new instance to be classified, and the answer to each question leads to traversal down the corresponding branch, eventually culminating in a class label. The process is comparable to the manner in which a physician asks a patient questions, where the next question depends upon the previous answer, leading to a diagnosis. We often make these kinds of generalizations implicitly. A teacher may believe that a student with high attendance and serious countenance is a good student, and a tennis player may conclude that a sunny day with normal humidity is perfectly suited to play tennis. While we arrive at these conclusions by a process of accumulated experience, decision tree building algorithms require that all the data be available along with their class la