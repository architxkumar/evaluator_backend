rpus of raw unlabeled sentences? It is
still possible to learn a grammar from such a corpus, but it is more difficult. First of all,
weactually have twoproblems: learning the structure ofthe grammarrules and learning the
896 Chapter 23. Natural Languagefor Communication
probabilitiesassociatedwitheachrule. (Wehavethesamedistinctioninlearning Bayesnets.)
We ll assume that we re given the lexical and syntactic category names. (If not, we can just
assume categories X ,...X and use cross-validation to pick the best value of n.) We can
1 n
then assume that the grammar includes every possible (X Y Z) or (X word) rule,
although manyoftheseruleswillhaveprobability 0orclose to0.
Wecanthenuseanexpectation maximization(EM)approach,justaswedidinlearning
HM Ms. Theparameters wearetrying tolearn are therule probabilities; westartthem offat
random oruniform values. Thehidden variables aretheparse trees: wedon t know whether
astringofwords w ...w isorisnotgenerated byarule(X ...). The Estepestimates
i j
the probability that each subsequence is generated by each rule. The M step then estimates
theprobability ofeachrule. Thewholecomputationcanbedoneinadynamic-programming
INSIDE OUTSIDE fashion with an algorithm called the inside outside algorithm in analogy to the forward ALGORITHM
backwardalgorithm for HM Ms.
Theinside outsidealgorithmseemsmagicalinthatitinducesagrammarfromunparsed
text. Butithasseveraldrawbacks. First,theparsesthatareassignedbytheinducedgrammars
areoften difficult tounderstand andunsatisfying tolinguists. Thismakes ithard tocombine
handcrafted knowledge with automated induction. Second, itis slow: O(n3m3), where nis
the number of words in a sentence and m is the number of grammar categories. Third, the
space of probability assignments is very large, and empirically it seems that getting stuck in
localmaximaisasevereproblem. Alternativessuchassimulatedannealing cangetcloserto
the global maximum, at a cost of even more computation. Lari and Young (1990) conclude
that