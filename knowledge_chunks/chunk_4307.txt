problems, but then negotiation occurs to decide what agents wil] take responsibility for which subtasks. No one agent is in charge, although there is a single shared goal among all the agents. They must cooperate both in forming a plan and in executing it. No one agent is in charge, and there is no guarantee that a single goal will be shared among all the agents. They may even compete with each other. 2In this respect, reasoning programs are no different from other large programs [Dijkstra, 1972]. 338 Artificial Intelligence RSENS DATES SSSR METI Although these approaches differ considerably, there is one modification to a simple, single agent view of reasoning that is necessary to support all of them in anything other than a trivial way. We need a way to represent models of agents, including what they know, what they can do, and what their goals are. Fortunately, what we need is exactly the set of mechanisms that we introduced in Section 15.4. But now, instead of using modal operators and predicates (such as BELIEVE. KNOW, KNOW-WHAT, CAN-PERFORM, and WILLING-TOPERFORM) to model writers and speakers, we use them to model agents in a distributed system. Using such operators, it is possible for each agent to build a model of both itself and the other agents with which it must interact. The self-descriptive model is necessary to enable the agent to know when it should get help from others and to allow it to represent itself accurately to other agents who may wish to get help from it. The model of other agents is necessary to enable an agent to know how best to get help from them. Planning for Multi-agent Execution The least distributed form of distributed reasoning is that in which a single agent: 1. Decomposes the goal into subgoals, and 2. Assigns the subgoals to the various other agents This kind of reasoning is usually called multi-agent planning. The first step, problem decomposition, is essentially the same as it is for single-agent planning systems. Ideally the 