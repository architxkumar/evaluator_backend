..., Nk, N), where k is the size of the vocabulary, ny, is the number of times word w appears in the document d and N 2; n;. (b) Maximum Likelihood Estimation assuming P(x y) . Bernoulli(i, p), where p is the parameter of the Bernoulli distribution and i 0, 1 . In our case, we would have k Bernoulli distributions. (c) Bayesian Parameter Estimation assuming that prior P(y) Dir(), where a (a4, da, ..., a), the elements of the vector are the parameters of the Dirichlet distribution. (d) Bayesian Parameter Estimation, assuming that prior P(y) 7 Beta(a, B), where a and B are the parameters of the Beta distribution. 3. Design an HMM for aligning two protein sequences. 4. Design an HMM for aligning multiple protein sequences. 5. Implement an HMM for modelling a sequence of heads and tails generated by two coins: fair and biased coins. (a) Implement Viterbi and forward algorithm for the problem. (b) Implement HMM training via MLE and Baum Welch algorithm by using suitable training examples. 6. Implement the K-means clustering algorithm, and try it on a sample data set. 7. Given the following training data of protein sequences and their labels, Table 18.9 Protein sequences and labels Sequence Label ADEEF 1 ADCFY 1 ADEFG 1 CGNSS 2 CDEES 2 DECES 2 3 4 EFHHT FFTTH KKRTY KGGTR 4 (a) Model each sequence as described in this example and estimate the model parameters. (b) Use the estimated model parameters to assign an appropriate class label for a new sequence AADEF. 8. For the example in Section 18.2, what is the probability that Anisa s sprinkler is on, given that the grass in both her as well as Malala s lawn is wet. That is, compute the value of P(Xs5 1 X4 1, Xj 1). 9. Devise the algorithm reconstructVS(G,S) described in Figure 18.9. The task is to find all those hypotheses that are more general than some hypothesis in S, and less general that some hypothesis in G. 10. Show that the set of constraints Low, P, Yes v Medium, P, Yes High, ?, Yes as found by the ID3 algorithm is e