arning. . _. . In competitive learning, output units fight for Fig. 18.19 A Competitive Learning Network control over portions of the input space. A simple competitive learning algorithm is the following: 1. Present an input vector. 2. Calculate the initial activation for each output unit. 3. Let the output units fight until only one is active. 4. Increase the weights on connections between the active input units. This makes it more likely that the output unit will be time the pattern is repeated. One problem with this algorithm is that one output unit may learn to be active all the time it may claim all the space of inputs for itself. For example, if all the weights on a unit s input lines are large, it will tend to bully the other output units into submission. Learning will only further increase those weights. The solution, originally due to Rosenblatt (and described in Rumelhart and Zipser [1986]), is to ration the weights. The sum of the weights on a unit s input lines is limited to 1. Increasing the weight of one connection requires that we decrease the weight of some other connection. Here is the learning algorithm. Algorithm: Competitive Learning Given: A network consisting of n binary-valued input units directly connected to any number of output units. Produce: A set of weights such that the output units become active according to some natural division of the inputs. 1. Present an input vector, denoted (,x;, 43, ....%,). 2. Calculate the initial activation for each output unit by sum of its inputs.! 3. Let the output units fight until only one is active.!! 4. Adjust the weights on the input lines that lead to the single active output unit. x; . Aw,= 1 ~My forall j=1,....n '0 There is no reason to pass the weighted sum through a sigmoid function, as we did with backpropagation, because we only calculate activation levels for the purpose of singling out the most highly activated output unit. 11 Ag mentioned earlier, any, method for determining the most highly 