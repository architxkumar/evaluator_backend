babilistic reasoning, however, we need more information. In particular, we need to know, for each value of a parent node, what evidence is provided about the values that the child node can take on. We can state this in a table in which the conditional probabilities are provided. We show such a table for our example in Fig. 8.3. For example. from the table we see that the prior probability of the rainy season is 0.5. Then, if it is the rainy season, the probability of rain on a given night is 0.9; if it is not, the probability is only 0.1. Attribute Probability p{Wet\Sprinkler, Rain) 0.95 P(Wet\Sprinkler, ~Rain) 0.9 p(Wei\-Sprinkler, Rain} 0.8 p(Wei\-Sprinkler, =Rain) 0.1 Pp Sprinkler\Rainy Season) 6.0 p{ Sprinkler\~Rainy Season) 1.0 p{Rain \Rainy Season) 09 p(Rain \ =Rainy Season) 0.1 p{ Rainy Season) 0.5 Fig. 8.3. Conditional Probabilities for a Bayesian Network To be useful as a basis for problem solving, we need a mechanism for computing the influence of any arbitrary node on any other. For example, suppose that we have observed that it rained last night. What does that tell us about the probability that it is the rainy season? To answer this question requires that the initial DAG be converted to an undirected graph in which the arcs can be used to transmit probabilities in either direction, depending on where the evidence is coming from. We also require a mechanism for using the graph that guarantees that probabilities are transmitted correctly. For example, while it is true that observing wet grass may be evidence for rain, and observing rain is evidence for wet grass, we must guarantee that no cycle is ever traversed in such a way that wet grass is evidence for rain, which is then taken as evidence for wet grass, and so forth. There are three broad classes of algorithms for doing these computations: a message-passing method [Pearl], 1988], a clique triangulation method {Lauritzen and Spiegelhalter, 1988], and a variety of stochastic algorithms. The idea behind