as the best choice at each node. In the forward phase, the algorithm picks a node from the fringe of the best partial solution and expands it (like in AO ). In the backward phase (lines 9-10), it adopts a dynamic programming approach to update costs. Taking a cue from (Mausam and Kolobov, 2012), we have restricted this dynamic programming to Policy Iteration. The interested reader is referred to (Hansen and Zilberstein, 2001) for a version that includes Value Iteration as well. LAO (state space S, start state Start, action set A, costs C, ) 1 Ge Start 2 G 6 3. Fringe Start Forward phase 4 while Fringe 5 do remove some node N from Fringe expand N and add its new children C to G 7 compute the heuristic value of each child inc 8 Cost revision g Z N VU ancestors of N in G, 10 perform Policy Iteration on Z 1d pack to Forward phase 12 Gp subgraph with hyper-edges marked by best partial policy 13 Fringe leaves of G, that are not goal states 14 return G, Figure 17.38 Like AO , the algorithm LAO works in two phases. In the forward phase, it follows hyper-edges marked by the best partial policy, expands a nonterminal node, and evaluates the heuristic values of the children. In the backward phase, it adopts a different approach for cost revision, since the graph may have loops. The cost revision process used here is Policy Iteration. The algorithm terminates when the graph for the best policy has no leaf nodes. Observe that the set Z on which dynamic programming is done to solve for values includes (line 9) only the expanded node N and its ancestors in the solution graph being constructed. It excludes the children C of node N. The reason for not including the children is that their value is determined by the heuristic function and cannot be changed in the process of running the dynamic programming procedure . At the same time, the children do affect the value of N during the procedure, and through N, the values get propagated to the ancestors of N as well. The LAO algorithm ta