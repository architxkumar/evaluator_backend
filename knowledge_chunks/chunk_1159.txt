constructed by exploiting domain knowledge. In general, a Bayesian belief network over N variables is characterized by its joint distribution, defined as the product of the probabilities of all its variables conditioned on their parents, N PX, ..., Xy) i P(X, Parents;) Given the Bayesian network and the conditional probabilities between the linked variables (in both directions), one could estimate the probabilities of some unknown variables given some known variables. In this context, the notion of conditional independence is very valuable, and fortunately can be determined simply by inspecting the state of the network and its observed variables. The statement of conditional independence is as follows. Let X, Y and Z be three random variables and let x, y and z represent the three values that the variable can take. Then if the conditional distribution of X given the value of Z does not depend upon Y, depicted by, P(x 3. ) P z) we say that the X is conditionally independent of Y given Z. In other words, if one knows the value of the variable Z, then the probability of any value of the variable X does not depend upon the probability of any value of Y. We write this property as (X,Z, Y) as in (Pearl, 1988). The relation is symmetric. That is, (X, Z, Y) I(Y, Z, X). The notation due to Dawid (1979) is also used. In the special case when the two variables are unconditionally independent, like two coin tosses, then we write I(X, , Y). The relation can be extended to sets of variable x , Y and Z . Another way of looking at conditional independence is as follows. Consider the joint probability of X and Y, given the value of variable X. We then have, P(x, y 2) P(x y, ) P(y ) by the product rule P(x ) PQ ) by conditional independence. This says that if we know the value of Z then the joint probability of X and Y is simply the product of their marginal properties. An interesting feature of BBNs is that many such conditional relations can be seen from the graphical point of view