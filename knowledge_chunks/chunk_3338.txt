estion then is where does one obtain such a large number of reliable probabilities? To Simplify equation 6.8, it is sometimes assumed that the E1 are Statistically independent. In that case, the numerator and denominator probability terms P(E1. E2..... factor into 112 Probabilistic Reasoning Chap. 6 P(E,H1) P(E1IH,)P(E2IH,). . . resulting in a somewhat simpler form. But, even though the computations are straightforward, the number of probabilities required in a moderately large system can still be prohibitive, and one may be forced to simply use subjective probabilities when more reliable values are not available. Furthermore, the E are almost never completely independent. Consequently, this assumption may introduce intolerable errors. The formulas presented above suggest how probabilistic evidence would be combined to produce a likelihood estimate for any given hypothesis. When a number of individual hypotheses are possible and several sources of evidence are available, it may be necessary to compute two or more alternative probabilities and select among them. This may mean that none, one, or possibly more than one of the hypotheses could be chosen. Normally, the one having the largest probability of occurrence would be selected, provided no other values were close. Before accepting such a choice; however, it may be desirable to require that the value exceed some threshold to avoid selecting weakly supported hypotheses. In Section 6.5 we describe a typical system which combines similar values and chooses only those conclusions that exceed a threshold of 0.2. Bayesian Networks Network representations of knowledge have been used to graphically exhibit the interdependencies which exist between related pieces of knowledge. Much work has been done in this area to develop a formal syntax and semantics for such representations. We discuss related topics in some detail in Chapter 7 when We consider associative networks and conceptual graphs. Here, however, we are more inte