ror surface is a vector made up of the partial derivatives of the error in the dimension of each weight. When we want to minimize the error value, the weight adjustment must be done in the opposite direction. Thus, the weight adjustment rule can be written as, we w,-1 oe (18.61) The partial derivative used here can in turn be obtained by differentiating Eq. (18.59) and substituting 2-9, w; x; for zx . Observe that only the term will remain in the final equation and the negative sign will cancel giving the delta rule, w, Wt Lye 7 (ty y) ix (18.62) where X;y is the i input of the training example XeT. This can be written as, w, Ww, Aw; (18.63) where Aw; 1 Zyer (ty 2x) ix (18.64) The algorithm that uses the above delta rule will have to compute the linear sum for each training example XCET, and add up the terms of the form n (tx-Zx) x, for each X to compute Aw;. The algorithm is given in Figure 18.23. PerceptronDeltaRule (Training Set: T, Attributes: x,..x, , Learning rate: n) 1 for ie 0 ton we 0 initializing weights repeat for ie 0 ton Aw. 0 for each X x,, .., , in T t training class label i linear sum for X for ie 0 ton Aw, Aw, n (t - 1) x, for ie 0 ton we w, Aw, 2 until convergence 13 return W W , ., IVE OW MYMUHHBwWW 1 1 1 1 FIGURE 18.23 The Perceptron training algorithm with the delta rule computes the partial derivatives in error E over the entire set of training data before adjusting the weights once. It repeats this process until there is no change in the weights. In the process, it minimizes the cumulative error over the training data. The algorithm using the delta rule is more robust because it always converges to the minimum error weights. However, it does this at a greater computational cost because before making each adjustment, it inspects the entire training data, whereas the error correcting rule makes adjustments, looking at each individual example. Learning Boolean Functions Neural networks in general and Perceptrons in particular can also be employed