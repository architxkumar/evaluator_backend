t. In each iteration, it sweeps over the entire state space updating the value function V,, in the n' iteration, based on the value function V, ,. The update procedure is known as the Bellman update or the Bellman backup. Via (s) mittgeg Lies Pls, a, 8 C(s, a, 5 ) V,(s ) The reader is encouraged to compare the Bellman backup rule with the iterative policy evaluation rule (Section 17.6.1.2). The main difference is that there a policy is specified and its value has to be evaluated; while in the Bellman backup, one does not have access to a policy and has to search for both the optimal policy and the corresponding, optimal value function simultaneously. The Value Iteration procedure ends with the optimal value function V . Given the optimal value function, the optimal policy m can be constructed by the greedy approach that constructs 1 given V, described in the preceding section. The algorithm adapted from (Mausam and Kolobov, 2012) is described in Figure 17.35. First, the policy function is initialized randomly. Then, the Bellman backup is applied iteratively till the maximum residual for any state becomes less than a specified value e (lines 4-9). The algorithm returns the optimal policy constructed by the greedy approach described above. Value-iteration (state space S, action set A, costs C, ) 1 neo 2 for k 1 to ISI 3 do V,(s,) random-value initialize the values randomly 4 repeat 5 nenel 6 for k 1 to S 7 do Compute V,(s,) using Bellman backup 8 Compute Residual,(s,) Va(S ) - Vnor (Se) 9 until max... Residual,(s) 10 return WseS a (s) argmingg, Eyes P(S, a, S )(C(s, a, 8 ) V(s ) Figure 17.35 The Value Iteration algorithm begins by initializing the value function for each state randomly. Then it applies the Bellman update to each state till all states have reached the desired threshold for the residual. It then constructs and returns the greedy policy mV for the value function. As discussed above, the Value Iteration algorithm is similar to the iteration process used f