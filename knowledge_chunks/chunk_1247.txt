hapter 15. Nevertheless, a concept description can be constructed by inspecting the tree. Every path from the root to the concept is a conjunction of constraints on a set of attributes on the path. The different paths together are different sets of alternative sets of constraints, and can be joined together in a disjunction. Let us look at the concept VeryHigh in our training set. The leftmost path in the figure represents the constraints Exp-Symb Low, Education PhD, HandsOn Yes which we can write as L,P,Y . The second one represents Exp-Symb Medium, HandsOn Yes, Education PhD which in the order of our schema is M,P,Y . The rightmost path represents the constraints Exp-Symb High, HandsOn Yes . Since it does not say anything about Education, we can assume that any value would do and we can represent the constraints as H,?,Y . Thus, the hypothesis that matches the training set is, Low, P, Yes v Medium, P, Yes v High, ?, Yes as found by the ID3 algorithm. The reader is encouraged to verify that this can be simplified to ?, P, Yes High, ?, Yes . This essentially says that to be in the VeryHigh salary category, one needs to be Hands-On and in addition either have a PhD degree or High levels of experience. The question is how does ID3 algorithm working with a complete hypotheses space produce a structure that can be used to classify unseen instances? The answer is that because it has a bias towards smaller trees. This bias is a preference bias in which the algorithm prefers smaller trees. If the data set had elements with many attributes then the hypotheses space would be large. One can then alter the termination criterion that prevents the tree from becoming too deep. This could be done by stopping the partitioning process as soon as the majority of the elements in a node are of one class. One can also include a pruning step that removes nodes with a very small number of training instances. The algorithm ID3 is described in Figure 18.12. The algorithm is recursive in nat