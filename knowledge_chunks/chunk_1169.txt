s policy differs from the previous one is in the action to be taken in state sj. In all other states, the actions are identical. This is not surprising, given that these are the only two that are likely to be the desired policies. The reader is encouraged to verify that all other policies (for example the one with TT9 (S1) a43) are not going to work. The other policies are not likely to be of interest. Nevertheless, the above arguments were made at a qualitative level. We need a quantitative approach that will choose the best policy from the set of all possible policies. This will also involve choosing between one of tog and TT5g amongst others. Given the utility function, one can then look at a policy that maximizes the overall expected utility obtained by applying the policy. The first step however is to evaluate a given policy. How to Evaluate a Policy? If one is to look at different policies in search of the optimum policy, one needs a mechanism to evaluate any policy. A policy can be depicted by a policy hyper-graph in which exactly one directed hyper-edge, representing the prescribed action, emanates from each state, and ends in all the states that the action could end up in. Consider first a policy that has no cycles. Consider a simpler version of the problem from Figure 17.29 shown in Figure 17.30 in which there are only four states, s1, So, Ss and Sg where Sg, as before, is the goal state. We have removed some states from the original problem to allow us to select a proper policy. A proper policy is one in which an agent is guaranteed to reach a goal state. If we had left states s3, s4, Sg and s7 in, and removed the backward moves, then these states could have been dead-end states. An SSP with dead-end states requires greater sophistication because in addition to the cost that one has to minimize, one will also have to take into account the probability of reaching a goal state, and perhaps a trade off between the two criteria. Let agent choose a policy 77 f