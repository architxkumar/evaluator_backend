 done by playing as many games as possible. The evaluation function e(J) looks at a board position and returns a number. The task of learning is to update the weights used in the evaluation function to take into account the outcome of the game. Let B,, Bo, ..., Bn be the sequence of board positions in which MAX makes n moves before the game ends. Note that the game could end with either MAX or MIN making the last move, but at the end of the game the outcome is known. Let e(B,), e(Bo), ..., e(Bp) . Let 6(B)) the target or training value of that the learning system wants to use as a new estimate of the evaluation function based on the game played. A simple way of obtaining this training value has turned out to be is to use the current estimate of the board position e(Bj1;) where MAX chooses next as the training value (Mitchell, 1997). That is, eB) e(Bj 1) (18.51) Consider the last move made in the game. For simplicity, let us assume that MAX made the last move B,. Now since MAX chooses the best successor state, it can be assumed that the value of 6(B,) can safely be assumed to be the outcome. After updating the value of state B,, this revised value can be used as the revised value for the preceding node Bn-1The idea of reducing the temporal difference is to adjust the weights of the evaluation by a controlled amount in the direction that reduces this difference. That is, for each weight w;, the updated value is computed as, Ww, w, 1) (E(B) e(B)) v; (18.52) Observe that the term (6(B;) e(B))) v; could be both positive or negative, and is proportional to the value v; that is the contribution of the i feature to the evaluation function. If 6(B;) e(B;), that is the revised estimate is higher, and therefore better for MAX, the weights that contributed to the evaluation are increased in proportion to the value of the feature v;. On the other hand, if the outcome is worse for MAX then the term is negative and the weights that contributed more are decreased. The parameter n i