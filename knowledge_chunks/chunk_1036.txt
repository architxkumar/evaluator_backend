f U are the eigenvectors of M, and is a diagonal matrix whose diagonal elements are eigenvalues of M arranged in decreasing order. This result is due to Matrix Diagonalization Theorem (Strang, 2009) and applies to square matrices, but not to rectangular ones like the document-word matrix. Previous attempts at factor analysis applied the idea to word-word matrices or document-document matrices, which are square. This is referred to as single-mode factor analysis. In contrast, two-mode factor analysis starts off with a rectangular word document matrix M of dimensions m x n (corresponding to m words and n documents), and rank r. The key apparatus is the Singular Value Decomposition (SVD) of M, which is given by: M UxVT where, U is an m x m matrix whose columns are orthogonal eigenvectors of MMT. Vis ann x n matrix whose columns are orthogonal eigenvectors of M M. The eigenvalues Ay, Ag, ..., A, of MMT are the same as eigenvalues of M M. The square root of these r eigenvalues, called singular values, are arranged in descending order along the diagonal of the matrix , all other elements of which are set to 0. We have seen before that small eigenvalues contribute less to the effect of the action of a matrix M on vectors. Extending this intuition to SVD, it is interesting to see the effect of considering only the top k singular values, and discarding the rest (flipping them to 0). Thus, the matrix 2 is shrunk to a k x k diagonal matrix X,. We also delete the columns corresponding to low (and zero) singular values in U and V to obtain U and V respectively. U, 2, and V can now be combined to yield M U2,v7 (16.2) is a k-rank approximation to M This result is pivotal to our discussion of LSI below. Firstly, we note that SVD achieves dimensionality reduction. Let M be a document-word matrix, with each row representing a document. Geometrically, the rows of U and V are co-ordinates of points corresponding to documents and words mapped onto a k-dimensional space. Typically, the a