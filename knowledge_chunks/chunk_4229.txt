 to produce a parse, almost all the systems that are actually used have two main components: e A declarative representation, called a grammar, of the syntactic facts about the language A procedure, called a parser, that compares the grammar against input sentences to produce parsed structures 15.2.1. Grammars and Parsers The most common way to represent grammars is as a set of production rules. Although details of the forms that are allowed in the rules vary, the basic idea remains the same and is illustrated in Fig. 15.6, which shows a simple context-free, phrase structure grammar for English. Read the first rule as, A sentence is composed of a noun phrase followed by a verb phrase. In this grammar, the vertical bar should be read as or. The 292 Artificial Intelligence S eaiaeeaee ee denotes the empty string. Symbols that are further expanded by rules are cailed nonterminal symbols. Symbols that correspond directly to strings that must be found in an input sentence are called terminal symbols, S NP VP NP the NP1 NP > PRO NP > PN NP ~ NP1 NP1 > ADJS N ADJS > | ADJ ADJS VP3V VP - VNP N file | printer PN > Bill PRO 1 ADJ = short | long | fast V printed | created { want Fig. 15.6 A Simple Grammar for a Fragment of English Grammar formalisms such as this one underlie many linguistic theories, which in turn provide the basis for many natural Janguage understanding systems. Modem linguistic theories include: the government binding theory of Chomsky [1981; 1986], GPSG [Gazdar et a/., 1985], LFG [Bresnan, 1982], and categorial grammar [Ades and Steedman, 1982; Oehrle et al., 1987]. The first three of these are also discussed in Sells [1986]. We should point out here that there is general agreement that pure, context-free grammars are not effective for describing natural languages. As a result, natural language processing systems have less in common with computer language processing systems (such as compilers) than you might expect. Regardless of the theoretical basis of the