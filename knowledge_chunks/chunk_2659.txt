hownin Figure21.4. Figure21.5illustrates
theperformance ofthepassive TDagentonthe 4 3world. Itdoesnotlearnquiteasfastas
the AD Pagent and shows muchhigher variability, but itismuch simplerand requires much
lesscomputationperobservation. Noticethat TDdoesnotneedatransitionmodeltoperform
itsupdates. Theenvironment suppliestheconnection betweenneighboring statesintheform
ofobserved transitions.
The AD Papproach andthe TDapproach areactually closely related. Bothtrytomake
local adjustments tothe utility estimates inordertomakeeach state agree withitssucces-
sors. One difference is that TD adjusts a state to agree with its observed successor (Equa-
tion (21.3)), whereas ADP adjusts the state to agree with all of the successors that might
occur, weighted by their probabilities (Equation (21.2)). This difference disappears when
the effects of TD adjustments are averaged over a large number of transitions, because the
frequencyofeachsuccessorinthesetoftransitionsisapproximatelyproportionaltoitsprob-
ability. A more important difference is that whereas TD makes a single adjustment per ob-
served transition, ADP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P. Although the observed transition makes only a
local change in P, its effects might need to be propagated throughout U. Thus, TD can be
viewedasacrudebutefficientfirstapproximation to ADP.
Each adjustment made by ADP could be seen, from the TD point of view, as a re-
sult of a pseudoexperience generated by simulating the current environment model. It
is possible to extend the TD approach to use an environment model to generate several
pseudoexperiences transitionsthatthe TDagentcanimaginemighthappen,givenitscurrent
model. Foreachobservedtransition, the TDagentcangenerate alargenumberofimaginary
1 Thetechnical conditionsaregiven onpage725. In Figure21.5wehaveused (n) 60 (59 n), which
satisfiestheconditions.
838 Chapter 21. Reinforcement Learning
1
0.8
0.6
0.4
0.