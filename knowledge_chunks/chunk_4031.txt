s very much like the constraint satisfaction procedure that we examined in Section 3.5. One problem with this approach is that given a set of n assumptions, the number of possible contexts that may have to be considered is 2 . Fortunately, in many problem-solving scenarios, most of them can be pruned without ever looking at them. Further, the ATMS exploits an efficient labeling system that makes it possible to encode a set of contexts as a single context [A1, A2, A3, A4] that delimits the set. To see how both of these things work, it is necessary to think of the set of contexts that are defined by a set of assump- (At, A2, Ad] (A1, A2, Ad] [A1, A3, Ad] [A2, AS, AA] tions as forming a lattice, as shown for a simple Se example with four assumptions in Fig. 7.13. Lines going upward indicate a subset relation- {A1, A2]_[A4, AS] f A1, Ad) 1A2, AS] [A2, Ad] (AS, Ad] ship. The first thing this lattice does for us is to (At) [Az] (A3] [Aa] illustrate a simple mechanism by which contradictions (inconsistent contexts) can be aw La propagated so that large parts of the space of 2 {} contexts can be eliminated. Suppose that the Fig. 7.13 A Context Lattice 168 Artificial Intelligence context labeled {A2, A3} is asserted to be Inconsistent. Then all contexts that include it (i.e., those that are above it) must also be inconsistent. Now consider how a node can be labeled with all the contexts in which it has a valid justification. Suppose its justification depends on assumption Al. Then the context labeled {Al} and all the contexts that include it are acceptable. But this can be indicated just by saying (A1)}. It is not necessary to enumerate its supersets. In general, each node will be labeled with the greatest lower bounds of the contexts in which it should be believed. Clearly, it is important that this lattice not be built explicitly but only used as an implicit structure as the ATMS proceeds. As an example of how an ATMS-based problem-solver works, let s return to the ABC Mur