rd
(althoughnotimpossible) torepresentasadecisiontree. Ontheright,wehavetherestaurant
example. The solution problem is easily represented as a decision tree, but is not linearly
separable. Thebestplanethrough thedatacorrectlyclassifiesonly65 .
18.7.3 Multilayerfeed-forward neural networks
(Mc Culloch and Pitts,1943)werewellawarethatasinglethreshold unitwouldnotsolveall
their problems. In fact, their paper proves that such a unit can represent the basic Boolean
functions AND, OR,and NOT andthen goes ontoargue that anydesired functionality canbe
obtainedbyconnecting largenumbersofunitsinto(possibly recurrent)networksofarbitrary
depth. Theproblem wasthatnobodyknewhowtotrainsuchnetworks.
This turns out to be an easy problem if we think of a network the right way: as a
function h (x)parameterized bytheweights w. Considerthesimplenetwork shownin Fig-
w
ure 18.20(b), which hastwoinput units, twohidden units, and twooutput unit. (Inaddition,
each unit has a dummy input fixed at 1.) Given an input vector x (x ,x ), the activations
1 2
732 Chapter 18. Learningfrom Examples
h (x,x) h (x,x)
W 1 2 W 1 2
1 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
4 4
0 2 0 2
-4 -2 x 1 0 2 4 -4 -2 0 x 2 -4 -2 x 1 0 2 4 -4 -2 0 x 2
(a) (b)
Figure18.23 (a)Theresultofcombiningtwoopposite-facingsoftthresholdfunctionsto
producearidge.(b)Theresultofcombiningtworidgestoproduceabump.
oftheinputunitsaresetto(a ,a ) (x ,x ). Theoutputatunit5isgivenby
1 2 1 2
a g(w w a w a )
5 0,5, 3,5 3 4,5 4 g(w w g(w w a w a ) w g(w 4 w a w a ))
0,5, 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2 g(w w g(w w x w x ) w g(w 4 w x w x )).
0,5, 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2
Thus, we have the output expressed as a function of the inputs and the weights. A similar
expression holds for unit 6. As long as we can calculate the derivatives of such expressions
with respect to the weights, we can use the gradient-descent loss-minimization method to
train the network. Section 18.7.4 shows exactly how to do this. And because the function
repre