e between them (via B). In the Section below, we briefly describe a Factor Analytic technique called Latent Semantic Indexing (LSI) that mines concepts from a document collection by exploiting higher order associations between terms. Latent Semantic Indexing LS was proposed as a technique for concept extraction in (Deerwester et al., 1990) .The objective is to determine a set of underlying factors or concepts, that best explain the relationship between the terms and documents. This is not very different from the goal of most factor analysis research from the sixties to the nineties. What distinguishes LS from most earlier approaches is its two-mode factor analysis which allows it to express both words and documents in terms of the same underlying concepts. To start with, we have a term document matrix. Each element in that matrix is a weight showing the relevance of the term to the corresponding document. The first significant step in Linear Algebra is to view a matrix, such as this as an operator. This means that the matrix can act upon a vector (when it is multiplied with that vector), and relocate it to a different position. For example, the square matrix? 2-1 1 M -l1 2 -1 1-1 2 can act on the vector l 4 2 3 and move it to a new location given by M4: Ww MA 0 5 In the underlying geometry of the space, the action of a matrix M can be viewed as a combination of translation and rotation of A in the general case. We are interested in characterizing a matrix M formally in terms of its properties that govern its action on vectors; the concept of eigenvectors does precisely that. We consider all vectors x that, when acted on by M, stretch themselves to a different location aX: where Ais a scalar, but do not undergo any rotation. Thus, MX AxX (16.1) The vectors satisfying (16.1) are called eigenvectors, and each of these eigenvectors is associated with a corresponding value of A referred to as an eigenvalue. We rewrite (16.1) as (M Al) 7 0, where is an identity matrix of 