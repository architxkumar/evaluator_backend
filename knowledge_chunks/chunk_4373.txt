rectly with probability 1/4. The complexity of learning a concept is a function of three factors: the error tolerance (#), the number of binary features present in the examples (f), and the size of the rule necessary to make the discrimination (f). If the number of training examples required is polynomial in h, , and f, then the concept is said to be learnable. Some interesting results have been demonstrated for concept learning. Consider the problem of learning conjunctive feature descniptions. For example, from the list of positive and negative examples of elephants shown in Fig. 17.22, we want to induce the description gray, mammal, large. It has been shown that in conjunctive learning the number of randomly chosen training examples is proportional to the logarithm of the total number of features [Haussler, 1988; Littlestone, 1988]. Since very few training examples are needed to solve this induction problem, it is called Jearnable. Even if we restrict the learner o positive examples only, conjunctive learning can be achieved when the number of examples is linearly proportional to the number of attributes [Ehrenfeucht et al., 1989]. Learning from positive examples only is a phenomenon not modeled by least-commitment inductive techniques such as version spaces. The introduction of the error tolerance h makes this possible: After all, even if all the elephants in our training set are gray, we may later encounter a genuine elephant that happens to be white. Fortunately, we can extend the size of our randomly sampled lraining set to ensure that the probability of misclassifying an elephant as something else (such as a polar bear) is an arbitrarily small 1/h. gray? mammal? Jarge? vegetarian? wild? + + + + (Elephant) (Elephant) (Mouse) (Giraffe) (Dinosaur) (Elephant) Fig. 17.22 Six Positive and Negative Examples of the Concept Elephant Formal techniques have been applied to a number of other learning problems. For example, given positive and negative examples of strings