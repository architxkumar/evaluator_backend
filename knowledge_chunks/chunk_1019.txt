 the supervised case, we not have to rely on explicit sense labels from WordNet. Some authors make a terminological distinction to clarify this point, and use word sense discrimination as opposed to word sense disambiguation , while referring to unsupervised WSD. A third and an interesting approach to WSD is to use a corpora of bilingual texts for disambiguation. Words that are polysemous in English are often not polysemous in Hindi, for example. The corresponding words in Hindi can then serve as sense identifiers. If we have a corpus of texts in English and their corresponding Hindi translations, we can use Machine Learning techniques to exploit contextual information to disambiguate polysemous English words. 16.2.5 Part of Speech Tagging Determining the correct Part of Speech (such as noun, verb, adjective, adverb or determiner) of words in a sentence is a critical step for several NLP operations, including parsing. There are broadly two kinds of POS taggers: rule based taggers and stochastic taggers. Rule based taggers rely on a set of hand coded rules such as IF preceding word is DET, THEN current word is NOT VERB where DET stands for determiner (for example the or a ). While rule based taggers are efficient, they involve substantial knowledge acquisition overhead. To overcome this shortcoming, several supervised machine learning approaches have been explored to induce rules from annotated corpora. Stochastic taggers rely on the frequency of tags or sequence of tags, as estimated from a corpus. The simplest scheme is based on the unigram model where the most frequent tag is assigned to a word. A bigram tagger recognizes that sequences such as DET NN are more likely than DET VB . Unlike rule based taggers that operate on a rigid set of rules, stochastic taggers aim at exploiting corpus statistics assigning the most likely sequence of tags to words in a sentence. A very popular scheme for stochastic tagging is the Hidden Markov Model (HMM) tagger. The idea behind 