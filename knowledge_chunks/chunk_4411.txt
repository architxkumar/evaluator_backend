ave as in Hopfield nets, Annealing is the process of gradually moving from a high temperature down to a low temperature. The randomness added by the temperature helps the network escape from local minima. There is a leaning procedure for Boltzmann machines, i.e., a procedure that assigns weights to connections between units given a training set of initial states and final states. We do not go into the algorithm here; interested readers should see Hinton and Sejnowski [1986]. Boltzmann Jearning is more time-consuming than backpropagation, because of the complex annealing process, but it has some advantages. For one thing, it is easier to use Boltzmann machines to solve constraint satisfaction problems. Unlike backpropagation networks, Boltzmann machines do not make a clear division between input and output. For example, a Boltzmann machine might have three important sets of units, any two of which could have their values clamped, or fixed, like the input layer of a backpropagation net activations in the third set of units would be determined by parallel relaxation. If the annealing is carried out properly, Boltzmann machines can avoid local minima and learn to compute any computable function of fixed-sized inputs and outputs. 8 One deterministic variation of Boltzmann learning [Peterson and Anderson, 1987] promises to be more efficient. 392 Artificial Intelligence 18.2.5 Reinforcement Learning What if we train our networks not with sample outputs but with punishment and reward instead? This process is certainly sufficient to train animals to perform relatively interesting tasks. Barto [1985] describes a network which learns as follows: (1) the network is presented with a sample input from the training set, (2) the network computes what it thinks should be the sample output, (3) the network is supplied with a real-valued judgment by the teacher, (4) the network adjusts its weights, and the process repeats. A positive value in step 3 indicates good performance, while a