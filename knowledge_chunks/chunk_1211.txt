e values it takes. The discrete feature takes a value from a finite set of possible values. For example, a feature may take one of the two values from the set: yes, no . The continuous feature takes a value from the set of real numbers. The height of students in a class is an example of a continuous feature. The label associated with training example is from a set of labels Y. Let us see what the training data for a spam filter looks like: The training examples presented to the learning algorithm are emails, each with one of the two labels spam or not-spam. Thus, Y spam, not-spam . Each email can be represented using the set of words contained in it. In that case, the total number of features d used for email representation is equal to the number of unique words in the dictionary. Each email x; has d features. The k-th feature of i-th email, x ), takes either a real value denoting relative frequency of the k-th word or a binary value indicating whether the k-th word occurred in the email. In addition, email x; has an associated class label y; CE Y that indicates whether the email is spam or not. Thus, the training data, X contains pairs (x;, yj). Each training example, provided with spam filter, has exactly one label. However, there are applications where an example may take more than one label. The goal of supervised learning is to learn the class conditional probability distribution of examples over the feature space (known as generative models) or to learn a separator or boundary between the classes (known as discriminative models) and use them to label each new example with one of the classes. In generative models, the algorithm assigns a probability of belonging to a particular class for each new instance. In discriminative models, a decision procedure assigns a label to new instances. There are two phases in supervised learning: (i) Learning or training and (ii) Inference. In the training phase, the algorithm learns a concept from the training data. In the inf