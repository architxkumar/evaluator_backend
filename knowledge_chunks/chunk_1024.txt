NP noodles) X P(PREP with) x P(NP chopsticks) 0.0009 P(Parse,) P(S NP VP) x P(VP VP PP) x P(VP V NP) x P(PP PREP NP) x P(NP Deepa) x P(V ate) x P(NP noodles) x P(PREP with) x P(NP chopsticks) 0.00225 wl p N , Ny JUN ia Deepa y Deepa yp PP PP aS ate y Pan prep NP PREP NP Vv NP noodles with chopsticks with chopsticks ate noodles Figure 16.10 Two valid parses of the sentence Deepa ate noodles with chopsticks . The right parse tree (Parse2) has a higher probability and would thus be preferred. The reader would have observed that the probabilities corresponding to each of the two parse trees are very low. This is expected, since the grammar can potentially generate an infinite number of sentences which define the sample space, though the probabilities of very long sentences are expected to be close to zero. An example of such a long meaningless sentence accepted by the grammar above is Deepa ate noodles with chopsticks with spoons with chopsticks with spoons with Deepa . An important question that we have not addressed so far is the following: How do we acquire the rule probabilities in a PCFG? The answer lies in using an annotated Treebank corpus (for example the Penn Treebank corpus) that contains a large number of sentences and their corresponding parse trees. The parse trees use rules of the form a c, where a is the antecedent (say NP) and c is the consequent (say DET N). The probability of the rule a c is the ratio of the number of times the rule appears in the Treebank corpus and the number of times a occurs. The approach mentioned above assumes a large corpus of sentences, each of which are associated with their correct parse trees. Construction of such treebanks involves substantial human intervention. It would be interesting to explore if we could start with a corpus that is only partially labelled with parse trees. This can be cast as a machinelearning problem where an inference has to be done in the presence of missing data. A classic approach is the Expectati