ury, Dhoni, captain, Warne, bowler as 7 dimensions. Each document is then represented as a vector in this space, where a 1 and 0 indicates the presence and absence of a word in a document respectively: D,: 1.1, 1, 0, 0, 0, 0 D,: 1, 0,0, 1, 1, 0, 0 D;: 1.0,0,0,0, 1,1 The retrieval process is inspired by the observation that documents sharing a lot of words map onto vectors that are close to each other in the vector space. If two documents share no words at all, the corresponding vectors are orthogonal. The similarity of two vectors is estimated by the cosine of the angle between vectors. For example, the cosine of the . . angle between the vectors corresponding to dD, and D, is D, xD, D, x B, Note that the denominator is the product of the norms of the two vectors. This has the positive effect of accounting for the dissimilarities in the lengths of the documents being compared. A typical Web query, for instance, may just be a few words long, while the documents it is compared against may have thousands of words. In the example above, only the presence and absence of a term is considered in modelling the relevance of a term to a document. In practice, two other pieces of information can be used to arrive at better estimates of relevance. The first is a local measure of relevance. An example is the term frequency, defined as the number of times a term occurs in a document. The second is a global measure, in the sense that it attempts to capture the discriminating power of a term by examining its presence across the collection. Words like the , of and a , for instance, may occur in most documents in the collection, and thus have very little value in discriminating one document from the rest. An example of a global measure is the inverse document frequency (idf), which is defined as follows: sim(D,, Dy) idf log ( ) where N is the total number of documents in the collection and n is the number of documents in which the word occurs. The logarithm is used for scaling, since