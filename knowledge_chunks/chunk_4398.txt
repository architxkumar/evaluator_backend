yer by #;, and in the output layer by o, Weights the input layer to the hidden layer are denoted by w1,, where the subscript i indexes the input units and j indexes the hidden units. Likewise, weights connecting the hidden layer to the output Jayer are denoted by w2,,, with i indexing to hidden units and / indexing output units. 2. Initialize the weights in the network. Each weight should be set randomly to a number between 0.1 and 0.1. wl; = random(-0.1,0.1) forall i=0,...,A,j=1,..,B w2,, = random(-0.1,0.1) forall i=0,...,B f= 1..,C 3. Initialize the activations of the thresholding units, The values of these thresholding units should never change. X= 1.0 hy = 1.0 4. Choose an input-output pair. Suppose the input vector is x, and the target output vector is y; Assign activation levels to the input units. 5. Propagate the activations from the units in the input layer to the hidden layer using the activation function of Fig. 18.16: 1 h, = for all j= 1...., B q i+ gem J Note that 7 ranges from 0 to A. w1,, is the thresholding weight for hidden unit j (its propensity to fire irrespective of its inputs). xj is always 1.0. 3 Succes. ful large-scale networks have used topologies like 203-80-26 [Sejnowski and Rosenberg, 1987], 960-9-45 [Pomerleau, 1989], and 459-24-24-1 [Tesauro and Sejnowski, 1989]. A larger hidden layer results in a more powerful network, but too much power may be undesirable (see Section 18.2.3). 388 Artificial Intelligence 6. Propagate the activations from the units in the hidden layer to the unit in the output layer. 1 . Oj= SE for all j= 1,...,C lte xo wih Again, the thresholding weight w2, ; for output unit j plays a role in the weighted summation, h, is always 1.0. 7. Compute the errors of the units in the output layer denoted 82,. Errors are based on the network s actual output (9;) and the target output (y,). 62; =0,(1 o)(y;~ 9) for all j = 1,...,C 8. Compute the errors of the units in the hidden layer, denoted 61 je Cc 61, =h(1 h) 82, - w2, = for