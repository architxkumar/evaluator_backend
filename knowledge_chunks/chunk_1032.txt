e variability. In recent years, there is a significant thrust on coming up with appropriate representations for concepts, and exploiting different knowledge sources for building these concept representations. At a broad level, there are three different knowledge sources: linguistic, background and introspective. Below, we discuss some prototypical concept mining approaches under these three categories. Linguistic Knowledge A lexical resource like WordNet can help in capturing relatedness between words. If cat licking mirror is presented as a query to an JR system, we would expect a document on animal biting glass to be marked as relevant. Several systems have been designed to realize this goal using WordNet which helps us with the knowledge that cat is a hyponym of animal and mirror is a kind of glass . One approach is to augment a query with all related terms from WordNet, and then do a traditional retrieval. A second approach is to use numeric measures of WordNet similarity, as explained in Section 16.2.3 above. Background Knowledge Humans often use a lot of background knowledge in answering questions posed to them. It is, for example, almost impossible to analyse an event pertaining to an Israel-Palestine conflict, unless someone has a good prior knowledge on the Middle East crisis. Lexicons like WordNet clearly fall short of capturing such knowledge. Interestingly, however, electronic encyclopaedias like Wikipedia have emerged as a rich storehouse of background knowledge. Each Wikipedia article, which can be treated as a distinct concept, can be represented by the words in its text. In addition, the articles are linked to each other and to Web pages outside Wikipedia using hyperlinks. Each article is categorised under a concept hierarchy that can also be exploited. (Gabrilovich et al., 2007) propose a technique which they call Explicit Semantic Analysis (ESA), in which they treat each Wikipedia article as a concept. A vector space is then constructed with each c