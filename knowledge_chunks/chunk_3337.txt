es (probabilities of Ej -. H and II) directly from these. The answer is that in general this is not possible. To compute an inferred probability requires a knowledge of the joint distributions of the ground predicates participating in the inference. From the joint distributions, the required marginal distributions may then be computed. The distributions and computations required for that approach are, in general, much more complex than the computations involving the use of Bayes' Rule. Consider now two events A and A which are mutually exclusive (Afl A = 0) and exhaustive (AU'A) = S. The probability of an arbitrary event B can always be expressed as P(B) = P(B & A) + P(B & A) = P(BIA)P(A) + P(BA)P(-A) Using this result, equation 6.4 can be written as P(HIE) P(EIH)P(H) / [P(EIH)P(H) + P(E171)P(71)] (6.6) Equation 6.6 can be generalized for an arbitrary number of hypotheses H1. i = I .... . k. Thus, suppose the If, partition the universe; that is, the H, are mutually exclusive and exhaustive. Then for any evidence E, we have k P(E) = P(E & H1) = P(EIH,)P(H,) and hence, P(EH1)P(H) P(H,IE) -- k (6.7) >P(El H,)P(H) Finally, to be more realistic and to accommodate multiple sources of evidence E1, E2.....E,,,, we generalize equation 6.7 further to obtain P(HIIEI,E2, . . ,E) = P(1,E2, . . . . EI-I,)P(H,) (6.8) ..(..........EH1)P(Jj1) If there are several plausible hypotheses and a number of evidence sources, equation 6.8 can be fairly complex to compute. This is one of the serious drawbacks of the Bayesian approach. A large number of probabilities must be known in advance in order to apply an equation such as 6.8. If there were k hypotheses. H,,, and m sources of evidence, E. then k + m prior probabilities must be known in addition to the k likelihood probabilities: The real question then is where does one obtain such a large number of reliable probabilities? To Simplify equation 6.8, it is sometimes assumed that the E1 are Statistically independent. In that case, the numer