ful in learning. But the ultimate goal of learning is to get a system into a position where it can solve problems better. Do not confuse learning algorithms with others. The perceptron convergence theorem, due to Rosenblatt [1962], guarantees that the perceptron will find a solution state, i.e., it will lear to classify any linearly separable set of inputs. In other words, the theorem shows that in the weight space, there are no local minima that do not correspond to the global minimum. Figure 18.11 shows a perceptron learning to classify the instances of Fig. 18.9. Remember that every set of weights specifies some decision surface, in this case some two-dimensional line. In the figure, & is the number of passes through the training data, i.e., the number of iterations of steps 3 through 6 of the fixed-increment perceptron learning algorithm. 10 0.41 -0.17 0.14 100 0.22 -0.14 0.11 306 -0.10 -0.08 0.07 635 -0.49 -0.10 0.14 Fig. 18.11 A Perceptron Learning to Solve a Classification Problem The introduction of perceptrons in the late 1950s created a great deal of excitement. Here was a device that strongly resembied a neuron and for which well-defined learning algorithms were available. There was much speculation about how intelligent systems could be constructed from perceptron building blocks. In their 384 Artificial Intelligence book Perceptrons, Minsky and Papert [ 1969] put an end to such speculation by analyzing the computational capabilities of the devices. They noticed that while the convergence theorem guaranteed correct classification of linearly separable data, most problems do not supply such nice data. Indeed, the perceptron is incapable of learning to solve some very simple problems. One example given by Minsky and Papert is the exclusive-or (XOR) problem: Given two binary inputs, output | if exactly one of the inputs is on and output 0 otherwise. We can view XOR as a pattern classification problem in which there are four patterns and two possible outputs