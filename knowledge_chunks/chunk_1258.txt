es the action that has been taken in each state. In Section 17.6, we studied how an optimal policy for achieving a goal can be learnt when the actions are stochastic in nature. That is, the agent is not completely sure in which state it would end up after executing a particular action. In this section, we study how an agent might learn the best actions when the uncertainty arises due to the actions of another agents, more specifically an opponent in a game situation. We have described the Bellman equations for MDPs and some algorithms to solve them in Section 17.6.1. Here we look at Temporal difference learning, first used by Samuel, then formalized by Sutton, and used effectively by Tesauro (see Chapter 8 on game playing). In Section 8.4.3, we saw how champion programs to play the game of Backgammon have been implemented by Gerald Tesauro, using a strategy of playing millions of games against themselves and learning from the exercise. Much earlier, Arthur Samuel s Checkers playing program also learnt by playing many games against itself. In this section, we take a brief look at how these programs solved the problem of temporal credit assignment. How does one decide which of the many moves in a winning (or losing) game were responsible for the outcome. This form of learning is known as Temporal Difference Learning. The reward associated with action need not be immediate, and could be delayed over time. A child who finds that saving a few rupees every now and then results in a considerable amount by which a storybook could be purchased, may learn to do so. The word reward used in MDP community is in the sense of giving a feedback for the individual action, similar to the way the signal from the output layer of a feedforward neural network is propagated back to internal nodes. The main thing is that when a set of actions are available to an agent and the agent tries out different actions or combinations of actions, and over a period of time learns the actions that fet