 the critical aspects of the training example. In the case of the fork, we know that the double simultaneous attack is important while the precise position and type of the attacking piece is not. Much of the recent work in machine learning has moved away from the empirical, data-intensive approach described in the last section toward this more analytical, knowledge-intensive approach. A number of independent studies led to the characterization of this approach as explanation-based learning. An EBL system attempts to learn from a single example x by explaining why x is an example of the target concept. The explanation is then generalized, and the system s performance is improved through the availability of this knowledge. Mitchell er ai. [1986] and De Jong and Mooney [1986] both describe general frameworks for EBL programs and give general learning algorithms. We can think of EBL programs as accepting the following as input: Ladd Noa Fig. 17.14 A Fork Position in Chess A Training Example What the leaming program sees in the world, e.g., the car of Fig. 17.7 * A Goal Concept A high-level description of what the program is supposed to learn An Operationally Criterion A description of which concepts are usable A Domain Theory A set of rules that describe relationships between objects and actions in a domain From this, EBL computes a generalization of the training example that is sufficient to describe the goal concept, and also satisfies the operationality criterion. Let s look more closely at this specification. The training example is a familiar input it is the same thing as the example in the version space algorithm. The goal concept is also familiar, but in previous sections, we have viewed the goal concept as an output of the program, not an input. The assumption here is that the goal concept is not operational, just like the high-level card-playing advice described in Section 17.3. An EBL program seeks to operationalize the goal concept by expressing it in terms t