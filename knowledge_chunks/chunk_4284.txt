) 9(0.008) 12(0.111) 6(0.006) 1(0.0001) 8(0.007) 0f0) never [60] 0(0) 0) 10.02) 000) (0) 0(0) 30.05) (0) 1(0.02) 00) 00) cared [11] 0(0) 0(0) 0) 00) 10.090) 0 0) 00) 00) 0(0) 30.273) 0(0) to [2267] 0(0) 00 00) -99(0.044) 1(0.00004) 0(0) 00) 0(0) 0(0) 0) 2 0.00008) ask [12] 0(0) 0(0) 0) 010) 0) 0x0) 0x0) 00) 00) 00) 0(0) Natural Language Processing 325 Extending this concept we may define a trigram wherein given a sequence of two words we predict the next one. In the example sentence above we could calculate the probability of a trigram as P(andltold her). 15.5.4 Smoothing While N-grams may be a fairly good way of predicting the next word, it does suffer from a major drawback it banks heavily on the corpus which forms the basic training data. Any corpus is finite and there are bound to be many N-grams missing within it. There are numerous N-grams which should have had non-zero probability but are assigned a zero value instead. Observe such bigrams in Table 15.3 It is thus best if we could assign some non zero probability to circumvent the problem to some extent. This process is known as setoothing. Two known methods are described herein. Add-One Smoothing Let P(w;) be the normal unigram (single word) probability without smoothing (unsmoothed) and N be the total number of words in the corpus then, P(w)) = c(w Ec(w;) = e(w)YN This is the simplest way of assigning non-zero probabilities. Before calculating the probabilities, the count c of each distinct word within the corpus is incremented by 1. Note the total of the counts of all words has now increased by D the number of distinct types of words in the language (vocabulary). The new probability of a word after add one smoothing can now be computed as Paaai wi) = (c(w)+1 (N+D) The reader is urged to re-compute the probability using the information in Table 15.3 and inspect the fresh values. Witten-Bell Discounting The probability of unseen N-grams could be looked upon as things we saw once (for the first time), As we g