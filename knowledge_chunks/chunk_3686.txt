ng the discrimina ti On tree. In the above equation, quantities computed as H = p, * log p, with .p, = are known as the information theoretic entropy wherep, is the probability of occurrence of some event i. The quantities H provide a measure of the dispersion or surprise in the occurrence of a number of different events. The gains G1 measure the information to be gained in using a given attribute. In using attributes which contain much information, one should expect that the size of the decision tree will be minimized in some sense, for example. in total number of nodes. Therefore, choosing those attributes which contain the largest gains will, in general, result in a smaller attribute set. This amounts to choosing those attributes which are more relevant in characterizing given classes of objects. In conclud.ng this section, an example of a small decision tree for objects described by..ur attributes is given in Figure 19.2. The attributes and their values are horned = {yes, no}, color = {black, brown, white, grey}, weight = {heasy. medium, light}, and height = {tall. short}. One training set for this example consists 404 Examples of Other Inductive Learners Chap. 19 Color 0.361 bk G 0205/\ /\ /\ 1218+ 1614 131517Figure 19.2 Discrimination tree for three attributes ordered by information gain. Ahhreviutions are br brown. bk black, w white. g gray. V = yes. n = no. h hcas, ni medium. I light, t tall, and short. of the eight instances given below where members of the class C have been labeled with + and nonmembers with - (Class (' mi ghtfor example. he the class of cows). If (brown heavy tall no -) I? (black heavy tall yes +) 13 (white light short yes -) 14 (white heavy tall yes +) 15 (grey light short yes -) 16 (black medium tall no - 17 (grey heavy tall no ) 18 (black medium tall yes +) The computations required to determine the gains are tabulated in Table 11) example. to compute the gain for the attribute color, we first compute H, = 3 8 * lou, 3 / 8 - 5 / 8 loe 