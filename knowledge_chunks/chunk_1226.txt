ions of either label sequence or parameters such as transition and emission matrix. These techniques are known as Viterbi and Baum-Welsch training respectively. Their algorithms are given in Figures 18.3 and 18.4 respectively. HMM-Viterbi (HMM: H( ), Observations: X) 1 for each 0. X 2 label, Randomly choose a label from L 3 repeat 4 Estimate (T,0) using Bayesian or Maximum Likelihood S Compute most likely labels for the observations using until convergence in label 7 return 6 FIGURE 18.3 The Algorithm HMM-Viterbi is used for training of the HMM when the label sequence is not provided as part of the training data. Here, we show an iterative procedure to estimate parameters of HMM, 8, by randomly initializing hidden sequence or label sequence. The subsequent iterations ensure that the process will converge to a local minimum. Note that both Viterbi and Baum-Welsch algorithm converge to locally optimal estimates, based on the initialization. This is due to convergence properties of EM in general. Hence, these algorithms should be run with different initializations and selecting the parameters that lead to the best log-likelihood. HMM-Baum-Welch (HMM: H( ), Observations: X) 1 Randomly initialize the transition matrix T 2 Randomly initialize the emission matrix E 3 repeat t C Compute most likely labels for the observations using Estimate (f,0) using Bayesian or Maximum Likelihood until convergence in 4 s 7 8 return 0 FIGURE 18.4 The algorithm HMM-Baum-Welch is also used for training of the HMM when label sequence is not provided as part of the training data. Here, we show an iterative procedure to estimate parameters of HMM, , by randomly initializing model parameters. The subsequent iterations ensure that the process will converge to a local minimum. 18.3.2 Finding P(X; 6) Find probability of a given sequence, X x;, Xp, -, X, where each xjex, being generated from a given HMM . The sequence X can be generated by traversing various paths in the Markov chain. Each path gen