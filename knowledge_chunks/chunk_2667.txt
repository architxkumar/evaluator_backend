ace fairly soon, visiting them only by
accident thereafter. However,itmakesperfectsensefortheagentnottocareabouttheexact
utilities ofstatesthatitknowsareundesirable andcanbeavoided.
21.3.2 Learning anaction-utility function
Nowthatwehavean active AD Pagent, letusconsider how toconstruct anactivetemporal-
difference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a fixed policy, so, if it learns a utility function U, it will need to
learn a model in order to be able to choose an action based on U via one-step look-ahead.
Themodelacquisition problemforthe TDagentisidenticaltothatforthe AD Pagent. What
ofthe TDupdateruleitself? Perhapssurprisingly, theupdaterule(21.3)remainsunchanged.
Thismightseem odd, forthefollowing reason: Supposetheagent takesastep thatnormally
Section21.3. Active Reinforcement Learning 843
2.2
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0 20 40 60 80 100
setamitse
ytilit U
(1,1) 1.4
(1,2)
(1,3) 1.2
(2,3)
(3,2) 1
(3,3)
(4,3) 0.8
0.6
0.4
0.2
0
0 20 40 60 80 100
Number of trials
ssol
ycilop
,rorre
SMR
RMS error
Policy loss
Number of trials
(a) (b)
Figure21.7 Performanceoftheexploratory AD Pagent. using R 2and Ne 5. (a)
Utility estimates for selected states overtime. (b) The RMS error in utility values and the
associatedpolicyloss.
leadstoagooddestination,butbecauseofnondeterminism intheenvironmenttheagentends
upinacatastrophicstate. The TDupdaterulewilltakethisasseriouslyasiftheoutcomehad
been the normal result of the action, whereas one might suppose that, because the outcome
was a fluke, the agent should not worry about it too much. In fact, of course, the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability, as we would hope. Once
again,itcanbeshownthatthe TDalgorithm willconverge tothesamevaluesas AD Pasthe
numberoftrainingsequences tendstoinfinity.
There is an alternative TD method, calle