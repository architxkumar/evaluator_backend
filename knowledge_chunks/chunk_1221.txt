construct higher order HMMs. Their discussion is out of scope of this book. Interested readers can refer to (Bishop, 2006). Higher order HMMs are used for finding genes in the genome sequence and a bunch of other biological sequence mining applications. Their details can be obtained from (Durbin et al., 1998), An HMM has the following components. 1. A set of observation symbols 2. 2. Neen Yu B VU E . Y isa cet of labels and B and E are special labels used for denoting begin and end state of the markov chain. We append special labels B and E at the start and the end of label sequence Y. Thus Y becomes , B, V4, Vos Ym E with Vicien iE Y. Markov Chain is used for modelling of the label dependencies. It has L states. Each state corresponds to a label in Y and it emits a symbol. Thus, each symbol in X comes from the set of symbols S. That is xje 2. The states B and E are dummy states and do not emit any symbol. Transition probability matrix stores probability of transition from state y; , to y; for every y; , and y; from the set of labels. We denote it as P(y; y-1) where yj, y i-l Y. There are L L entries in the transition matrix. Let Tj,) ,; be the transition matrix. Note that each row of the transition matrix sums to 1. Emission matrix stores probability of emitting a symbol oez state ye . a We denote it as P(sly;). It stores this probability for every ve . Oecd in every - Thus, the emission matrix has iY rows and Z columns. Let oY 2 be the emission matrix. Note that the each row sums to 1. A Hidden Markov Model is completely specified with the above components. We will use q to denote the parameters of HMM, which includes T and O. The set of labels, observation symbols and the Markov chain are provided as part of a specification. We also use to denote complete HMM specification along with the parameters. The meaning of will be clear from the context and we will state it explicitly whenever required. The following three problems are associated with HMM, out of which, t