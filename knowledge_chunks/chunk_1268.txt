ancy between the class label predicted by the Perceptron and the one specified with the training example. The Perceptron training rule or the error correcting rule adjusts each weight w; as follows, Ww, w, I(t p) x; (18.56) where t is the target class label for the training example x;,...,x and p is the class label predicted by the Perceptron, x; is the input value and nis a small constant called the earning rate that modulates the impact of each misclassified training value on the weight. The learning rate is a value between 0 and 1, and is sometimes decreased as the training algorithm progresses. Observe that a similar idea was used in Eq. (18.52). To understand the error correcting rule, recall that the class labels are either 1 or 1. If the target value f is 1 and the predicted value p is 1 then the term (t p) is 2. In this case, the weight w; is increased by an amount 2 n x;. This would in turn increase the weighted sum 2j 0,n W X;, which is what is required because one desires this sum to be a value greater than 0, so that the Perceptron would output 1. Likewise, when the target is -1 and the predicted value is 1, a similar amount is subtracted from w; thus making an attempt to reduce the sum to a value below 0. The Perceptron training algorithm using the error correcting rule is given below in Figure 18.20. It takes a set of training examples T, each described by a set of attributes A, as used in the D3 algorithm in Figure 18.12. It inspects the training examples one by one, if necessary repeatedly, until convergence. For each training example it computes Ziz0,n W; x; to arrive at a prediction p looks at the target label t, and applies the error correction rule. PerceptronErrorCorrecting (Training Set: T, Attributes: x,..x , , Learning rate: n) 1 for ie 0 ton a) initializing weights repeat for each X x,, .., x, in T t training class label P predicted class label for i O0Oton 8 w, ow, y(t - p) x, 9 until convergence 10 return W Wo, .., YAU BWW FIGURE 18.20 The