Da AAV where a is a small constant, commonly thought of as a learning rate parameter, w is the vector of weights that parameterizes the network, and V,,Y; is the gradient of network output with respect to weights. The equation computes the weight change for a single output unit. For multiple units, the computation is repeated. The quantity A is a heuristic that controls the degree of temporal credit assignment. When A 1 then the feedback goes arbitrarily back in time. That is, it affects the weights of all preceding moves. At the other extreme, when A 0, the no preceding moves are affected. With different values in between, one can control the extent to which the weights are affected. Tesauro used a value of 0.7 initially, but later used the value 0 since the performance was similar but much faster (Tesauro, 2002). The first version of the program used only a raw representation of the board. A total of 198 input nodes represented the position of the checkers, at each point and on the bar, and the number borne off the board, and whose turn it is to move. It did not have any knowledge of Backgammon encoded by an expert. With 40 internal nodes and 200,000 training games, it was able to compete successfully against Neurogammon! In the second version of TD-Gammon 1.0, Tesauro combined the knowledge intensive handcrafted features of Neurogammon with the unsupervised TD learning, and found that the program was a much better player. It was trained over 300,000 games. Next, in TD-Gammon 2.0, a selective two-ply search was introduced. The second ply search was done only on a subset of (good) successors at the one-ply level, determined by applying the evaluation function. Then in the second ply, a 1-ply move decision is made for each of the 21 possible dice rolls for the opponent, and the probability weighted average (weighting nondoubles, twice as much as doubles) of the resulting states is computed. The twoply search process is illustrated in Figure 8.43 below. ( Pruned stat