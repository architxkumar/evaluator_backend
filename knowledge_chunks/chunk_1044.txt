 Bayes Rule, a function of two quantities P(H E) and P(E). The former is called the translation model and the latter, the language model. The idea of language models has received a lot of attention over the last decade. The basic idea is that given a large corpus, useful statistics may be obtained based on bigrams or trigrams, which are sequences of two or three words respectively. A language model in English may suggest that the probability of to following going (written as P(to going)) is higher than the probability of on following going (P(on going)). The translation model, on the other hand, consists of a set of parameters that define how words in the target language can be generated from the source language. Note that the translation model is only concerned about generating an appropriate set of target words and is agnostic to how they are arranged to give rise to the target sentence. This later aspect is taken care of, by the language model of the target language, which filters out any ungrammatical constructs. Adopting the convention of IBM Model 34, the following are some of the parameters in the translation model: (a) Parameters like t(makaan house), are translation probabilities, which gives the probability of producing makaan from house. (b) Fertility parameters like n(1 house), which gives the probability that house will produce exactly one Hindi word, whenever house appears. (c) Distortion parameters like d(5 2) which gives the probability that an English word in position 2 (of an English sentence) will generate a Hindi word in position 5 (of a Hindi translation). In practice, a richer distortion parameter like d(5 2, 4, 6) is used in IBM Model 3, which is just like d(5 2), except also given that the English sentence has four words and Hindi sentence has six words. Also, an additional set of parameters may be needed to capture the fact that a Hindi word may appear out of nowhere, i.e. when there is no corresponding English word. We make two observations