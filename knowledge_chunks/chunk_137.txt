radient. The algorithm is shown below in Figure 4.3. RandomWalk () 1 node random candidate solution or start 2 bestNode node 3 for ie lton 4 do node RandomChoose (MoveGen (node) ) 5 if h(node) h(bestNode) 6 then bestNode node 7 xeturn bestNode FIGURE 4.3 RandomWalk explores the search space in a random fashion. Function RandomChoose randomly picks one of the successors of the current node. The above algorithm has n random moves. Given a surface defined by the evaluation function, Random Walk and Hill Climbing are two extremes of local search. Random Walk relies on exploration of the search space. Its performance is dependent upon time. The longer you explore the space, the more the chances of finding the global optimum. Because it is totally oblivious of the gradient, it never gets stuck on a local optimum. Hill Climbing, on the other hand, relies on the exploitation of the gradient. Its performance depends upon the nature of the surface. It terminates on reaching an optimum. If the surface has a monotonic gradient towards the global optimum, HC will quickly reach that. Otherwise, it will reach the nearest local optimum. Simulated Annealing (SA) is an algorithm that combines the two tendencies, explorative and exploitative, of the two search methods. The basic idea is that the algorithm makes a probabilistic move in a random direction. The probability of making the move is proportional to the gain of value made by the move. Traditionally, this gain is associated with energy? and we use the term AE to represent the change in the evaluation value. The larger the gain, the larger is the probability of making the move. The algorithm will make a move even for negative gain moves with nonzero probability, though this probability decreases as the move becomes worse. The point is that the algorithm will make moves against the gradient too, but will have a higher probability of making better moves. Consider a maximization problem from which three states A, B and C are shown 