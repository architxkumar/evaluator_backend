y suited to play tennis. While we arrive at these conclusions by a process of accumulated experience, decision tree building algorithms require that all the data be available along with their class labels. That is, like the algorithms Find-S and Candidate-Elimination seen earlier, it is a supervised learning procedure. Unlike the other two algorithms though, decision tree building algorithms can tolerate erroneous data or noise, and one can also control the level of detail at which discrimination should be done. These algorithms explore the space of decision trees and they have a preference bias for smaller trees. In fact by controlling the tree construction process, one can check the phenomenon of overfitting. We look at the well known ID3 (Iterative Dichotomiser 3) algorithm devised by Ross Quinlan (1986). Given a training set with N elements from K classes, the basic idea is to identify those attributes whose values separate the different classes. Each such attribute then becomes a question to ask of a data set, and partitions the data set based on the different values. The idea then, like in the game of Twenty Questions , is to identify the class in as few questions as possible. The algorithm associates the entire data set with the root node of the decision tree that it is constructing. The data set may have elements of different classes. The task to choose an attribute whose answer will separate the classes as much as possible. The different partitions are then treated recursively in the same manner, till the partitions have elements of only one class or some other termination criterion is used. One can now imagine why it can tolerate errors. If an element with a wrong class label has crept into the data set, it will go along with the elements of the correct class. If the process is continued till the partitions are completely homogenous then this element would have separated towards the end, and can be identified as being wrongly labelled. Some amount of post-