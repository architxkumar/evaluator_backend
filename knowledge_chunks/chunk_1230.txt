n terms of the values of a set A of attributes A;, Ao, ..., Ay . Let h be a hypothesis h: X yes, no defined such that h is a function of the values of the attributes a,4, ayo, ..., Axx of the element xexX. We say that the system has learned the target concept C iff, VxeX (h(x) e(x)) (18.39) That is, the discovered hypothesis agrees with the target function on each element from the domain. Given the set of attributes A, one can define hypothesis functions in different ways. Given each description schema, a space of hypotheses is defined over which the learning algorithm explores in search of the hypothesis that matches the given training set. The hope is that the hypothesis will match the target function on unseen instances as well. This is expressed by the Inductive Learning Hypothesis. Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples (Mitchell, 1997). We begin with the simplest schema conjunctive hypotheses. 18.4.1 Conjunctive Representations The conjunctive representation is a conjunction of individual constraints on each attribute. The constraint are expressed as a pattern pj, po, ..., Px , where each p; is one of the following, 1. The symbol ? , which is a wild card symbol that matches any value of the corresponding attribute. 2. The symbol , that does not match any value of the corresponding attribute. This implies that the hypothesis is a null hypothesis that does not match any individual. 3. A specific value of the attribute. Let us look at an example employee domain in which people are described by the following attributes and the set values they can take. Experience: Low, Medium, High or equivalently L, M, H Education: aes Masters, or equivalently B, M, P Hands-on: No, Yes or equivalently N, Y The above schema has been adopted from Table 15.3 by replacing numerical data for the attribute Experience with nominal types b