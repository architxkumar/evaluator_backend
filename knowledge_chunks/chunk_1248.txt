e of one class. One can also include a pruning step that removes nodes with a very small number of training instances. The algorithm ID3 is described in Figure 18.12. The algorithm is recursive in nature in which subtrees are constructed recursively. At each node, the attribute that partitions the associated set with maximum separation of the different classes is chosen. Observe that each attribute can be used only once. This is in contrast to kd-trees, where an attribute can be used more than once. The D3 algorithm essentially does Hill Climbing (see Chapter 3) over the space of possible decision trees. It begins with the empty tree and recursively builds subtrees in a greedy manner, choosing the most promising attribute for each node to partition the data. Like Find-S algorithm, and unlike Candidate-Elimination, it outputs a single hypothesis given a training set labelled with class data. Unlike both the algorithms, D3 is not sensitive to noise (errors) in the training data. The algorithm D3 is designed to handle nominal attributes. In order to be able to use this algorithm, we manually converted the Experience attribute which had numeric data to Exp-Symb, which had three nominal values Low, Medium and High. The algorithm C4.5, and C5, devised also by Ross Quinlan (1992) extends D3 to handle numeric data as well. Algorithm C4.5 improves upon D3 in other ways as well. It is more robust in handling noisy data, and can process data with missing values too. It produces rules directly and allows post pruning of the rules to avoid overfitting , by controlling how deep the tree grows. The algorithm C4.5 handles attributes with numeric (or continuous) data by placing split points at appropriate locations, usually close to the half-way mark. If one were to arrange the elements in increasing order of the attribute value then a split point is never placed between two values, corresponding to elements having the same class label. The algorithm in fact tries out different poss