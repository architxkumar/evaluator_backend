tic actions. The above plot appears similar to the plot in Figure 17.32, but one must keep in mind that the Value Iteration algorithm has to consider all possible actions at each step, as compared to a prescribed action in evaluating a given policy. The Value Iteration algorithm converges to the optimal value functions for any initial values. This is an advantage over the Policy Iteration algorithm that required the initial policy to be proper. Application of each instance of the Bellman update may access all states and all actions, and is therefore of order S x A . The backup is allied to each state and therefore the computational complexity of each iteration is O( S ? x A ). Attempts to improve the computational complexity of Value Iteration have been along the following lines. The Gauss-Seidel version makes the updated values of states available for other states in the same iteration. Asynchronous Value Iteration algorithms question the need to update each and every state in each iteration. Prioritization of the order in which states are selected is another approach. The idea is to select those states for updating earlier which are likely to change. For example, if the successors of a state in the transition graph have not changed, then backing up values from them is not going to change the value of the state. The interested reader is encouraged to refer to (Mausam and Kolobov, 2012) for a detailed discussion of these algorithms. In the following section, we look at an algorithm that employs heuristic search to cut down on the states that need to be explored. Lao Computing a policy is a little bit like implementing Dijkstra s shortest path algorithm for a complete graph, in the sense that a policy specifies the optimal actions for all possible starting states. Given our focus on planning, algorithms that explore the state space for reaching a goal state from a given start state, are of interest. We look at an algorithm LAO (Hansen and Zilberstein, 2001) that gene