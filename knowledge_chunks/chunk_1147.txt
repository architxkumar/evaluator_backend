clash problem (see Exercises 18, 19 and 20). 17.5.2 Diagnosis vs. Prediction We have seen above that the Bayes rule allows us to express conditional probabilities in a symmetric manner. That is, PY .) P(Y) POX N PC) Given the value of any of the two variables, we can determine the probability of the other one. One of the situations when this is used extensively is in the problem of classification based on evidence, or the problem of diagnosis. From the perspective of classification, one gets to see some evidence E and one has to pick the most likely hypothesis H. Given the evidence E, the belief accorded to the hypothesis H is given by, PH E) P(E H) P(H) P(E) The reason why we choose this direction, given the symmetric nature of the product rule, is that this captures the direction of causal behaviour. In the real world, the cause (that is hypothesized) results in behaviour (symptoms) and not the other way around. For example, getting malaria results in high fever and shivering. The disease causes the symptoms and this relation can be captured with greater fidelity. Observe that P(HighFever Malaria) 0.97 is equivalent to say that (Malaria ... HighFever) in the logical framework but with a degree of belief less than 1. Thus, if the hypothesis space has (say) three possibilities (h,, hp and h3) and we have seen some evidence E then we can compute the products as in the previous section and choose the maximum a posteriori hypothesis as one with the highest product, Aap argmaxhey (P(h E) P(E)) If the number of variables that constitutes the evidence is many, we could represent them by Ey, Eo, ..., E,. Then the maximum likelihood hypothesis would be, hwap argmaxner (P(hle1, 2, --, n) P( 1, 2, 1 En) which by the product rule would become, hap argmaxpen (P(hle, 2, .--, n) P( 4 2, ---1 n) P(e2 3, wees On) ... P( p) This leads us towards the Naive Bayes Classifier that we look again in Chapter 18. It is claimed that probability theory was devised to address problems in gambl