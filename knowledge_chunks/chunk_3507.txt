 the next line where a push is made to the lower NP network. As before, the NP succeeds with the structure (NP (boy (Small) DEF)) being returned as the value of (I Register VP is then set to the list returned by BUILDQ (line 24) which consists of VP followed by the verb phrase and control is passed to SS. H. Since S5 is a terminal node and the end of the input sentence has been reached. BU1LDQ will build the final sentence structure from the TYPE, SUB), AUX. and VP register contents. The final structure constructed is (S DCL (NP dog (big) DEFI) (VP (V likes((NP (boy (small) DEN(() The use of recursion, are tests, and a variety of arc and node combinations give the ATNs the power of a Turing Machine. This means that an ATN can recognize any language that a genera! purposecomputer can recognize. This versatility also makes It possible to build, deep sentence structures rather than just structure.s with surface features only. (Recall that surface features relate to the torm of words. phrases, and sentences, whereas deep features relate to the content or meaning of these elements). The ability to build deep structures requires that other appropriate tests . be included to cheek pronoun references, tense, number agreement, and other featdres. Because of their power and versatility. ATNs have become popular as a model for general purpose parsers. They have been used successfully in a number of natural language systems as well as front ends for databases and expert systems. Sec. 12.5 Semantic Analysis and Representation Structures 255 12.5 SEMANTIC ANALYSIS AND REPRESENTATION STRUCTURES We have now seen how thestructure of a complex sentence can be determined and how a representation of that structure can be constructed using different types of parsers. In particular, it should now be clear how an ATN can be used to build structures for different grammars, like those described in Section 12.3. But, we have not yet explained how the final semantic knowledge structures are c