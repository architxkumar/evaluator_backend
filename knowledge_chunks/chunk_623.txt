ture (Muscettola et al., 1998) in NASA s Deep Space One spacecraft. The need for autonomy here is acute because it takes a long time for information to travel over astronomical distances, and by the time a human on Earth senses something and reacts, and the command reaches the spacecraft, it may be too late. Autonomy is also useful on satellites orbiting the planet keeping a watch on storms, floods, fires and other phenomena. More recently, there have been efforts to build teams of robotic agents for autonomous activities like search and rescue operations (see for example (Alboul et al., 2010), (Marjovi et al., 2010) and (Meyer et al., 2011)). At the very minimum, such a program must have a set of goals or tasks to perform and be able to sense its environment. Ants, for example, are a well studied example of such simple agents. But we are in quest of a higher level of intelligence. This would require that the agent is aware of its resources and abilities, and be able to make informed judgments. Further, it should learn from its experience . Learning from experience is a slow process, even for the quickest on the uptake. It may not be enough to rely on one s own experience. For an agent to emulate human level of performance, it will need to benefit from shared knowledge accrued over generations in societies. 11.1.1 Belief, Desire and Intentions One of the popular approaches to devising agent based systems is to build them on the Belief-Desire-intention (BDI), first expounded by Michael Bratman (1987; 1990). According to this model, rational agents will need to have three kinds of information to act in a rational manner. The beliefs of an agent correspond to what the agent knows about the world. A rational agent would have a model of the world in its head, which would facilitate reasoning required to produce rational actions. Sophisticated agents would have a representation of themselves in the model of the world they hold. Rationality is tied up with goals of the age