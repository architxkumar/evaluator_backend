nes (SVMs), and Logistic Regression are some examples of discriminative supervised learning algorithms. At a deeper level, both discriminative and generative models can be seen to be equivalent. We will describe two examples of generative supervised learning algorithms: (i) Naive Bayes (NB) classifier, and (ii) Hidden Markov Model (HMM). And the Decision Tree (DT) classifier, which is an example of discriminative supervised learning algorithm. Then, we look at K-means clustering which is an example of unsupervised learning. Finally, we will have a brief look at Reinforcement Learning. 18.1 Naive Bayes Classifiers A Naive Bayes Classifier (NB) is very simple yet very powerful classifier. NB assumes that the features are conditionally independent, given the class labels. With this assumption, class-conditional joint distribution between features becomes P(x y) PEO, xO... x ) (18.3) P y) P O y) ... PRO y) (18.4) T41 PO 9) (18.5) In the Naive Bayes classifier, the learning problem is to determine class-conditional densities or mass functions for each feature, depending on its nature and a class prior, given a set of training examples. Classconditional density estimations are obtained for continuous features, while class conditional mass function estimations are obtained for discrete features. The number of parameters to be learnt depends on the parametric form of class conditional densities or mass functions. For example, if we use Gaussian distribution to model a continuous feature, we need to learn parameters (i) mean p, and (ii) standard deviation o. Thus, the key point in learning of NB classifier is to carefully model each feature with appropriate distributions. The wisdom for modelling can be obtained from domain experts or from the training data, if available in abundance. The learning problem is then to estimate relevant parameters from the training data using either maximum likelihood or Bayesian estimation techniques. For a learning problem with q classes, we 