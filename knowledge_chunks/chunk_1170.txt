to the cost that one has to minimize, one will also have to take into account the probability of reaching a goal state, and perhaps a trade off between the two criteria. Let agent choose a policy 77 for the finite horizon problem consisting of the following actions, m (s4) a4 T(S2) a2 M1(Ss) a5 11(Sg) stay The corresponding policy graph along with probabilities and costs associated with the prescribed actions is shown in Figure 17.30. Observe that only action a, is stochastic, and the probability of reaching the goal in two steps is 1. Pls , a), 82) 0.6 a A81, My, 82) 12 Gi) Ply, a, 5) 0.4 P(S, 42, 8 ) 1.0 C(s;, a), 84) 10 a a, Sg) 8 s S,. P(85, as, Sg) 1.0 C(S5, as, 55) 6 Figure 17.30 A policy graph for finite domain MDP problem. The first action is nondeterministic leading to one of sz or ss, from where the two chosen actions are deterministic. We would now like to compute the value of each state, which is the total expected cost of reaching a goal state starting in that state. Let us call this function as V7 the valuation function given the policy 77. V"(sg) Vselm) 0 Vs) V(s. ) C(s5, a5, sG) 10 Vs) V(s, ") C(s2, a2, sG) 8 V"(s1) V(sy ) P(s,, 44, 2) C(S , 44, 2) C(S9, 42, Sg) P(51, 4, 85) C(S1, 41, 55) C(S5, a5, Sg) 0.6 12 8 0.4 10 6 0.6 x 20 0.4x 16 18.4 The state sg has value zero because it is already the goal state. The value of ss is 6 because as is a deterministic action leading to the goal state with cost 6. Similarly, the cost of sp is 8. The value of s; is more involved. There are two ways of reaching the goal. One via s2 has total cost 20 and has a probability of 0.6 being taken. The other via ss has cost 16 and has probability 0.4 being taken. The expected cost is then the sum of the costs of the two routes multiplied by the probability of each route being taken. Very often however, policies are cyclic. That is, one may revisit the same state again. In fact, it would be quite likely that in a stochastic domain, an agent may need to try some actions repe