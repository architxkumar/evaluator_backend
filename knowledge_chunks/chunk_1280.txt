ices. In the equation below, the weight of the link from the " neuron to the j neuron is being adjusted, along the direction opposite to the gradient of the error E (see Eq. 18.59) with respect to the weight wj. ae av, Wy wy (18.67) The main difference is in the calculation of the derivative. There are two different cases. The first, for the output layer neurons, is similar to the delta rule, except that the neurons in the feedforward network implement the sigmoid function 2. The second case is for the hidden layer neurons. Consider the terms needed to compute the derivatives shown in Figure 18.28 (in the style of (Mitchell, 1997)). Let j be the hidden-layer neuron that applies the sigmoid function to the weighted sum 2wjx;, we will refer to as net;. Let 0; be the output of the j neuron, which is one of the inputs to a set of output layer neurons we will refer to as outputs. We will also use the term downstream(j) to stand for all the neurons that receive the output of neuron j. In the case of the three layer feedforward network ? we are looking at, these two terms, output and downstream, refer to the same set, but in other architectures they could be different. Finally o, is the output of neuron k in the output layer, and t, is the target output for that neuron. fo, Hwy bw - 8 fL- Boel Lig roe olney) . My net; Lwiyx eo a eae , Xn Wry ae Downstram( ) FIGURE 18.28 The terms associated with a hidden neuron j. The hidden neuron receives a set of inputs x;...x, and a bias 1 and computes a weighted sum net. It applies the sigmoid function to net; to generate the output 0; which is one of the inputs to a set of neurons Downstream() ) or output neurons. Given that the influence of w; can only be via net the derivative term could be written as, dE dE 4 Onety Ow; dnet; dwvy aE, OX, x; dnet ; ow, OE . dnet; (18.68) Let us consider the simpler case for a neuron k in the output layer. We can write, ) ) doy dnet, do, dnet, 0.1 00, F) O; 2 Lre outputs G kk o,) an et; do; t (4 0 )