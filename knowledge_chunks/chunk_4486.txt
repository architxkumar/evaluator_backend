to a degree by work on computer games. As we mentioned in Chapter 12, gameplaying programs must commit to irrevocable moves because of time constraints. Algorithm: Real-Time-A* 1. Set NODE to be the start state. 2. Generate the successors of NODE. If any of the successors is a goal state, then quit. 3. Estimate the value of each successor by performing a fixed-depth search starting at that successor. Use depth-first search. Evaluate all leaf nodes using the A* heuristic function f = g + h , where g is the distance to the leaf node and h is the predicted distance to the goal. Pass heuristic estimates up the . search tree in such a way that the f value of each internal node is set to the minimum of the values of its children,! 4, Set NODE to the successor with the lowest score, and take the corresponding action in the world. Store the old NODE in a table along with the heuristic score of the second-best successor. (With this strategy, we can never enter into a fixed loop, because we never make the same decision at the same node twice.) If this node is ever generated again in step 2, simply look up the heuristic estimate in the table instead of redoing the fixed-depth search of step 3. 5. Go to step 2. We can adjust the depth to which we search in step 3, depending on how much time we want to spend planning versus executing actions in the world. Provided that every part of the search space is accessible from every other part, RTA* is guaranteed to find a path to a solution state if one exists. The path may not be an optimal one, however. The deeper we search in step 3, the shorter our average solution paths will be. Of course, the task itself may impose limits on how deep we can search, as a result of incomplete information. RTA* is just one example of a limited-horizon search algorithm. Another algorithm, due to Hansson and Mayer [1989], uses Bayesian inference. Dean and Boddy [1988] define a related notion, the a7ytime algorithm. An anytime algorithm is one that can 