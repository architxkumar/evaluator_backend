dance. The learning problem is then to estimate relevant parameters from the training data using either maximum likelihood or Bayesian estimation techniques. For a learning problem with q classes, we need to learn parameters of class-conditional distribution of each feature for each of the classes. In addition, we need to estimate the prior probability of each class. It is sufficient to estimate priors for q 1 classes, since the prior for the remaining class can be obtained using the fact that 2gP(q) 1. The inference problem using an NB classifier is to predict the class label for a new example. The probability of each class label yje Y for new example Xnew is predicted using Bayes theorem Pope LYD P OY) PQ) Sy) et PO new) (18.6) The label with the highest probability among the set of labels for Xnew is predicted as its label. Formally, J; argmax PQ; Xpew) (18.7) yey Since the term P(Xnew) in Eq. (18.6) is constant for all the classes, Eq. (18.7) becomes Ji argmax PRrew ) PO ) (18.8) yey argmax P() M421 P y) (18.9) yer Example 1: Text Classification NB classifier is used extensively in text categorization. The text document is represented using a bag of word representation that contains a set of words, without specifying their order of occurrence. A tiny training data set containing 4 documents from two classes, namely Bio and CS , is shown in Table 18.1. Table 18.1 A small collection of tiny documents. DoclD Words Label 1 Algorithms, Tree, Graph CS 2 Tree, Life, Gene, Algorithms Bio 3 Graphs, NP, Algorithms, Tree 4 Protein, Assay, Cell The class priors can be estimated as follows: y AQ; CS) (Docs with label CS ) POr CS) 2 4 0.5 Lj LAG ) (Total docs) : (Docs with label Bio ) P(y Bio) 2 4 0.5 (Total docs) The class-conditional probability of each feature (word in this case) is computed as follows: P( Algorithms , y CS ) Pv, y CS") ( Algorithms in CS docs) 2 (Total words in CS docs) 7 P( Algorithms y CS ) 0.3 Similarly, P( Tree"ly "CS ) 5 . P( Graph y "CS ) e PCNP fy 