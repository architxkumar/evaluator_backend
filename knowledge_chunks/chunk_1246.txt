 Class High 1 Low 16 Medium 6 VeryHigh 13 Total 48 For example, the Higheyp, partition induced by Exp-symb has 13 elements of which 6 have the label High (salary) and 7 have label VeryHigh. The entropy of this set is, Entropy(High zy) (6 13) log,(6 13) (7 13) log,(7 13) -0.462 -1.115 -0.538 -0.893 0.515 0.481 0.996 Observe that this has only two terms because there are elements of only two classes in the partition. The entropies of the other two partitions are computed similarly. Entropy(Lowp,,) 1.160 Entropy(Mediumg,,) 1.991 The information gain for Exp-Symb is computed as shown. Gain(S, Exp-Symb) Entropy(S) ( High,, Exp-Symb)) Entropy(High,,) -( Low;,, Exp-Symb ) Entropy(Low;,,) Medium g,, Exp-Symb)) Entropy(Mediumg,) 1.957 0.271 0.996 0.354 1.160 0.375 1.991 0.530 Similarly, the gain for the other two attributes are, Gain(S, Education) 0.301 Gain(S. Hands-On) 0.381 The attribute Exp-Symb yields the maximum information gain and is therefore used to partition the set S into three partitions Highe,, with 13 elements, Lowge,, with 17 elements, and Mediume,, with 18 of the original 48 elements. The process continues recursively with these three sets. The resulting decision tree is shown in Figure 18.11. bs ) High a Medium ia" i Hands-on ( (Cain a A Hands- a) vis E E Ne M (Hamden (and-on je " cae ae No a w Low Medium Low Very High Low P u BAZ MNP B M be Medium Medium High High Very High FIGURE 18.11 The decision tree produced by the algorithm ID3 for the data set in Table 18.3. The decision tree is basically a discriminative structure. A new instance can be inserted in the root and it traverses down the tree eventually falling into a bucket at the leaf node, labelled with its class name. We have seen variations on the theme in the Rete net in Chapter 6 and the kd-tree in Chapter 15. Nevertheless, a concept description can be constructed by inspecting the tree. Every path from the root to the concept is a conjunction of constraints on a set of attributes on the path. Th