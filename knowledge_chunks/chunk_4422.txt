, then they are probably part of the same object. If these 398 Artificial Intelligence constraints can be encoded in a network structure, then parallel relaxation is an attractive technique for combining them. Because relaxation treats constraints as soft .e., it will violate one constraint if necessary to satisfy the others it achieves a global best-fit interpretation even in the presence of local ambiguity or noise. 18.3.3. Combinatorial Problems Parallel relaxation can also be used to solve many other constraint satisfaction problems. Hopfield and Tank [1985] show how a Hopfield network can be programmed to come up with approximate solutions to the traveling salesman problem. The system employs n? neural units, where n is the number of cities to be toured. Figure 18.22 shows how tours themselves are represented. Each row stands for one city. The tour proceeds horizontally across the columns. The starting city is marked by the 12 3 4 5 6 7 8 active unit in column 1, the next city by column 2, etc. The tour shown in Fig. 18.22 goes through cities D, B, E, H, G, F, C, A, and | back to D. Like all Hopfield networks, this n by 7 array contains a number of weighted connections. The connection weights are initialized to reflect exactly the constraints of a particular problem instance. !? First of all, every unit is connected with a negative weight to every other unit in its column, because only one city at a time can be visited. Second, every unit inhibits every other unit in its row, because each city can only be visited once. Third, units in adjacent columns inhibit each other in proportion to the distances between cities represented by their rows. For example, if city D is far from city G, then the fourth unit in column 3 will strongly inhibit the seventh units in Fig. 18.22 The Representation of a columns 2 and 4. There is some global excitation, so in the absence Traveling Salesman Tour in of strong inhibition, individual units will prefer to be active. a Hopfield 