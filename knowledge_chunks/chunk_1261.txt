, one can adopt a different approach for move selection. Instead of the policy specifying a move for each state, one can generate the successors of a given state, apply the evaluation function, and choose the move that leads to the best valued successor. In practice, since evaluation functions are rarely perfect, one does a lookahead of a few plys and back up the values from the horizon using the minimax rule (as described in Chapter 8). Figure 18.16 below shows a sequence of five moves made by MAX, and four by MIN, resulting in a win for MAX. For each move, MAX considers all moves possible and selects the move that results in the best valued board position. FIGURE 18.16 A one step lookahead game playing agent generates the successors at each turn, and selects the move that leads to the best valued successor. The move to be made is thus decided with an evaluation function applied to successor states to determine the best move. Instead of a policy that specifies a move for each state, we have a generic evaluation function that can be applied to any state and that a numeric value from the range -Large, Large . Let us assume that the evaluation function is a linear function of values, which are values of individual features as discussed in Section 8.1.2. Let the evaluation function be the weighted sum of features. Let there be K features used to compose the evaluation function so that for any state J the evaluation e(J) is, e(J) IK, wrt, (18.50) where v; is the numeric value of the i. feature and w; is the weight that determines the importance of the feature. This evaluation function has to be learned. We pose the learning task as learning the set of weights for the evaluation function. In fact, one can observe that any means of evaluating a board position that uses weights would be amenable to this form of learning. In fact, in the TD-Gammon program, it was the weights of a neural network that were being learnt. Reinforcement Learning happens following a process of tr