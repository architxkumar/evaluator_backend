 a, ) C(s, a, 8.) V7,(5 ) Applying this iterative process, the sets of equations for the policy Tlgg, we have the following iterations in which V5, is replaced with 0 for all n. VES wi(S1) O0.9 15 VS,(s5) 0.1 10 VS, (s4) VEE (s,) 0.7 10 VPS (sg) 0.3 6 VY, (s,) VPS 4383) 0.6 7 VS,(s) 0.4 1 VPS,(s3) VEE s(s4) 0.6 7 VS (s,) 0.4 1 VPS, (s,) VES ay(85) 0.6 5 VF ,(sg) 0.4 3 V ,(57) VEE (55) 0.7 7 VPS (s;) 0.3 1 VF, (s6) VES a(S7) 0.6 7 VS,(s4) 0.3 1 VP ,(57) There are several questions one can ask of this iterative process. The following observations have been made in (Mausam and Lolobov, 2012), Given a proper policy, the algorithm converges to a fixed point which is both unique and optimal. The values it converges to are the solutions to the set of linear equations the iterative process is based upon. The running time of the algorithm can be controlled by using a threshold e on the residual. The residual is the magnitude of the difference in the value for successive iterations, that is V",44(s) - V7,44(s) . By choosing a threshold appropriately, the values produced can be made e-consistent. Each update of a variable, in the general case will require using all the neighbours in the state graph requiring O( S ) time, where S is the size of the state space. Since updates have to be done for each state, one iteration needs O( S2 time. The graph in Figure 17.32 shows the iterative values for the policy Tl5g Starting with a value of 0 at n 1 for every state. The graph also plots the residual for state sj. 70 60 cpt 155, Sy ne a HHA MAA AA AAAS, Si 5 a 5 a a a ed ee eee gn pg Sis Sees Residual 0 - 7 SSS ooo oot 12.3 4 5 6 78 9 1011 1213 14 15 1617 18 19 20 21 22 23 2425 26 27 28 29 30 Iteration Figure 17.32 A plot of iterated value functions starting with the value 0 for all states. The bottom of the graph plots the residual for the value of state S,. The value of the residual for s; drops to 0.153651 in the 20" iteration. This could be a reasonable point to stop the process. T