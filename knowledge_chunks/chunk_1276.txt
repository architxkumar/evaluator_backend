 both the Perceptrons to work together and represent the XOR function? That would be possible if we could rig up a system in which a pair is classified as 1, only if there is consensus amongst the two individual Perceptrons. How can one implement this consensus? We can indeed deploy a third neuron that captures the Boolean AND function, and feed the output of the OR neuron and the NAND neuron as input to the AND neuron. The details are left as an exercise for the reader. Then we have a two layer neural network that can represent the XOR function. Students of logic know that any logical function can be implemented using a small set of operators. Studying digital logic, we know that any Boolean logic circuit can in fact be realized using only the NAND gate or only using the NOR gate. This means that given a sufficient number of neurons, one can implement all logic functions. In their well known book named Perceptrons, Marvin Minsky and Seymour Papert (1969) questioned the representational and generalization capability of Perceptrons based on their inability to classify problems in which the classes are not linearly separable. They even went on to conjecture that the limitations could possibly carry over to multilayered networks of neurons. The effect of the book was to discourage researchers from looking at neural networks in the immediate aftermath. It was not until the Backpropagation algorithm (discussed below) was devised and a set of two books were published by James L. McClelland and David E. Rumelhart (1986; 1986a), that there was renewed interest in neural networks in the 1980s; and by the 1990s, they were quite a rage amongst researchers. We look at multilayer networks in the next section, but before that, let us look at the XOR problem and get some intuition on how it can be represented with more than one neuron connected together. We next look at multilayer neural networks and the algorithms to train them. 18.8.2 Feedforward Networks Network structures have