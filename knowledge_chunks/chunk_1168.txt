ges are costs of actions. The action set available to the agent is A a42, 443, a44, 815, 2G, 26, 45g, 457, 441, 474, Ag3, 431 . The stochastic behaviour of the actions is described in the Table 17.13. In addition, we assume that each state s, has an action called stay that can be executed deterministically. The effect of the stay action is to remain in the same state. Table 17.13 The probabilistic state transitions for the given actions Action Source Destination (probability) Destination (probability) ayy Sy, Sy (0.8) 5; (0.2) 43 51 50.0) 53 (1.0) 4 1 Sz (0.6) 55 (0.4) Qs 5 Ss (0.9) 54 (0.1) arg 2 5g (0.7) 56 (0.3) x5 52 5g (0.0) 55 (1.0) asg 55 Sg (0.6) 5; (0.4) 455 5s Sg (0.0) 7 (1.0) a3 53 5 (0.6) 53 (0.4) ay 54 5, (0.6) 54(0.4) ag3 5s 530.7) 55 (0.3) 74 5 530.7) 57 (0.3) Given the above problem, one can immediately think of two interesting policies, one trying to reach sg via sp and the other trying to reach Sg via Ss. The policies 72g and 75g are described below in which the action in each state se S is prescribed. The actions in each of the eight states as per 779g are, My (S1) ay2 TG(S2) Arg MG(S3) a3 My G(S4) 4, TM (S5) asg 1 (SG) stay MG(S6) 463 Ty (S7) 74 Observe that the policy directs the agent to take actions a;2 whenever it is in state s;. The only unintended effect of this action is to take the agent to sz instead of sz, from where the only moving action is back to sj. This means that, given the above set of action effects, there is no chance of the agent landing up in state s5. However, the policy does specify the action to be taken in that state. The other policy of interest ps5g has the following actions, TMsG(S1) a5 Ts (S3) Arg Ts (S3) a3 TMsg(S4) a4) TsG(Ss) Asg Ns (Sg) stay Ts (S6) 463 Ms(S7) 474 The reader would have noticed that the only place this policy differs from the previous one is in the action to be taken in state sj. In all other states, the actions are identical. This is not surprising, given that these are the only two that are like