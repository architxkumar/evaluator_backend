of descriptions in the concept space is exponential in the number of features and values. So enumerating them is prohibitive. However, it turns out that the version space has a concise representation. It consists of two subsets of the concept space. One subset, called G contains the most general descriptions consistent with the training examples seen so far; the other subset, called S, contains the most specific descriptions consistent with the Null Hypothesis Version Space Concept Space Training Examples Fig. 17.11 Concept and Version Spaces training examples. The version space is the set of all descriptions that lie between some element of G and some element of S in the partial order of the concept space. This representation of the version space is not only efficient for storage, but also for modification. Intuitively, each time we receive a positive training example, we want to make the S set more general. Negative training examples serve to make the G set more specific. If the S and G sets converge, our range of hypotheses will narrow to a single concept description. The algorithm for elimination algorithm. narrowing the version space is called the candidate 361 Learning Algorithm: Candidate Elimination Given: A representation language and a set of positive and negative examples expressed in that languz Compute: A concept description that is consistent with all the cositive examples and none of the negatis examples. i. Initialize G to contain one element: the nul! description (all features are variables). 2. Initialize S to contain one element: the first positive example. 3. Accept a new training example. If it is a positive example, first remove from G any descriptions that do not cover the example. Then, update the S set to contain the most specific set of descriptions in the version space that cover the example and the current elements of the S set. That is, generalize the elements of S as little as possible so that they cover the new training example. If it 