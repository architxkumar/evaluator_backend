ors. That is, the points in N-dimensional space will be clustered close to each other, and the weight vectors will be aligned closer to each other. If there are more than M neurons in the output layer then hopefully their weight vectors will also align themselves along the M clusters. The system would then have learnt the M patterns. The learning process is unsupervised, in the sense that it is not externally specified during training what cluster (or class) each input belongs to. Such networks are also known as self organizing systems. 18.8.4 Networks with Feedback While the feedforward networks are used to classify patterns (information flows from pattern to class labels), one can build networks with feedback that allow information to flow in all directions. Examples of such networks are the Hopfield Network and the Boltzmann machine. These networks can be used to store and recall patterns. All the neurons in the network can be treated both as input as well as output neurons, though invisible or hidden neurons can also be there. The way recall of patterns works is as follows. An input pattern is shown to the network by activating a set of neurons. The pattern may be a partial one, activating only some of the neurons, or even an erroneous one, representing a pattern the network has not seen before. The network is then allowed to go through a process of relaxation after which, the network settles into a minimum energy state (Hopfield, 1982). Consider the Hopfield network of four neurons shown in Figure 18.31. Assume that each neuron can be in one of two states, and the influences are positive and negative. Positive influences are excitatory and the connected neurons tend to be in the same state; while negative influences are inhibitory, driving them towards opposite states. The degree of influence is determined by the weight of the connection. Negative weights imply inhibitory influence. At any point, the state of a neuron is determined by the states of neurons conn