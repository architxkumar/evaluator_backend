ighted fashion. The other option would be to seek more training instances to np discriminate between the remaining candidate hypotheses. 3. At some point during the process of inspecting the training instances, the version space becomes empty. This could mean two things. One, that the training examples have an error; or two, that the target concept does not exist in the hypothesis space. Fortunately, one does not have to represent the entire version space. It is enough to represent the most specific hypotheses, known as the specific boundary S, and the most general ones, known collectively as the general boundary G. Formally, the sets S and G are defined with respect to the training set T as follows. G ge H Consistent (g, T) a (-ag , g) A Consistent (g , T) (18.42) S se H Consistent (s, T) a CAs , s ) A Consistent (s , T) (18.43) It has been shown that the version space can be reconstructed from these two sets as follows. VSy,7 he H (Ase S) a Age G) a (g 2, h 2, 5) (18.44) The algorithm Candidate-Elimination initializes the version space to the hypotheses space. The general boundary of this space is ?, ?, ..., ? and the specific boundary is the empty hypothesis , , ..., D . Then for every positive training instance it sees, the algorithm removes any hypotheses in G that are inconsistent with the example. And like Find-S it also removes any hypothesis in S that does not match the training example and replaces it by generalizations that are just enough to match it, with the additional constraint that the generalization is subsumed by (is less general than) some hypothesis in G. Finally, it removes any hypothesis in S that has a more general hypothesis in S. The treatment for negative training example is symmetric. The algorithm is given in Figure 18.9. Candidate-Elimination (Training Set: T, Target Concept: C) 1S , , .., D 26 ?, ?, 2 ) 3 for each teT 4 if te Cc instance t is lebelled yes for class C 5 then for each geG 7 if g(t) no then remove g 8 for each seS 9 if s(