initializing weights repeat for each X x,, .., x, in T t training class label P predicted class label for i O0Oton 8 w, ow, y(t - p) x, 9 until convergence 10 return W Wo, .., YAU BWW FIGURE 18.20 The Perceptron training algorithm using the error correcting rule repeatedly adjusts the weights when it encounters misclassified training examples. The weight adjustment essentially moves the linear separator towards the misclassified point. The termination criterion has been left unspecified in the algorithm. We will discuss the criteria for convergence, after the following discussion. First let us understand the computation that the Perceptron is doing. Having learnt a set of weights wWo, ..., W, , the Perceptron accepts the feature vector x,, ..., X, of a new element and computes the sum Zj 9,n w; xX; assuming Xo 1 implicitly. If this sum is greater than 0 then it outputs a value 1 signifying that the element belongs to a given class, else it outputs a value 1, signifying that it does not, and therefore it belongs to the other class. The Perceptron is essentially a linear classifier that can distinguish between two classes. This is depicted in Figure 18.21 for two classes (Class-1 and Class-2) in a two dimensional space. Each element from the domain is represented by two features, values x1, X2 and the hyperplane is a line defined by, Wo wy Fx, WX, 0 (18.57) This can be rewritten as, X (ay, 17) x, (vo 1vy) (18.58) which is the more familiar form of a line in a two dimensional space. The reader will observe that the role of Wo, which is the final bias learnt by the system, is essentially to define the displacement of the line, while the slope is defined by w. Given a new element v;, v2 , the Perceptron computes the sum (Wo w4 v1 W2 v2), and if it is greater than 0, the element is predicted to be from Class-2. Else it is from Class-1. This is equivalent to saying that the output is 1 if 2j 1,, w V; -Wo. This shows that the role of the bias b wo value is to position the l