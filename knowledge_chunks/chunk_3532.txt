0 0f07 * o o 0 o I/ + + o 0 o A' + + + C 000/++**++ c, 0 0 0/ + + o 7* + / + + -4oo I'+ I 000 ., +++++ 00/ +++ f d( X) + X2W2 * 0 Figure 13.2 A linear decision function. Sec. 13.2 The Recognition and Classification Process 275 belonging to class C2 when d(X) > 0. When d(X) = 0 the classification is indeterininate, so either (or neither) class may be selected. When class reference vectors, prototypes R1. j = I .......are available. decision functions can be defined in terms of the distance of the X from the reference sectors. For example, the distance d,(X) = (X - R,)'(X - R,) could be computed for each class C, and class CA would then be chosen when d A = min{C}. For the general case of c ^ 2 classes. C1, CC_ a decision function may be defined for each class d1, 6, ,...,d,. A class.decision rule in this case would he defined to select class c1 when < d(X) for ij = I. 2..... c, and i ^6 j. When a line d (or more generally a hyperplane in Jr-space) can he found that separates classes into two or more groups as in the case of Figure 13.2. e the classes are linearly separable. Classes that overlap each other or surround one another, as in Figure 13.3, cannot generally be classified -,kith the uc ol irnple linear decison functions. For such cases, more general nonlinear (or piece\ 'c linear) functions may be required. Alternatively, some other selection technique t like hcuri'.- tics) may be needed. The decision function approach described above is an example of detcrrninitie recognition since the x, are deterministic variables. In cases where the attribute values are affected by noise or other random fluctuations, it may he more upprupriale to define probabilistic decision functions In such cases, the attribute vectors X are treated as random variables., and the decision functions are defined as measures ol likelihood of class inclusion. For example, using Bayes' rule, one can compute the v conditional probability P(C,IX) that the class of an object o is C, gi en the ub