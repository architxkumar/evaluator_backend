might look for a mechanism to fill in the arc weights accurately by consulting an external source like the Wordnet (see Chapter 16). Buttermilk FIGURE 15.30 A sample CRN for a meal classification task. The rectangular nodes are E nodes and the arcs between them represent local similarity. The oval nodes are cases defining a typical meal in that style. The solid arrows capture the relevance of E nodes for case nodes. In the second stage, activation spreads to the case nodes. The two arrays E and C contain the activation values of the E nodes and the case nodes respectively. In the algorithm below, we assume the query Q to be a list (somehow) containing the indices of the matching E nodes. The algorithm described above in fact operates in a backward manner. For each E node, the aggregated activation is computed; and then for each case node, the aggregated activation value is computed. The aggregation functions simAggr and relAggr are of order O(M) steps because activation is transferred from each E node in both. Since M JE nodes and N case nodes receive propagated values, the overall complexity of retrieval is O(M 2 MN) which is O(M 2). This is because it does not exploit the fact that the similarity arcs are likely to be sparse. A variation that only does forward propagation like the AC3 algorithm (see Chapter 9) is likely to be much faster. The reader is encouraged to devise an algorithm that propagates only non-zero values. 15.2.8 Diversity Conscious Retrieval The basic idea behind CBR is that given a problem, one retrieves similar cases from the case base. This is based on the premise that similar problems have similar solutions. The idea of retrieving K cases instead of one is that a solution constructed from the K cases is likely to be more robust, specially in ill understood domains. In recommender systems however, the task is to help a user select a product that best matches her requirement. The retrieval set is not used to construct one solution. Rather it of