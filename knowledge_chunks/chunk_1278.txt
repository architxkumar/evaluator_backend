sum does not give us any additional power. We need a function that is both non-linear and differentiable. Typically, the function that is chosen is the sigmoid function, reproduced below to contrast it with the step function used in the Perceptron. The sigmoid function s, like the signum function, is a squashing function, implying that it squashes or limits the output however large the input is. Here we choose the signum function s to serve as the function f we apply to the summation z. 2) o) e ) (18.65) The function is applied to the linear sum z defined in Eq. (18.54) and limits the output between 0 and 1. Figure 18.27 contrasts the sigmoid function with the threshold function (which is like the signum function, but limits the output between 0 and 1). A variation of the sigmoid function, known as the bipolar sigmoid function, limits the output between 1 and 1. Other popular activation functions are the hyperbolic-tangent function and the arc-tangent function. z) 1ifz 0 0 otherwise Threshold function Sigmoid function Fig. 18.27 The threshold function is not a differentiable function while the sigmoid function is. The threshold function is a step function while the sigmoid function approaches the values 0 and 1 asymptotically. As a increases, the sigmoid function makes the transition more rapidly, approaching the threshold function as a A . The derivative of the sigmoid function s is easily computable. do( SO o(2) 1-0) (18.66) These networks are known as feedforward networks because information flows in one direction from the input neurons to the output neurons. For every input shown to the network, the output is a classification of the input pattern. For example, if the input to the network is a linearized array obtained by scanning and digitizing handwritten characters, the output could be the ASCII code for the character. The Backpropagation Algorithm The Backpropagation algorithm was first devised by Werber (see (Werber, 1994) for more information) and published