he network s actual output (9;) and the target output (y,). 62; =0,(1 o)(y;~ 9) for all j = 1,...,C 8. Compute the errors of the units in the hidden layer, denoted 61 je Cc 61, =h(1 h) 82, - w2, = forall j= 1,...,B fet 9. Adjust the weights between the hidden layer and output layer. The learning rate is denoted 7; its function is the same as in perceptron learning. A reasonable value of 77 is 0.35. Aw2, = 7 - 62;-h; forall i=0,...B, falc 10. Adjust the weights between the input layer and the hidden layer. Awl, =7-6);-x; forall i=0,..,A,j=1..,B i 11. Go to step 4 and repeat. When all the input-output pairs have been to the network, one epoch has been completed. Repeat steps 4 to 10 for as many epochs as desired. The algorithm generalizes straightforwardly to networks of more than three layers. For each extra hidden layer, insert a forward propagation step between steps 6 and 7, an error computation step between steps 8 and 9, and a weight adjustment step between steps 10 and 11. Error computation for hidden units should use the equation in step 8, but with i ranging over the units in the next layer, not necessarily the output layer. The speed of learning can be increased by modifying the weight modification steps 9 and 10 to include a momentum term a. The weight update formulas become: Aw2,(t + 1) = 7-62; - hy + a Aw2,(1) Awl,(t+ 1) = 79-81; +h, + a Awl,(O where fy, x, 6 1, and 6 2; are measured at time t + 1. Aw,(#) is the change the weight experienced during the previous forward-backward pass. If @ is set to 0.9 or so, learning speed is improved. 4 The error formula is related to the derivative of the activation function. The mathematical derivation behind the backpropagation learning algorithm is beyond the scope of this book. 5 Again, we omit the details of the derivation. The basic idea is that each hidden unit tries to minimize the errors of output units to which it connects. 6 A network with one hidden layer can compute any function that a network with many hi