(Gabrilovich et al., 2007) propose a technique which they call Explicit Semantic Analysis (ESA), in which they treat each Wikipedia article as a concept. A vector space is then constructed with each concept as a dimension. Every word is represented in this space as a vector, which has a component 1 across a dimension, if the corresponding Wikipedia article contains that word, and has a component 0 otherwise. Since any given piece of text is simply a vector sum of its word vectors, it is easy to map both the query and the given set of documents to the concept space. Cosine similarities between the query and documents are computed and relevant documents are retrieved and ranked, as usual. Introspective Knowledge The idea here is to infer associations between words and phrases by investigating their co-occurrence patterns within a collection of documents. For example, the word automobile is likely to co-occur with words like gear , chassis or suspension in many documents, and can thus be inferred to be semantically related to these words. Statistical techniques can exploit these co-occurrence patterns to generate word clusters, which can in turn be used for query expansion. Co-occurrences have their limitations, however. (Lund et al., 1996) observe that near synonyms like road and street fail to co-occur in their huge corpus. In a French corpus containing 24 million words from the daily newspaper Le Monde in 1999, (Lemaire et al., 2006) found 131 occurrences of Internet, 94 occurrences of Web, but no co-occurrences at all. This has motivated researchers to investigate ways of modelling higher order co-occurrence patterns between words. If words A and B cooccur in some documents and words B and C in some others, words A and C can be said to share a second order co-occurrence between them (via B). In the Section below, we briefly describe a Factor Analytic technique called Latent Semantic Indexing (LSI) that mines concepts from a document collection by exploiting higher 