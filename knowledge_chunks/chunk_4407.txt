 There are an infinite number of orientations and expressions to a face, and an infinite number of fonts and sizes for a character, yet humans learn to classify these objects easily from only a few examples. We would hope that our networks would do the same. And, in fact, backpropagation shows promise as a generalization mechanism. If we work in a domain (such as the classification domains just discussed) where similar inputs get mapped onto 390 Artificial Intelligence SR ONAN MOSER similar outputs, backpropagation will interpolate when given inputs it has never seen before. For example, after learning to distinguish a few different sized As from a few different sized Bs, a network will usually be able to distinguish any sized A from any sized B. Also, generalization will help to overcome any undesirable noise in the inputs. There are some pitfalls, however. Figure 18.17 shows the common generalization effect during a jong training period. During the first part of the training, performance on the training set improves as the network adjusts its weights through backpropagation. Performance on the test set (examples that the network is not allowed to learn on) also improves, although it is never quite as good as the training set. After a while, network performance reaches a plateau testing as the weights shift around, looking for a path to further set improvement. Ultimately, such a path is found, and performance on the training set improves again. But performance on the test set gets worse. Why? The network Fig. 18.17 A Common Generalization Effect in has hegun to memorize the individual input-output pairs Neural Network Learning rather than settling for weights that generally describe the mapping for all cases. With thousands of realvalued weights at its disposal, backpropagation is theoretically capable of storing entire training sets; with enough hidden units, the algorithm could learn to assign a hidden unit to every distinct input pattern in the training set. It