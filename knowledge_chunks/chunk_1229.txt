s X and (n) gives the corresponding label sequence. Y argmax P(X, Y: 0) (18.37) y Y (n) (18.38) Viterbi (HMM: H( ), Observations: X, Parameters: 6) 1 wH(B) 1 2 repeat 3 for i l1ton 4 for each y,e Y 5 WB) ces P(EI Yn) Y( n) 6 (EB) Argmax P(Ely,) ( ,) J return Ye (n) FIGURE 18.6 The Viterbi algorithm for the HMM is used to obtain the most probable sequence of label for a given observation sequence. 18.4 Concept Learning In Chapters 13 and 14, we discussed in detail the representation and reasoning with categories or concepts. A concept is a subset of the universe of discourse or the domain. We started by defining atomic concepts like brother and parent and used them to define other concepts like uncle. In this section, we explore how concepts may be formed or learnt from instances of the concept. We assume that each individual in the domain is described by a set of attributes, and based on values of these attributes, certain concepts may be defined. For example, in the romantic fiction of the last century an eligible bachelor could be described as tall, dark? and handsome, and perhaps some other features like being sensitive, intelligent, having a sense of humour, being rich, etc. The task of concept learning is as follows. Given that one has a set of training instances for which it is known whether they belong to a concept or not, to learn a general concept in terms of the attributes of the instances. The idea is that once such a concept has been learnt then one can use the knowledge to classify previously unseen instances correctly. This process of learning from examples is known as nductive Learning. Let X x1, Xo, ..., xn be the universe of discourse, and let CoyX be the concept of interest defined by the target function c: X yes, no . Let each member x; be described in terms of the values of a set A of attributes A;, Ao, ..., Ay . Let h be a hypothesis h: X yes, no defined such that h is a function of the values of the attributes a,4, ayo, ..., Axx of the element 