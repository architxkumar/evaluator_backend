ndwouldbechosenbythedecision-tree learning
algorithm astheroot.
Section18.3. Learning Decision Trees 705
18.3.5 Generalizationandoverfitting
On some problems, the DECISION-TREE-LEARNING algorithm will generate a large tree
when there is actually no pattern to be found. Consider the problem of trying to predict
whether the roll of a die will come up as 6 ornot. Suppose that experiments are carried out
with various dice and that the attributes describing each training example include the color
of the die, its weight, the time when the roll was done, and whether the experimenters had
their fingers crossed. If the dice are fair, the right thing to learn is a tree with a single node
that says no, But the DECISION-TREE-LEARNING algorithm will seize on any pattern it
can find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers
crossed and theyboth come out6, then the algorithm mayconstruct apath that predicts 6in
thatcase. Thisproblemiscalled overfitting. Ageneral phenomenon, overfittingoccurswith
OVERFITTING
alltypesoflearners,evenwhenthetargetfunctionisnotatallrandom. In Figure18.1(b)and
(c),wesawpolynomialfunctionsoverfittingthedata. Overfittingbecomesmorelikelyasthe
hypothesis spaceandthenumberofinputattributes grows,andlesslikelyasweincrease the
numberoftrainingexamples.
DECISIONTREE Fordecisiontrees,atechniquecalled decisiontreepruningcombatsoverfitting. Prun-
PRUNING
ing works by eliminating nodes that are not clearly relevant. We start with a full tree, as
generated by DECISION-TREE-LEARNING. We then look at a test node that has only leaf
nodes asdescendants. Ifthetestappears tobeirrelevant detecting onlynoiseinthedata then weeliminate the test, replacing it witha leaf node. Werepeat this process, considering
eachtestwithonlyleafdescendants, untileachonehaseitherbeenpruned oraccepted asis.
Thequestionis,howdowedetectthatanodeistestinganirrelevantattribute? Suppose
weareatanodeconsistingofppositiveandnnegativeexamples. Iftheattribu