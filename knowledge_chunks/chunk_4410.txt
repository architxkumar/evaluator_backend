arch technique, called simulated annealing,. appeared in the literature. Simulated annealing, described in Chapter 3, is a technique for finding globally optimal solutions to combinatorial problems. Hinton and Sejnowski [1986] combined Hopfield networks and simulated annealing to produce networks called Boltzmann machines. To understand how annealing applies, go back to Fig. 18.4 and imagine it as a black box. Imagine further a ball rolling around in the box. If we could not see into the black box, how could we coax the ball into the deepest valley? By shaking the box, of course. Now, if we shake too violently, the ball will bounce from valley to valley at random. That is, if the ball were in valley A, it might jump to valley B; but if the ball were in valley B, it might jump to valley A. If we shake too softly, however, the bail might find itself in valley A, unable to jump out. The answer suggested by annealing is to shake the box violently at first, then gradually slow down. At some point, the probability of the ball jumping from A to B will be larger than the probability of jumping from B to A. The bal! will very likely find its way to valley B, and as the shaking becomes softer, it will be unable to escape. This is what we want. How is this idea implemented in a neural network? Units in Boltzmann machines update their individual binary states by a stochastic rather than deterministic rule. The probability that any given unit will be active is given by p-: 1 p= 14 e Abl T where AEF is the sum of the unit s active input lines and T is the temperature of the network. Stochastic updating of units is very similar to updating in Hopfield nets, except for the temperature factor. At high temperatures, units display random behavior, while at very low temperatures, units behave as in Hopfield nets, Annealing is the process of gradually moving from a high temperature down to a low temperature. The randomness added by the temperature helps the network escape from local min