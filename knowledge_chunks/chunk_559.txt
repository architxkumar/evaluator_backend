 the top down fashion. The cost of traversing each arc in the DTG(v) is the base cost (1 for fluents and 0 for derived variables) plus the cost of achieving the conditions associated with the transition. For a given condition v' e , the cost depends upon the value e in the state when the transition occurs. To facilitate that the algorithm commits greedily to whatever transition it can make to some value d and annotates the resulting node a with the local state achieved by the plan to reach a . Fast Downward has three different search algorithms implemented. 1. Best First Search Using the Causal Graph Heuristic It also uses a notion of preferred operators similar to FF s use of helpful operators, but helpful transitions which are generated by looking at the paths found in the DTGs. The algorithm maintains two OPEN lists, one for all operators and one for the preferred operators, and picks nodes alternately from the two lists. The algorithm also uses deferred heuristic evaluation. Instead of evaluating all successors of a given node s, they are placed on the OPEN list with the heuristic value of s. The successors are evaluated only when they are picked from the OPEN list. This has the effect that if better successors are found early, their siblings are never actually evaluated. In fact, the successor itself is not put in the OPEN, but a pointer to the parent and the operator generating the successor is stored, and the successor is generated only when it needs to be evaluated. 2. Multi-heuristic Best First Search This is a variation of best first search that allows the use of multiple heuristic functions. The different heuristic values are not combined into one value. Instead, separate OPEN lists are maintained for each heuristic function. The search algorithm picks nodes for expansion alternately from the lists. When successors are generated, they are evaluated by the different functions and inserted into the respective lists. The idea here is that different heuristic