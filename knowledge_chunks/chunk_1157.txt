istribution. If we now take some new evidence E into account, then we would need to compute the posterior probabilities, P(RIE) P(RIQ, E) P(Q E) P(R 7Q, E) P(-Q E) but this changes the problem to a completely new one. 17.5.4 Belief Networks Given that the probability P(a) assigns a numerical value to the belief in the proposition a given everything the agent knows about the world, if one wants to relate the truth value of a statement to the truth values of other statements, one must take into account all other statements that could possibly influence the given statement. Thus, one has to consider the joint probability of all such variables. Given the joint probability distribution, one can compute the marginal and conditional properties of different variables. However, constructing the joint probability distribution could be a humongous task, and the computations to be done to derive specific probabilities proportionally expensive. Human beings, on the other hand, make probabilistic observations very quickly, especially on the relatedness of different variables. For example, most people would agree that the appearance of dark clouds bodes a (welcome) rain shower. Similarly, they would agree that the fall of a coconut from a tree in Kerala has no bearing on the possibility of snow in Manali 4. And we do this without recourse to storing huge joint probability tables and enormous amounts of computation. Belief networks are an attempt to marry probability theory with networks used in various forms for knowledge representation, and the application of propagation techniques over networks for making inferences. There are two types of probabilistic network based representations that are popular. Markov networks are undirected graphs in which an edge between two nodes represents conditional dependence between the two nodes, which stand for random variables. Bayesian belief networks are directed acyclic graphs in which a directed edge captures a causal relation between two ra