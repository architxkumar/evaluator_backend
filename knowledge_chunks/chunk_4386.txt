f perceptrons can be trained on sample input-output [fh pairs until it learns to compute the correct function. The amazing property ik at, . . . . : positive/negative of perceptron learning is this: Whatever a perceptron can compute, it can : threshold learn to compute! We demonstrate this in a moment. At the time perceptrons were invented, many people speculated that intelligent systems could be Fig. 18.6 Perceptron with constructed out of perceptrons (see Fig. 18.8). Adjustable Threshold implemented as Additional Weight Xp Wr sensory elementary actions on inputs feature complex the world detectors feature decision detectors Makers aa ea >| Le Fig. 18.7 A Perceptron with Many Inputs and Fig. 18.8 An Early Notion of an Intelligent System Built from Trainable Perceptrons Many Outputs Connectionist Models 381 AAT ATTEN AAT ERE ITLL NY NRT Since the perceptrons of Fig. 18.7 are independent of one another, they can be separately trained. So let us concentrate on what a single perceptron can learn to do. Consider the pattern classification problem shown in Fig. 18.9. This problem is /inearly separable, because we can draw a line that separates one class from another. Given values for x, and X2, we want to train a perceptron to output 1 if it thinks the input belongs to the class of white dots arid 0 if it thinks the input belongs to the class of black dots. Pattern classification is very similar to concept learning, which was discussed in Chapter 17. We have no explicit rule to guide us; we must induce a rule from a set of training instances. We now see how _ Fig. 18.9 A Linearly Separable perceptrons can learn to solve such problems. Pattern Classification First, it is necessary to take a close look at what the perceptron Problem computes. Let x be an input vector (x), x,...1,,). Notice that the weighted summation function g(x) and the output function o(x) can be defined as: h go) = y Wik i=0 1 if g(x) >0 a(x) = . 0 if g(x) <0 Consider the case where we have only two in