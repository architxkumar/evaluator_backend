d position to a jumping position instantaneously. Smoothness can be implemented as a change in the weight update rule; essentially, the error of an output becomes a combination of real error and the magnitude of the change in the state units. Enforcing the smoothness constraint turns out to be very important in fast learning, as it removes many of the weight-manipulation options available to backpropagation. A major problem in supervised learning systems lies in correcting the network s behavior. If enough training data can be amassed, then target outputs can be provided for many input vectors. Recurrent networks have special training problems, however, because it is difficult to specify completely a series of target outputs. In shooting basketbalis, for example, the feedback comes from the external world (i.e., where the basketball 400 Artificial Intelligence lands), not from a teacher showing how to move each muscle. To get around this difficulty, we can learn a mental model, a mapping that relates the network's outputs to events in the world. Once such a model is known, the system can learn sequential tasks by backpropagating the errors it sees in the real world. So it is necessary to learn two different things: the relationship between the plan and the network s output, and the relationship between the network s output and the real world. Networks of this type are described by Jordan [1988]. Fig. 18.24 shows such a network, which is essentially the same as a Jordan net except for the addition of two more layers: another hidden layer and a layer representing results as seen in the world. First, the latter portion of the network is trained (using backpropagation) on various pairs of outputs and targets until the network gets a good feel for how its outputs affect the real world. After these rough weights are established, the whole network is trained using real-world feedback unti} it is able to perform accurately. Articulatory Units SS -O>0- ~~ > Hidden Target Uni