eems his secretary at MIT thought the machine was a real therapist, and spent hours revealing her personal problems to the program, only to be deterred when Weizenbaum told her (who was outraged at this invasion of privacy) that he had access to the logs of all the conversations. Somewhat disturbed by the seriousness with which people were taking ELIZA, Weizenbaum wrote another book (Weizenbaum, 1976) to debunk the aura around the program, and said that it is we humans who tend to anthropomorphize such programs. Nevertheless, since 1991, an annual competition called the Loebner Prize has been held, in which a set of judges interact with a set of chatterbots, programs that chat, to decide which one is the most humanlike. Though no program has been able to fool the judges yet, there has been an admirable amount of sophistication in the participating programs over the years. Unlike ELIZA which had no knowledge backing its conversations, the modern versions have access to databases and topical world information. The 2009 competition was won by David Levy s program named Do-Much-More, in which it responded to a question about its identity with S Well, am studying engineering. That ought to give you an idea about who am, but would sooner be working at the Cadbury s factory. One well known objection to the Turing Test was put forward by John Searle (1980) who says that programs like ELIZA could pass the Turing Test without understanding the words they were using at all. One must keep in mind that ELIZA itself has no understanding of words, but it cannot also be said to have passed the test. Searle illustrated his argument, now known as the Chinese Room argument, by a thought experiment. He imagines himself in a room, acting like a computer, manipulating Chinese symbols according to some rules. People outside the room slide in some slips with Chinese symbols, and Searle passes back some Chinese symbols in return. Does this mean that he understands Chinese? As one can see, t