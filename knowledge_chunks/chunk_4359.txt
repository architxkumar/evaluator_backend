is that the goal concept is not operational, just like the high-level card-playing advice described in Section 17.3. An EBL program seeks to operationalize the goal concept by expressing it in terms that a problem-solving program can understand. These terms are given by the operatioriality criterion. In the chess example, the goal concept might be something like bad position for Black, and, the operationalized concept would be a generalized description of Situations similar to the training example, given in terms of pieces and their relative positions. The last input to an EBL program is a domain theory, in our case, the rules of chess. Without such knowledge, it is impossible to come up with a correct generalization of the training example. Explanation-based generalization (EBG) is an algorithm for EBL described in Mitchell et al. [1986]. It has two steps: (1) explain and (2) generalize. During the first step, the domain theory is used to prune away all the unimportant aspects of the training example with respect to the goal concept. What is left is an explanation of why the training example is an instance of the goal concept. This explanation is expressed in terms that satisfy the operationality criterion. The next step is to generalize the explanation as far as possible while still 366 Artificial Intelligence SECRETE SMARTER, LIZ SUTRAS EES he ENN MPP DSMR OEE LIT EN describing the goal concept. Following our chess example, the first EBL step chooses to ignore White s pawns, king, and rook, and constructs an explanation consisting of White s knight, Black s king, and Black s queen, each in their specific positions. Operationality is ensured: all chess-playing programs understand the basic concepts of piece and position. Next, the explanation is generalized. Using domain knowledge, we find that moving the pieces to a different part of the board is still bad for Black. We can also determine that other pieces besides knights and queens can participate in fork attack