nerate alargenumberofimaginary
1 Thetechnical conditionsaregiven onpage725. In Figure21.5wehaveused (n) 60 (59 n), which
satisfiestheconditions.
838 Chapter 21. Reinforcement Learning
1
0.8
0.6
0.4
0.2
0
0 100 200 300 400 500
setamitse
ytilit U
0.6
(4,3)
(3,3) 0.5
(1,3)
(1,1) 0.4
(2,1)
0.3
0.2
0.1
0
0 20 40 60 80 100
Number of trials
ytilitu
ni
rorre
SMR
Number of trials
(a) (b)
Figure21.5 The TD learningcurvesforthe 4 3 world. (a) Theutility estimates fora
selectedsubsetofstates,asafunctionofthenumberoftrials. (b)Theroot-mean-squareerror
intheestimatefor U(1,1),averagedover20runsof500trialseach. Onlythefirst100trials
areshowntoenablecomparisonwith Figure21.3.
transitions. Inthisway,theresultingutilityestimateswillapproximatemoreandmoreclosely
thoseof ADP ofcourse, attheexpenseofincreased computation time.
Ina similar vein, wecan generate more efficient versions of AD Pby directly approxi-
mating the algorithms forvalue iteration orpolicy iteration. Even though the value iteration
algorithm is efficient, it is intractable if we have, say, 10100 states. However, many of the
necessary adjustments to the state values on each iteration will be extremely tiny. One pos-
sible approach to generating reasonably good answers quickly is to bound the number of
adjustmentsmadeaftereachobservedtransition. Onecanalsouseaheuristictorankthepos-
PRIORITIZED sibleadjustmentssoastocarryoutonlythemostsignificant ones. Theprioritizedsweeping
SWEEPING
heuristic preferstomakeadjustments tostateswhose likelysuccessors havejustundergone a
large adjustment in their own utility estimates. Using heuristics like this, approximate ADP
algorithmsusuallycanlearnroughlyasfastasfull ADP,intermsofthenumberoftrainingse-
quences, butcanbeseveralordersofmagnitudemoreefficientintermsofcomputation. (See
Exercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.
Approximate AD Palgorithms haveanadditional advantage: intheearlystages oflearning a
new environment, the envir