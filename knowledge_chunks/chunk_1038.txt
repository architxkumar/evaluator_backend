ures would have co-occurred similarly with documents. This would be true for closely related features like Middle East and oil which might appear in very similar contexts in a large document corpus. In such a case, it is intuitive that we can still go ahead with merging the two columns corresponding to the two features, and construct a new feature that averages or smoothes out the two original features. This is exactly what SVD achieves when it constructs a low rank approximation. In this context, we make a critical distinction between the true rank and effective rank of a matrix. While the true rank takes into account all nonzero singular values, effective rank discards the very small ones. Thus, merging two closely related features changes the true rank but maintains the effective rank of the matrix. The ability of SVD to identify latent co-occurrence patterns is the main reason for its improved effectiveness in retrieval tasks compared to the plain vector space model based on a bag of words. Also, the new features which are referred to as concept features are expected to be more robust indicators of meaning in comparison to the original feature set. This can be viewed as a step of feature extraction. It is important to note that extracted features can be expressed as a linear weighted combination of original features. There is another notable consequence of merging features: although LSI deals reasonably well with synonymy, (Deerwester et al., 1990) observes that the solution it offers to polysemy is at best partial. This is also confirmed by the results of their experiments. The problem lies in the fact that LSI forces a word to have a single representation in the concept space; thus a word with multiple meanings is represented as the weighted average of the different meanings. It is possible that none of the real meanings is close to the average, leading to a serious distortion. Fourthly, both words and documents are treated in a uniform way by LSI. The concept