cute the
actiontheoptimalpolicyrecommends. Orshouldit?
21.3.1 Exploration
Figure 21.6 shows the results of one sequence of trials for an ADP agent that follows the
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that, in the
39th trial, it finds a policy that reaches the 1 reward along the lower route via (2,1), (3,1),
(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the
276th trial onward it sticks to that policy, never learning the utilities of the other states and
neverfindingtheoptimalroutevia(1,2),(1,3),and(2,3). Wecallthisagentthegreedyagent.
GREEDYAGENT
Repeatedexperimentsshowthatthegreedyagentveryseldomconvergestotheoptimalpolicy
forthisenvironment andsometimesconverges toreallyhorrendous policies.
Howcanitbethatchoosingtheoptimalactionleadstosuboptimalresults? Theanswer
is that the learned model is not the same as the true environment; what is optimal in the
learned modelcantherefore besuboptimal inthetrueenvironment. Unfortunately, theagent
does not know what the true environment is, so it cannot compute the optimal action forthe
trueenvironment. What,then,istobedone?
What the greedy agent has overlooked is that actions do more than provide rewards
according tothecurrentlearnedmodel;theyalsocontribute tolearning thetruemodelbyaf-
fectingthepercepts thatarereceived. Byimprovingthemodel,theagentwillreceivegreater
rewards in the future.2 An agent therefore must make a tradeoff between exploitation to
EXPLOITATION
maximize its reward as reflected inits current utility estimates and exploration to maxi-
EXPLORATION
2 Noticethedirectanalogytothetheoryofinformationvaluein Chapter16.
840 Chapter 21. Reinforcement Learning
2
1.5
1
0.5
0
0 50 100 150 200 250 300 350 400 450 500
ssol
ycilop
,rorre
SMR
3 1
RMS error
Policy loss
2 1
1
1 2 3 4
Number of trials
(a) (b)
Figure21.6 Performanceofa greedy ADP agenttha