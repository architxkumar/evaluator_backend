.1) are called eigenvectors, and each of these eigenvectors is associated with a corresponding value of A referred to as an eigenvalue. We rewrite (16.1) as (M Al) 7 0, where is an identity matrix of dimensions matching M; this is called the characteristic equation. Solving it in our example, we have the following three eigenvectors l 1 1 1 0 1 associated with the eigenvalues A, 1, Az 1 and A3 4 respectively. We now study the effect of M on any arbitrary vector x 1 x 2 3 We can express x asa linear combination of p 1: Vv 2 and yp 3 The revised position My is now given by Mx M(1v, 2v, 3v3) Mv , 2Mv, 3Mv, The interesting aspect of this rewrite is that we can see that the total effect of M on x is expressed as a weighted combination of effects due to each eigenvector. Eigenvectors, having very small eigenvalues associated with them, have a small effect on the operation of M on x: In the example above, the eigenvector associated with the eigenvalue A3 4 will have a more pronounced effect in characterizing M as an operator, compared to the two other eigenvectors, each associated with eigenvalue 1. This intuition is critical to our treatment of SVD below. Moving on to a few more definitions, a family of a finite number of vectors is said to be linearly independent if none of them can be expressed as a linear combination of the remaining ones. The rank of a matrix M (not necessarily square) is the number of linearly independent columns (or rows) in it. It can be shown that the rank of a square matrix equals the number of its nonzero eigenvalues, counted with multiplicity. We now look at an important result in factor analysis. For a given square matrix real valued m x m matrix M with linearly independent eigenvectors, we can obtain a factorization M uU4U such that the columns of U are the eigenvectors of M, and is a diagonal matrix whose diagonal elements are eigenvalues of M arranged in decreasing order. This result is due to Matrix Diagonalization Theorem (Strang, 2009) a