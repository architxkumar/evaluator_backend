the costs incurred in staying put at s4 and s7 are very low, and therefore the savings by avoiding them are low as well. Instead, if one were to increase the probability of action asg succeeding by making P(Ss5,a5g,Sg) 0.9, we get the equation, V S(s5) 0.9 5 V F(sg) 0.1 3 V79(s7) Replacing this in the set of linear equations and solving them we get the values for 775g as, VG (25.8583, 21.0861, 33.525, 33.525, 8.89536, 40.9536, 40.9536 As expected, the increased probability of going from ss to sg has reduced the chance of the system looping back to s, and accumulating more cost. In fact, the expected cost of going from s, to sg is now 25.86, which is much closer to the ideal cost of 20. On the other hand, increasing the probability of going from ss to sg has not affected the expected costs for policy Tl2g because the policy drives the solution through sz. The only state for which the value has gone down is ss, which is the cost if you happen to start from there. Observe that even this is a little higher than the cost from ss in TI56, because when the action asg does not result in reaching sg, which happens 10 of the times, the agent has to go back to s; and find a path via So. Vi2G (38.2075, 24.7908, 45.8741, 45.8741, 10.1303, 53.3027, 53.3027 Solving a set of equations with N variables can be quite expensive. Instead, one can also adopt an iterative approach to evaluating a policy. Iterative Policy Evaluation The iterative policy evaluation algorithm initializes the values of all variables and then iterates through the linear equations by computing the new left hand sides using the old values in the right hand side. Let V , stand for the value function in the n" iteration. The values for each state variable that is not a goal state are then updated as, V a(S) Lye s Pls, a, ) C(s, a, 8.) V7,(5 ) Applying this iterative process, the sets of equations for the policy Tlgg, we have the following iterations in which V5, is replaced with 0 for all n. VES wi(S1) O0.9 15 VS,