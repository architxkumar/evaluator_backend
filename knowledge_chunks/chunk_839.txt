inferences about what might have happened, even though it was not stated explicitly. The following example from Cullingford s work shows how SAM could answer questions in a knowledgeable fashion. Input story (Culligford, 1981) John went to a restaurant. He ordered a hot dog. The waiter said they didn t have any. He asked for a hamburger. When the hamburger came, it was burnt. He left the restaurant. The following is the question-answer session with SAM (assisted with English by ELI and BABEL). Q1: Did John sit down in the restaurant? SAM : Probably. Q2: Did John order a hot dog? SAM: Yes. Q3 : Did John eat a hot dog? SAM: No, the waiter told John the management was unable to give it to him. Q4 : What did the waiter serve John? SAM: The waiter served John a hamburger. Q5 : Why didn t John eat the hamburger? SAM: Because the hamburger was overdone. Q6 : Did John pay the check? SAM: No. John was angry because the hamburger was overdone and so he left the restaurant. Observe that the Script helps interpret sentences like when the hamburger came correctly as the hamburger being in fact served by the waiter. This is partly because SAM has access to a semantic module that can match categories with actions, and partly because the role definitions in the script. Both dictate that an inanimate object cannot move by itself and has to be carried by an animate one. Answering questions is not a straightforward task. First, one has to understand the question and figure out what the asker wants to know, and then supply the pertinent information. For instance the correct response to the question Could you tell me the time? is the time at that moment, and not Yes , even though the latter seems on surface to be appropriate. Many such issues of question answering were investigated by Wendy Lehnert in her doctoral thesis as part of the Yale group (Lehnert, 1978). 14.5.1 MOPs A Script is a data structure that stands in isolation. As the problem solvers experience increases, the number of