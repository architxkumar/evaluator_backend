by symmetric weights. Units update their states asynchronously by looking at their local connections to other units. In addition to serving as content-addressable memories, Hopfield networks can solve a wide variety of constraint satisfaction problems. The idea is to view each unit as a hypothesis, and to place positive weights on connections between units representing compatible or mutually supporting hypotheses, and negative weights on connections between units representing incompatible hypotheses. As the Hopfield net settles into a stable state, it attempts to assign truth and falsity to the various hypotheses while violating as few constraints as possible, We see examples of how neural networks attack real-world constraint satisfaction problems in Section 18.3. Connectionist Models 391 A OCS The main problem with Hopfield networks is that they settle into local minima. Having many local minima is good for building content-addressable memories, but for constraint satisfaction tasks, we need to find the globally optimal state of the network. This state corresponds to an interpretation that satisfies as many interacting constraints as possible. Unfortunately, Hopfield networks cannot find global solutions because they settle into stable states via a completely distributed algorithm. If a network reaches a stable state like state A in Fig. 18.4, then no single unit is willing to change its state in order to move uphill, so the network will never reach globally optimal state B. If severa] units decided to change state simultaneously, the network might be able to scale the hill and slip into state B. We need a way to push networks into globally optimal states while maintaining our distributed approach. At about the same time that Hopfield networks were developed, a new search technique, called simulated annealing,. appeared in the literature. Simulated annealing, described in Chapter 3, is a technique for finding globally optimal solutions to combinatorial problems. H