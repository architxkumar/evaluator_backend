rom the inputs it sees. We would like to build these neural masses on a very large scale, and we would like them to be able to learn efficiently. Perceptrons got us part of the way there, but we saw that they were too weak computationally. So we turn to more complex, multilayer networks. | What can a multilayer network compute? The simple answer is: anything! Given a set of inputs, we can use summation-threshold units as simple AND, OR, and NOT gates by appropriately setting the threshold and connection weights. We know that we can build any arbitrary combinational circuit out of those basic logical units. In fact, if we are allowed to use feedback loops, we can build a general-purpose computer with them. The major problem is learning. The knowledge representation system employed by neural nets is quite opaque: the nets must learn their own representations because programming them by hand is impossible. Perceptrons had the nice property that whatever they could compute, they could learn to compute. Does this property extend to multilayer networks? The answer is yes, sort of. Backpropagation is a step in that direction. It will be useful to deal first with a subclass of multilayer networks, namely fully connected, layered, t t t feedforward networks. A sample of such a network is shown in Fig. 18.14. In this figure, x, 2; and 0,, represent unit activation levels of input, hidden, and output units. Weights on connections between the input and hidden layers are denoted here by w1,, while weights on connections between the hidden and output layers are denoted by w2,,. This network has three layers, although it is possible and sometimes useful to have more. Each unit in one layer is connected in the forward direction to every unit in the next layer. Activations flow from the input layer through the hidden layer, then on to the output layer. As usual, the knowledge of the network is encoded in the weights on connections between units. In contrast to the parallel relaxatio