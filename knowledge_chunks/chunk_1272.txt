understood by considering the error in classification in the first stage of the Perceptron, before the Signum function has been applied (Mitchell, 1997). Equation (18.54), which is the linear weighted sum of the input, is reproduced below. 2-0. n WiFX Now, given a weight vector W wo, ..., w, one can define the total error E(W) when this weight vector is used for classifying all the examples X;, ..., Xj7 in the training set T containing 7 training instances as follows. E(W) (s) 2yer (ty 2x0" (18.59) where ty is the target output for the training example X and 2, is its linear sum. It is half the sum of squares of errors for each example, and is always a non-negative quantity. Observe that it is no longer modulated by the signum function and is a continuous function. For every possible weight vector, this value is defined and one can define a hypersurface in n 2 dimensional space, where the (n 2)"dimension is the magnitude of this error. When n 1, this can be seen as a two dimensional surface in a three dimensional space. It turns out that, for the above error definition, this is always a smooth parabolic surface with one global minimum. The Hill Climbing or Gradient Descent algorithm (see Chapter 3) is a perfectly suited for exploring this surface to find the global minimum. The global minimum is the best one can do, given the set of training examples. Observe that if the examples are linearly separable, the minimum error will be zero. The gradient descent rule or the delta rule moves the weight vector down the steepest gradient in each step. The gradient of E(W) with respect to the weight vector W is itself a vector that points in the direction of the steepest ascent. This can be written as (Mitchell, 1997), (0E OF OE 18.60 azor) (2e Ow, 7 Ow, y The derivative of the error surface is a vector made up of the partial derivatives of the error in the dimension of each weight. When we want to minimize the error value, the weight adjustment must be done in the opposite di