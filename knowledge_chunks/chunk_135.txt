oint of the search is determined by the starting point, and the nature of the surface defined by the evaluation function. If the surface is monotonic then the end point is the global optimum; otherwise the search ends up at an optimum that is in some sense closest to the starting point, but may only be a local optimum. terated Hill Climbing exploits this property by doing a series of searches from a set of randomly selected different starting points. The hope is that the best value found by at least one of the searches will be the global optimum. The algorithm can be written as shown in Figure 4.1. IteratedHillClimbing (n) 1 node random candidate solution 2 bestNode node 3 for ic lton 4 do node random candidate solution 5 newNode Head (Sort, (MoveGen (node) )) 6 while h(newNode) h(node) for maximization 7 do 8 node newNode 9 newNode Head (Sort, (MoveGen (node) )) 10 if h(newNode) h(bestNode) il then bestNode newNode 12 return bestNode FIGURE 4.1 Algorithm terated Hill Climbing (IHC) for a maximization problem. It does a number of Hill Climbing runs from random starting points. The terated Hill Climbing approach will work well if the surface defined by the evaluation function is smooth at the local level, with perhaps a small number of local optima globally. Such a surface is illustrated in Figure 4.2. booosoccnacocobessssssssedcocoooooNN000000000000 . i i FIGURE 4.2 A smooth surface with a small number of local optima is well suited for Iterated Hill Climbing. A random starting point in any iteration from any of the shaded nodes would lead to the global maximum. The HC algorithm has the same space requirements as Hill Climbing. Both need a constant amount of space to run. The difference is that for different runs on the same problem, the Hill Climbing algorithm will always return the same result; while the HC algorithm may return different results. This is because its performance is determined by the random choice of the random starting points. Figure 4.2 shows a se