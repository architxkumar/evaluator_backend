1,2..... . n. General multilayer networks having n nodes (number of rows) in each of m layers (number of columns of nodes) will have weights represented as an n x in matrix W. Using this representation, nodes having no interconnecting links will have a weight value of zero. Networks consisting of more than three layers would, of course, be correspondingly more complex than the network depicted in Figure l5.. A neural network can be thought of as a black box that transforms the input vector x to the output vector y where the transformation performed is the result of the pattern of connections and weights, that is, according to the values of the weight matrix W. Consider the vector product X * W = There is a geometric interpretation for this product. It is equivalent to projecting one vector onto the other vector in n-dimensional space. This notion is depicted in Figure 15.9 for the two-dimensional case. The magnitude of the resultant vector is given b x * w = JX11w1 Cos o whee Jx J denotes the norm or length of the vector x. Note that this product is maximum when both vectors point in the same.directjon, that is, when 0 0. The WI1 WI, WI, LE VI I,, V., layer 1 layer 2 layer 3 F ne ig tw u o r r e k 15.8 A mulillayer neuraf Sec. 15.3 Nonproducti On System Architectures 345 Figure 15.9 Vector muttiplicatictn is tike vector projection. product is a minimum when both point in opposite directions or when 0 = 180 degrees. This illustrates how the vectors in the weight matrix W influence the inputs to the nodes in a neural network. Learning pattern weights. The interconnections and weights W in the neural network store the knowledge possessed by the network. These weights must be preset or learned in some manner. When learning is used, the process may be either supervised or unsupervised. In the supervised case, learning is performed by repeatedly presenting the network with an input pattern and a desired output response. The training examples then consist of the vector pai