etween graphs, strings, and even FOPL descriptions. In any case, it is assumed each object o is described by a unique point or event in the feature space F. Up to this point we have ignored the problem of attribute scaling. It is possible that a few large valued variables may completely dominate the other variables in a similarity measure. This could happen, for example, if one variable is measured in units of meters and another variable in millimeters or if the range and scale of variation for two variables are widely different. This problem is closely related to the feature selection problem, that is, in the assignment of weights to feature variables on the basis of their importance or relevance. One simple method for adjusting the scales of such variables is to use a diagonal weight matrix W to transform the representation vector X to X' = WX. Thus, for all of the measures described above, one should assume the representation vectors X have been appropriately normalized to account for scale variations. To summarize the above process, a subset of characteristic features which represent the a, are first selected. The features chosen should be good discriminators in separating objects from different classes, relevant, and measurable (observable) at reasonable cost. Feature variables should be scaled as noted above to prevent any swamping effect when combined due to large valued variables. Next, a suitable metric which measures the degree of association or similarity between objects should be chosen, and an appropriate clustering algorithm selected. Finally, during the Sec. 13.4 Recognizing and Understanding Speech 231 clustering process, the feature variables may need to be weighted to reflect the relative importance of the feature in affecting the clustering. 13.4 RECOGNIZING AND UNDERSTANDING SPEECH Developing systems that understand speech has been a continuing goal of Al researchers. Speech is one of our most expedient and natural forms of communication, and so 