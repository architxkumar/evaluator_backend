presentations, every learning system is biased, because it learns some things more easily than others. In our example, the bias is fairly simple e.g., we can learn concepts that have to do with car manufacturers, but not car owners. In more complex systems, the bias is less obvious. A clear statement of the bias of a learning system is very important to its evaluation. origin {Japan, USA, Britain, Germany, Italy manutacturer {Honda, Toyota, Ford, Chrysler. Jaguar, BMW, Fiat color {Blue, Green, Red, White} decade {1950, 1960, 1970, 1980, 1990, 2000} type e {Economy, Luxury, Sports} Fig. 17.8 Representation Language for Cars Concept descriptions, as well as training examples, can be stated in terms of these slots and values. For example, the concept Japanese economy car can be represented as in Fig. 17.9. The names x,, x , and x, are variables. The presence of x2, for example, indicates that the color of a car is not relevant to whether the car is a Japanese economy car. Now the learning problem is: Given a representation language such as in Fig. 17.8, and given positive and negative training examples such as those in Fig. 17.7, how can we produce a concept description such as that in Fig. 17.9 that is consistent with all the training examples? origin : Japan manufacturer : x X, color: Xp decade : Xy type: Economy Fig, 17.9 The Concept japanese economy car Before we proceed to the version space algorithm, we should make some observations about the representation. Some descriptions are more general than others. For example, the description in Fig. 17.9 is more general than the one in Fig. 17.7. In fact, the representation language defines a partial ordering of descriptions. A portion of that partial ordering is shown in Fig. 17.10. The entire partial ordering is called the concept space, and can be depicted as in Fig. 17.11. At the top of the concept space is the null description, consisting only of variables, and at the bottom are all the possible training instances, 