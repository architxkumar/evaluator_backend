e goals. We call such goals that we are willing to do without if need be, as soft goals or soft constraints or preferences. Preferences are desired goals and trajectory constraints and some preferences may be preferred more than others. For example, someone may have the goal of booking a seat in a train and have a preference of booking a window seat. Or someone planning an outing may have a preference of eating in a South Indian restaurant, with Ethiopian being a close second. Office etiquette may require that one knocks before opening a colleague s door. Planning a walk in Chennai one may have a preference to be in the shade at all times. Someone may have a desire (soft trajectory constraint) that at least once in his lifetime he will have a chance to behold the Knhangchendzonga. Each preference can be assigned a weight that determines the cost of violating that preference. The evaluation of a plan may incorporate such costs along with other costs of actions. Observe, that this converts the planning task to an optimizing problem in which one could continue to look for better solutions till an optimal one has been found. Different solutions achieve different (soft) goals. This is in contrast to the planning algorithms we have studied so far that are designed to find a least cost solution for the same problem represented by strong goals and trajectory constraints. Another difference with earlier approaches is that specifying preferences in a specific problem instance allows a user to optimize on different features at different times. An approach to adapting Graphplan for solving preferences may be to first extend the planning graph till all the strong goals are solved then extend the graph further in an attempt to solve any preferences that are not satisfied. The process of extending the plan (with more actions) will terminate when the cost of adding more actions outweighs the cost of leaving preferences unsatisfied. In PDDL3.0 (Gerevini and Long, 2005), a preference