 by Marvin Minsky, is of temporal credit assignment. When a sequence of actions leads to some eventual reward, how does one assign the credit of the final result to individual actions? This has been studied in the computer science community as the problem of Reinforcement Learning (RL). Reinforcement Learning evolved from the convergence of work in psychology and control theory. Psychologists and physiologists have always been interested in learning. The Russian physiologist lvan Pavlov (1849-1936) carried out some well known experiments which demonstrated that a dog could learn the association between a bell ringing and food being served, a process he called the development of conditioned reflexes . Numerous experiments have been conducted to demonstrate that rats can learn mazes' . The study of learning amongst humans has also been a subject matter of considerable interest. While the people working in control were interested in discovering optimal policies, given the reward function and the probability of making moves, the RL community was focused on learning by interacting with the environment. The form of learning is omnipresent amongst humans. A child may extend its hands towards a candle flame, but after a few instances of feeling the heat, quickly learns to stay away. An older child who may learn that pestering her parents in a marketplace always results in her getting a plate of jalebis, learns to do so more often. The most studied approach to reinforcement learning in the computing community has been by modelling the problem as a Markov Decision Process (MDP). Most of the work has been done in the area of control engineering in which the system has to decide upon an action in each state it is in. The intent or plan of the agent is expressed as policy that defines the action that has been taken in each state. In Section 17.6, we studied how an optimal policy for achieving a goal can be learnt when the actions are stochastic in nature. That is, the agent is n