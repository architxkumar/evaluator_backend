ssed) where similar inputs get mapped onto 388 Artificial Intelligence 6. Propagate the activations from the units in the hidden layer to the unit in the output layer. 0,= = for all Jz C 4 yey 1 +e t=O 2y hy Again, the thresholding weight w2p ; for output unit j plays a role in the weighted summation, h, is always 1.0. 7. Compute the errors of the units in the output layer denoted 62;. Errors are based on the network s actual output (o,) and the target output (y,). 62; = 0,( 1 0G; @) for all j = 1,..., C 8. Compute the errors of the units in the hidden layer, denoted 51 je Cc 61, = hl h) Y52,- w2,,= for all j= 1,..., B i=l 9. Adjust the weights between the hidden layer and output layer.* The leaming rate is denoted 7); its function is the same as in perceptron learning. A reasonable value of 77 is 0.35. Aw2, = 7)- &2;- h, forall i=0,...B, f=l.C 10. Adjust the weights between the input layer and the hidden layer. forall i=0,..,A, j=l, B 11. Go to step 4 and repeat. When all the input-output pairs have been to the network, one epoch has been completed. Repeat steps 4 to 10 for as many epochs as desired. f Awl, =7)- 51; x, The algorithm generalizes straightforwardly to networks of more than three layers. For each extra hidden layer, insert a forward propagation step between steps 6 and 7, an error computation step between steps 8 and 9, and a weight adjustment step between steps 10 and 11. Error computation for hidden units should use the equation in step 8, but with i ranging over the units in the next layer, not necessarily the output layer. The speed of learning can be increased by modifying the weight modification steps 9 and 10 to include a momentum term o. The weight update formulas become: Aw2,(t + 1) = 7-62; +h, + a Aw2,(2) Awl (t+ 1) = 77-61; - 4; + GAw1,(0) where hy, x, 6 |, and 62; are measured at time + 1. Aw,(1) is the change the weight experienced during the gy previous forward-backward pass. If a is set to 0.9 or so, learning speed is improved. * The err