x words. Also, an additional set of parameters may be needed to capture the fact that a Hindi word may appear out of nowhere, i.e. when there is no corresponding English word. We make two observations here. First, the number of parameters that need to be estimated is huge. Secondly, it appears quirky that such a strange scheme should ever work. Even the staunchest advocate of statistical MT would find it hard to justify that there is any remote resemblance to how humans do translation. It may also appear that a very large hand annotated parallel corpus of translated sentences would be needed to make robust estimates of these parameters. Even if such large corpora exist, they do not come with word-for-word alignments. However, it is possible to obtain estimates from non-aligned sentence pairs using the Expectation Maximization (EM) algorithm, which is a technique for parameter optimization in the face of missing values. Details of EM algorithm are presented in Chapter 18. The basic idea is to start with arbitrary values of the parameters and keep refining them in successive iterations. Each pair of sentences in the source and target language, places constraint on the values the parameters can take. Thus, the parameter estimation problem reduces to an interesting problem, akin to the cracking of a Sudoku puzzle, given a set of standard constraints. 16.3.5 Text Summarization An interesting application of NLP is in automatically generating summaries from natural language text. The generation could be extractive summarization, in which parts of the text (say, sentences) from the source are used verbatim to create summaries. In contrast, abstractive summarization is harder, in that it involves detailed interpretation of the text and regeneration of the substantive content. The process of summarization can be broken down into three major steps. The first is topic identification, the goal of which is to return the highest scoring units (sentences) based on their suitability