ly avoiding certain redundant computations. The details of this algorithm are presented in (Dasgupta et al., 2006). There is yet another interesting way of addressing the spell-check problem. Consider a wrongly spelt word W and words A, B, C that are candidate corrections for W. Let P(A W) be the posterior probability that A is the correct replacement for W. By using Bayes rule of probability, this is a posterior term which is proportional to the product of two quantities: the likelihood term P(WA) and the prior term P(A). Similarly, the likelihood and prior terms are computed for B and C as well. The word with highest posterior probability is chosen as the replacement for W. While the likelihood term takes care of systematic processes (like keyboard proximity of letters) that lead to a typo, the prior term ensures that a more frequent word is favoured over a less frequent one. The prior probabilities can be estimated using a standard corpus. (Kernighan et al., 1990) describes an approach to estimate the likelihood by recording frequencies of replacement of each English letter by another. 16.2.2 Morphology using Finite State Transducer In a way not very different from how words act as building blocks for sentences, words themselves are built up from a sequence of morphemes. Computational morphology encompasses two areas. The first is analysis, where a word is broken into its constituent morphemes. The second is synthesis, wherein a word is composed from morphemes. While it may appear that parsing a word to its morphemes is a simpler task than parsing a sentence, there are still interesting challenges. Consider the ambiguity in parsing the word foxes (a plural noun or a singular verb) for example, which cannot be resolved without access to contextual information. Three different sources of knowledge are necessary for morphological processing. The first is the exicon which has a listing of words and morphemes along with their roles (for example, adding an s gives the 