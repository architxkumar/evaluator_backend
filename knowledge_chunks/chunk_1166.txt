DPs in which there is a bound on the number of actions that can be executed. The second, infinite horizon MDPs with discounted costs and rewards. The idea here is that a discounting factor Osys1 gives lower weight to costs and rewards in the future. The lower is the value of y, the lesser the importance given to the (distant) future. The third class of MDPs are indefinite horizon MDPs in which the task is expected to be completed in a finite time, but with uncertainty in the time duration (like an aeroplane circling over an airport to get landing clearance, or a girl attempting to shoot a basket on a basketball court). It turns out that all three kinds of MDPs can be generalized into a stochastic shortest path MDP (SSP MDP) described below (Mausam and Kolobov, 2012). Stochastic shortest path MDPs introduce the notion of goal states. In that sense, they are similar to classical planning problems. However, they have a notion of costs that can incorporate the notion of rewards used in MDPs. By changing the sign of a positive reward, it can be incorporated into the cost function. In this way, SSP MDPs can allow for preferences and trajectory constraints as well. An SSP MDP is defined as a tuple S, A, P, C, G where, Sis a finite set of states. Ais a finite set of actions. P:Sx Ax S- 0,1 is the state transition probability P(s, a, s ') of going from state s to state s by action a applied is state s. C:SxAx S- 0, ) is the nonzero cost C(s, a, s ') 0 of going from state s to state s by action a applied is state s. The cost of going to a goal state from the goal state is however 0. G GS is a set of goal states which satisfy the following properties. For all sg G, for all a A and for all s e G, P(Sg, a,s ) 0, no action can take the system away from the goal state, P(Sg, a,Sg) 1, any action in the goal state keeps it in the goal state, C(Sq a,Sg) 0, the cost of staying in the goal state is 0. There is a condition that the SSP should have a proper policy. A policy is called pro