continuations from a given position, each time using randomly generated dice throws. The rollout score of a move is the average score obtained by the continuations commencing with that move. The move with the highest rollout score is judged to be the best move. And because the computer can play a very large number of games, the results have proven to be reliable; so much so that computer rollouts have become the standard method of evaluating Backgammon moves (Woolsey, 2000; Montgomery, 2000). Tesauro has suggested that with increasing computing power, one will be able to use the Monte Carlo rollout methods to produce much better programs. This approach also forms the basis of one of the successful Bridge playing programs (Ginsberg, 1999, 2001) and in the end game in Scrabble (Sheppard, 2002). Sheppard did try and use TD learning for the entire game of Scrabble but without success. He suggests that the algorithm works with spectacular results in Backgammon because the dice throws produce a sufficient number of states to cover the entire space; while in the case of Scrabble, the approach gets caught in local optima. The two approaches used in Neurogammon and TD-Gammon are both learning systems in which neural networks are trained to evaluate positions. But they differ widely in the source of learning. Neurogammon learnt from moves made by experts and as a consequence its knowledge was a reflection of the experts knowledge. TD-Gammon, on the other hand, started from scratch and learned by playing a large number of games against itself, rewarding moves that led to winning positions. It quickly developed the basic concepts in the game, and went on to become a champion level player. Its learning was unsupervised in contrast to the supervised learning of Neurogammon. In the process, a curious thing happened. Many of the notions that human experts held were proven to be wrong, eventually leading to modification of the expert knowledge itself. An example given in (Tesauro, 1