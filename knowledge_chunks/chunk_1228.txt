 1 repeat for i lton for each y,cY a (ys) Plaslya) 2 P( ysl s-1) 0-1 (Yi-n) det n W PWN return P(X) 2 P(Ely,) ,(y,) , FIGURE 18.5 The ForwardAlgorithm is used to find the probability of a sequence X being generated from a given HMM. 18.3.3 Finding Most Probable Label Sequence The second inference problem is concerned about finding the most probable sequence of labels Y, corresponding to a given observation sequence X, for a given HMM with parameters 9. Formally, Y argmax P(X, Y: 6) (18.28) Y The most probable label sequence can be obtained by enumerating all possible label sequences generating X and then calculating joint probability between observation symbol and each label sequence. As described in the previous section, such an approach is computationally expensive. An efficient algorithm can be designed for this task using dynamic programming technique, similar to forward algorithm (Figure 18.5), by storing results of intermediate computations. This algorithm (Figure 18.6) is known as Viterbi algorithm or max sum algorithm. The most likely label sequence for observation sequence till position can be computed as follows. PR 82.0 MV V 20 Sn) Pay) max PO Wx) PO. ta Pp Yea (18.29) Let us define viterbi variable y as follows: WO) PC Mb Jen) (18.30) y(B) 1 (18.31) Now Eq. (18.32) becomes POR Xa V1 Pe ND) PO) res Poibi-D 707-01 (18.32) In addition, we store pointer to y; , in (i 1) for constructing the most probable label sequence. G 1) argmax PO.) 101) (18.33) Yay G() B (18.34) Thus, the most probable label sequence for the observation sequence and the corresponding probability can be obtained at label E by the following recursions. YE) max PEb,) 10) (18.35) yn Y O(n) argmax P(Ely,) 0',) (18.36) ney The (E) is the probability of the most likely label sequence that generates X and (n) gives the corresponding label sequence. Y argmax P(X, Y: 0) (18.37) y Y (n) (18.38) Viterbi (HMM: H( ), Observations: X, Parameters: 6) 1 wH(B) 1 2 repeat 3 for i l1ton 4 for each y,e Y 5 