g. The interested reader is referred to (Coles et al., 2009) for more details. A more explicit separation of the sequencing of actions from their selection is implemented in the system POPF (Coles et al., 2010). The algorithm for generating the relaxed planning graph adapted from (Coles et al., 2008) is described in Figure 10.26. The algorithm is designed with the following features, not all of which were present in the relaxed temporal planning graphs used by earlier planners. TemporalRPG (S F, E, T ) 1 ie F 2 t -0 3 for each a, 4 if empty ee E .op A 5 then earliest(A) 6 else earliest(A) 0 7 while t 8 fine - fF, 9 actions, a, precond(a )C f, A earliest(A) t 10 for each new a actions, 11 fire fire VU add (a,) 12 actions, actions, VU a precond(a)c f, 13 for each new a. actions, 14 fuse fare U add (ay) 15 earliest(a) min(earliest(a), t 1b(a)) 16 if ELC fuse 17 then tett e 18 else endpoints earliest(A) t precond(a jc f, 19 if empty (endpoints) 20 then break 21 else t min(endpoints) 22 return R f o, nr Go, n FIGURE 10.26 Algorithm TemporalRPG builds a temporal relaxed planning graph. 1. Respect the PDDL2.1 start-end semantics. No action may be left executing in the goal state. In other words, E must be empty at termination. This implies that the construction of the RTPG may continue even after the goals appear. 2. Respect relations between start and end actions. The latter can only be applied after the former. 3. Take action durations into account. Insert an end action, only if the time stamp has moved sufficiently for that action to finish. The resulting RTPG ensures that a dead end is never signalled incorrectly, and also takes into account durations of action. The algorithm accepts as input a state description of the form S F, E, T and returns a relaxed planning graph made up of layers of fluents and actions, fp... p, actionsg , . We assume that it has access to the set of ground snap actions needed for planning as well as durations of actions. The algorithm begins b