ear that much can be learnt from a process of trial and error. Let us consider a simple example of playing a game. Suppose someone teaches a new game called tic-tac-toe that you were not familiar with. You could simply play out a few games with yourself and figure out that playing the first move in the corner is rewarding . Or say, two children play a million chess games against each other, and one of them experiments by opening with PK4 versus PKB3. The child would most likely realize at some point that PK4 is a much better move to start a chess game with. Of course, you cannot expect children to play a million games. But nothing stops a game playing program from doing that. In fact, such a program could simply play those games against itself, and learn it in double quick time. That is what the Scrabble playing program Tesauro did. One problem one faces when computing MDPs is the curse of dimensionality. One problem faced while learning policies in MDPs is that the number of states for which the action has to be specified is often far too many. As a corollary, the credit assignment problem also becomes a hard problem because in MDPs, the credit assignment is done by a process, for example value iteration that computes the immediate reward that an action results in. In game playing algorithms when the moves are deterministic, one can adopt a different approach. Since the moves are deterministic, the resulting state after an action is well defined. We consider two person games of the kind studied in Chapter 8. Then, if one can aim to learn a generic evaluation of the kind used in game playing algorithms, each state can be associated with a value. The difference is that the evaluation function is one generic function that can be applied to any state. Given such a function, one can adopt a different approach for move selection. Instead of the policy specifying a move for each state, one can generate the successors of a given state, apply the evaluation function, and ch