es
12 12
Figure18.3 Examplesfortherestaurantdomain.
is shown in Figure 18.3. The positive examples are the ones in which the goal Will Wait is
true(x ,x ,...);thenegativeexamplesaretheonesinwhichitisfalse (x ,x ,...).
1 3 2 5
We want a tree that is consistent with the examples and is as small as possible. Un-
fortunately, no matter how we measure size, it is an intractable problem to find the smallest
consistent tree; thereisnowaytoefficiently search through
the22n
trees. Withsomesimple
heuristics, however,wecanfindagoodapproximate solution: asmall(butnotsmallest)con-
sistenttree. The DECISION-TREE-LEARNING algorithmadoptsagreedydivide-and-conquer
strategy: always test the most important attribute first. This test divides the problem up into
smaller subproblems that can then be solved recursively. By most important attribute, we
meantheonethatmakesthemostdifferencetotheclassificationofanexample. Thatway,we
hopetogettothecorrectclassificationwithasmallnumberoftests,meaningthatallpathsin
thetreewillbeshortandthetreeasawholewillbeshallow.
Figure18.4(a)showsthat Type isapoorattribute,becauseitleavesuswithfourpossible
outcomes,eachofwhichhasthesamenumberofpositiveasnegativeexamples. Ontheother
hand,in(b)weseethat Patrons isafairlyimportantattribute,becauseifthevalueis None or
Some,thenweareleftwithexamplesetsforwhichwecananswerdefinitively(No and Yes,
respectively). Ifthe valueis Full,weareleftwithamixedsetofexamples. Ingeneral, after
the first attribute test splits up the examples, each outcome is a new decision tree learning
probleminitself,withfewerexamplesandonelessattribute. Therearefourcasestoconsider
fortheserecursive problems:
1. If the remaining examples are all positive (or all negative), then we are done: we can
answer Yes or No. Figure 18.4(b) shows examples ofthis happening in the None and
Some branches.
2. Iftherearesomepositiveandsomenegativeexamples,thenchoosethebestattributeto
splitthem. Figure18.4(b)shows Hungry beingusedtosplittheremainingexamples.
3. 