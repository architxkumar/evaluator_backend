 the total number of incorrect firings versus incorrect misfirings. Soon, w, will become large enough to overpower wp, while w, and w; will not be powerful enough to fire the perceptron, even in the presence of both x, and x3. Now let us return to the functions g(x) and o(x). While the sign of g(x) is critical to determining whether the perceptron will fire, the magnitude is also important. The absolute value of g(x) tells how far a given input 382 Artificial Intelligence vector lies from the decision surface This gives us a way of characterizing how good a set of weights is. Let W be the weigh vector (wp, Wy, -... W,) and let X be the subset of training instances misclassified by the current set of weights. Then define the perceptron criterion function, J( ), to he the sum of the distances of the misclassified input vectors from the decision surface: I= PV /Ywx) Y [75 e X |i=0 ex To create a better set of weights than the current set, we would like to reduce J(#'). Ultimately. if all inputs are classified correctly, J(w) = 0. How do we go about minimizing J() ? We can use a form of local-search hill climbing known as gradient descent. We have already seen in Chapter 3 how we car use hill-climbing strategies in symbolic AT systems. For our current purposes, think of J() as defining a surface in the space of all possible > weights. Such a surface might look like the one in Fig. 18.10. \ J In the figure, weight wy should be part of the weight space AS [J but is omitted here because it is easier to visualize J in only > E> ned three dimensions. Now, some of the weight vectors constitute solutions, in that a perceptron with such a weight vector will classify all its inputs correctly. Note that there are an infinite number of solution vectors. For any solution vector w,, we know that J(w,) = 0. Suppose we begin with a random weight vector W that is not a solution vector. We want to slide down the / surface. There is a mathematical method for doing this we compute the gra