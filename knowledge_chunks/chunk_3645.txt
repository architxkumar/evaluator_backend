 - ....... .... . .----. .s _. Ys No F'igure 17.1 A sirnpic perceptron. Sec. 17.2 Perceptrons 369 0 o 00/ 0/4. 1+ + 0 0/4. + - 00 /+ + 0 00/ + + + + 0/ + 0 Figure 17.2 Geometrical illustration of separable space. ues until optimal values of w are found at which time the system will have learned the proper classification of objects for the two different classes. The light sensors produce an output voltage that is proportional to the light intensity striking them. This output is a measure of what is known in pattern recognition as the object representation space parameters. The outputs from the ATUs which combine several of the sensor outputs (xe) are known as feature value measurements. These feature values are each multiplied by the weights w and the results summed in the comparator to give the vector product r = IN * x w,. When enough of the feature values are present and the weight vector is near optimal, the threshold will be exceeded and a positive classification will result. Finding the optimal weight vector value w is equivalent to finding a separating hyperplane in k-dimensional space. If there is some linear function of the x, for which objects in class-I produce an output greater than T and non-class-I objects produce an output less than T, the space of objects is said to be linearly separable (see Chapter 13 for example). Spaces which are linearly separable can be partitioned into two or more disjointed regions which divide objects based on their feature vector values as illustrated in Figure 17.2. It has been shown (Minsky and Papert, 1969) that an Optimal w can always be found with a finite nuiither of training examples if the space is linearly separable. One of the simplest algorithms for finding an optimum w (w*) is based on the following perceptron learning algorithm. Given training objects from two distinct classes, class-I and class-2 1. Choose an arbitrary initial value for w 2. After the rn" training step set = w,,, + d * x where d= +1 if r = w * 