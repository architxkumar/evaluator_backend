ceivedinthatstate. Typicaltrialsmightlooklikethis:
(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(4,3)
-.04 -.04 -.04 -.04 -.04 -.04 -.04 1
(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(3,2) (cid:2)(3,3) (cid:2)(4,3)
-.04 -.04 -.04 -.04 -.04 -.04 -.04 1
(1,1) (cid:2)(2,1) (cid:2)(3,1) (cid:2)(3,2) (cid:2)(4,2) .
-.04 -.04 -.04 -.04 -1
Notethat each state percept is subscripted withthe reward received. Theobject istouse the
informationaboutrewardstolearntheexpectedutility U (s)associatedwitheachnontermi-
nal state s. Theutility is defined to be the expected sum of (discounted) rewards obtained if
Section21.2. Passive Reinforcement Learning 833
policy isfollowed. Asin Equation(17.2)onpage650,wewrite
" (cid:12) U (s) E t R(S ) (21.1)
t
t 0
where R(s)istherewardforastate, S (arandomvariable)isthestatereachedattimetwhen
t
executing policy , and S s. Wewillinclude a discountfactor inall of ourequations,
0
butforthe4 3worldwewillset 1.
21.2.1 Directutility estimation
DIRECTUTILITY A simple method for direct utility estimation was invented in the late 1950s in the area of
ESTIMATION
ADAPTIVECONTROL adaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a state
THEORY
is the expected total reward from that state onward (called the expected reward-to-go), and
REWARD-TO-GO
eachtrialprovides a sampleofthisquantity foreachstatevisited. Forexample, thefirsttrial
in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two
samples of0.76 and 0.84 for(1,2), twosamples of0.80 and 0.88 for(1,3), and so on. Thus,
attheendofeachsequence,thealgorithmcalculatestheobservedreward-to-goforeachstate
andupdatestheestimatedutilityforthatstateaccordingly, justbykeeping arunningaverage
foreachstateinatable. Inthelimitofinfinitelymanytrials,thesampleaveragewillconverge
tothetrueexpectation in Equation(21.1).
It is clear that direct utility estimation is just an instance of