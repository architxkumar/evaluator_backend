ays return the same result; while the HC algorithm may return different results. This is because its performance is determined by the random choice of the random starting points. Figure 4.2 shows a set of shaded nodes from where the steepest gradient leads to the global maximum. Let us call this set the footprint of the HC algorithm. If in one of the outer loop iterations of IHC, a point in this region is chosen as the starting point then the algorithm will find the optimal solution. The likelinood of IHC finding the global optimum depends upon the size of the footprint. The larger the footprint is, the greater is the chance of starting in it, and finding the optimum. If the footprint covers the entire search space then HC itself will work. As the footprint gets smaller, one would need a larger number of iterations in the outer loop to have a good chance of finding the optimum. If the footprint is very small then the number of iterations in the outer loops may become prohibitively large, and one may have to look for an alternative approach. Such an evaluation function surface is depicted below in Figure 4.5, and the next section describes an approach that might work better there. 4.2 Simulated Annealing The algorithms we have seen so far have all depicted deterministic search moves. The HC algorithm above introduces a randomized aspect by choosing a series of random starting points followed by deterministic moves in the search space. We now look at the possibility of making random moves. The effect of this would be that even from the same starting point, the search may proceed differently for different runs of the algorithm. A completely random procedure is the Random Walk, in which the search process makes random moves in the search space in complete disregard of the gradient. The algorithm is shown below in Figure 4.3. RandomWalk () 1 node random candidate solution or start 2 bestNode node 3 for ie lton 4 do node RandomChoose (MoveGen (node) ) 5 if h(node) h(bestN