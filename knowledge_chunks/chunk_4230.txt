es. As a result, natural language processing systems have less in common with computer language processing systems (such as compilers) than you might expect. Regardless of the theoretical basis of the grammar, the parsing process takes s the rules of the grammar and compares them against the input sentence. Each rule . that matches adds something to the complete structure that is being built for the yp VP sentence. The simplest structure to build is a parse tree, which simply records the | oo, rules and how they are matched. Figure 15.7 shows the parse tree that would be PN Vv NP produced for the sentence Bill printed the file using this grammar. Figure 15.2 | : o N contained another example of a parse tree, although some additions to this grammar ll printed the NPY would be required to produce it. ADJS N Notice that every node of the parse tree corresponds either to an input word or | | to a nonterminal in our grammar. Each level in the parse tree corresponds to the e file Fig. 15.7 A Parse Tree application of one grammar tule. As a result, it should be clear that a grammar : for a Sentence specifies two things about a language: Its weak generative capacity, by which we mean the set of sentences that are contained within the language. This set (called the set of grammatical sentences) is made up of precisely those sentences that can be completely matched by a series of rules in the grammar. e Its strong generative capacity, by which we mean the structure (or possibly structures) to be assigned to each grammatical sentence of the language. >There is, however, still some debate on whether context-free grammars are formally adequate for describing natural languages (e.g., Gazdar [1982].) Natural Language Processing 293 So far, we have shown the result of parsing to be exactly a trace of the rules that were applied during it. This is not always the case, though. Some grammars contain additional information that describes the structure that should be built. We present a