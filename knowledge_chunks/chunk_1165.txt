gives lesser importance to states that occur farther into the future. This is akin to discounting the future value of money now. It is not surprising that the idea of measuring current worth by discounting the future value has been borrowed from economics. In fact, MDPs were first devised in economics. A common utility accruing function sums up the individual utilities multiplied by a value based on a discounting factor Osys1. Vso) po '(R(s,) C(s,,2(s,))) The smaller the value of y, the greater the importance given to states nearer to the time t 0. The sequence of states over which the value is computed is known as a history h so, 1, So, ... . The above equation says that for any history starting with the state sg and going through states sj, So ..., and so on, results in a value that can be assigned as the value of the initial state. What is the role of the policy here? The policy is the one that decides what actions are taken in each state, and therefore what state the system potentially moves to. However, given a policy 77, one could have several histories generated from the starting state sg, and in fact several other histories that could be generated starting at other states. The utility of a policy can then be defined as the expected value accrued over all such histories, multiplied by the probability of the history occurring given 7. Let H be the set of all possible histories and let h H be some history and let s, be the starting point of that history h (Ghallab et al., 2004). E(2) Spe Plh x) V(s, ) The task of planning with MDPs is to find an optimal policy m , such that the expected utility of the policy m is greater than the expected utility of any other policy 77, that is, E(7 ) E(7). One can distinguish three kinds of MDPs. The first are the finite horizon MDPs in which there is a bound on the number of actions that can be executed. The second, infinite horizon MDPs with discounted costs and rewards. The idea here is that a discounting factor Osys1 gives