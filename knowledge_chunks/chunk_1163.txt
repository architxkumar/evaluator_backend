 drawing a card ina game of blackjack; taking a step forward on slippery ice on a glacier; dialling a phone number (could result in a ring or an engaged tone); buying a lottery ticket; and making a final bid while haggling over the price of a T-shirt; and so on. 17.6.1 Markov Decision Processes How does an agent plan in such situations? In classical forward state space planning, one chooses an action and assumes that one goes to a new state from where one can plan again. When the actions are stochastic then one cannot assume that one will go to a particular state. Instead, one may have to consider a set of states in any one of which one can end up in with a certain nonzero probability. Would one need to choose actions from each of these states? That is what in fact an approach to planning with Markov Decision Processes (MDPs) does. Only one does not call the output of such reasoning a plan anymore, but a policy. In this section, we take a brief look at MDPs. Readers interested in greater detail, finer nuances, and theoretical analysis are referred to (Mausam and Kolobov, 2012). MDPs replace plans with policies. A (complete) policy m7 is a statement of intent for every state in the state transition system. If S is the state space on which the agent operates and A is the set of actions available then 1: SA The policy specifies an action in each state. A partial policy may specify actions for only some subset of the states. The process is a Markov process because the choice of action depends only on the state the system is in, and not on the previous history of states the agent was in. The domain of operation is stochastic because each action a A is stochastic. That is, the action does not result in transition to a new state deterministically. Instead, the system may move to a new state s' with a certain probability given by P(s, a, s ), where s, s S. The probability distribution must satisfy the condition for a given state s S and an action acA, X ves P(s, a, ) 1 That