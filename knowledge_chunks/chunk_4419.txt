 learn without supervision. 18.3 APPLICATIONS OF NEURAL NETWORKS Connectionist models can be divided [Touretzky, 1989b] into the following categories based on the complexity of the problem and the network s behavior: e Pattern recognizers and associative memories e Pattern transformers e Dynamic inferencers Most of the examples we have seen so far fall into the first category. In this section, we also see networks that fall into the second category. General inferencing in connectionist networks is still at a primitive stage. 18.3.1 Connectionist Speech Speech recognition is a difficult perceptual task (as we saw in Chapter 21). Connectionist networks have been applied to a number of problems in speech recognition; for a survey, see Lippmann [1989]. Fig. 18.21 shows how a three-layer backpropagation network can be trained to discriminate between different vowel sounds. The network is trained to output one of ten vowels, given a pair of frequencies taken from the speech waveform. Note the nonlinear decision surfaces created by backpropagation learning. Speech production the problem of translating text into speech rather than vice versa has also been attacked with neural networks. Speech production is easier than speech recognition, and high performance programs are available. NETtalk [Sejnowski and Rosenberg, 1987], a network that learns to pronounce English text, was one of the first systems to demonstrate that connectionist methods could be applied to real-world tasks. Linguists have long studied the rules governing the translation of text into speech units called phonemes. For example, the letter x is usually pronounced with a ks sound, as in box and axe. A traditional approach to the problem would be to write all these rules down and use a production system to apply them. Unfortunately, most of the rules have exceptions consider xylophone and these exceptions must also be programmed in. AJso, the rules may interact with one another in unpleasant, unforeseen ways. 