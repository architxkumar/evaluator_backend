rd matrix, with each row representing a document. Geometrically, the rows of U and V are co-ordinates of points corresponding to documents and words mapped onto a k-dimensional space. Typically, the axes are scaled using the k singular values to assign more importance to dimensions that are associated with high singular values. These reduced dimensional representations can then be compared against each other using the dot product or the cosine measure. Secondly, it can be shown that M is the best k-rank approximation to M in the least squares sense. The quality of an approximation M, is measured by the Frobenius Norm of the discrepancy matrix X M - Ma, which is given by m i The lower the value of X -, the better the matrix M, is, as an approximations to M. Viewing the low rank approximation problem as one of constraint optimization, it can be shown that, of all approximate matrices that satisfy the constraint that their rank is at most k, 7 is the one that registers a minimum value for X -. This conforms to our earlier intuition that removing very small singular values does not significantly affect M. The important thesis behind LS is that the small singular values correspond to noise due to word choice variability (synonymy). M isa less sparse representation compared to M that broadly retains the patterns of word association to documents, but at the same time smoothes it out to eliminate noise. Thirdly, we note that the correspondence between low singular values and noise due to word choice variation is not accidental. Considering a square matrix M with two identical columns, we can eliminate one of these and still retain the same rank. This is a trivial case of feature selection. If instead, M had nearly identical columns, it would mean that the two corresponding features would have co-occurred similarly with documents. This would be true for closely related features like Middle East and oil which might appear in very similar contexts in a large document corpus. I