ge s Analytical Engine. But she could have been describing artificial intelligence. While defining AI in terms of symbol processing it would only be right for us to inspect the problem of Symbol Grounding [Stevan Harnad, 1990, The Symbol Grounding Problem, Physica, D42, 335-346} and not forget about it while grasping any of the concepts discussed in this book. Harnad defines the symbol grounding problem citing the example of the Chinese Room [Searle, 1980]. The basic assumption of symbolic AI is that if a symbol system is able to exhibit behaviors which are indistinguishable from those made by a human being, then it has a mind. Imagine such a system subjected to the Turing test in Chinese. If the system can respond to ali Chinese symbol string inputs in just the manner as a native Chinese speaker, then it means (seems) that the system is able to comprehend the meaning of the Chinese symbols just the way we all comprehend our native fanguages. Searle argues that this cannot be and poses the question If he (who knows none of Chinese) is given the same strings and does exactly what the computer did (maybe execute the program manually!), would he be understanding Chinese? The rhetoric only leads to one unambiguous inference The computer does not understand a thing. It is thus important to note that the symbols by themselves do not have any intrinsic meaning (like the symbols in a book). They derive their meanings only when we read and the brain comprehends it. It goes to say that if the meaning of the symbols used in a symbol! system are extrinsic, unlike the meanings in our heads, then the model itself has no meaning. As the symbols themselves have no meaning and depend on other symbols whose meanings are also extrinsic, we seem to be reasoning around meaningless entities which itself is a meaningless affair! This is the symbol grounding problem. In the context of the meaninglessness of the use of symbols, Harnad provides a classic example of learning Chinese. Assume y