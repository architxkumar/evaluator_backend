t if the algorithm is initialized with a proper policy 779 then it is guaranteed to find the optimal policy. One has to be careful that the initial policy is a proper one, if one is to use Policy Iteration. The next algorithm we look at is free from any such problem. Value Iteration The Value Iteration algorithm devised by Richard Bellman in (1957) focuses directly on the value functions and searches in the value function space. The Bellman equations (also known as dynamic programming equations) given below, capture the optimality criteria for solving an MDP, which is an example of dynamic programming. Q (s,a) Xs'eg P(s, a, PC(s, a, s' ) Vi S) where Q'(s,a) is the expected cost of executing the action a in state s and then following the optimal policy, yielding the optimal value V (S ) for the resulting state. The optimal policy in turn is defined as one that chooses the action that optimizes the Q-value, V'(s) 0 ifseG mitgea Q (8,0) otherwise The Bellman equations have a unique solution that corresponds to the value of the optimal policy m . The first equation specifies how to compute the Q-value, given the optimal values V (S ) of all successor states, and the second one specifies that the optimal value V (s) for a state is obtained by choosing the action in that state that yields the lowest Q-value. The Bellman equations were first applied to engineering control theory and to other topics in applied mathematics, and subsequently became an important tool in economic theory, before being adopted by the probabilistic planning community.4 The Value Iteration algorithm begins by initializing the value function to some random value. Let us call the initial value function Vo. Then it goes through an iterative refinement process till the consecutive values become e-consistent. In each iteration, it sweeps over the entire state space updating the value function V,, in the n' iteration, based on the value function V, ,. The update procedure is known as the Bellman update o