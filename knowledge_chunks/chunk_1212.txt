bel to new instances. There are two phases in supervised learning: (i) Learning or training and (ii) Inference. In the training phase, the algorithm learns a concept from the training data. In the inference phase, the system uses the learned knowledge to classify new instances. The supervised learning set-up consists of a set Xp of rn training examples that are independent and identically distributed (i.i.d). Each training example x; Xp is represented using d features or attributes. Let Y be a set of class labels. The generative models explicitly model class conditional probability distribution of features and learn the associated parameters as part of the training process. The Naive Bayes (NB) classifier and the Hidden Markov Model (HMM) are examples of generative supervised learning algorithms. The inference stage is used to assign a label to new examples using the trained model. The probability of a new example Xpeyw belonging to class ye Y can be calculated using Bayes theorem as follows: P new ) PO) POF new) Por (18.1) new) PRnew ) PO) Byer Pl pew Y) PO) CD We need to learn P(Xnew y) and P(y) for each class. The supervised learning algorithms use different models for P(Xnew y) that can be learnt from Xp. The parameters of a given model are determined using either maximum likelihood estimation (MLE) or Bayesian estimation. MLE completely relies on Xp for parameter estimation, while Bayesian estimation incorporates prior user knowledge. MLE and Bayesian estimates agree on the class labels in the limit of a large number of training examples. On the other hand, discriminative techniques do not explicitly model class-conditional distributions. Instead, they focus on learning the boundary or separator between classes. Classifiers like Decision Trees, Support Vector Machines (SVMs), and Logistic Regression are some examples of discriminative supervised learning algorithms. At a deeper level, both discriminative and generative models can be seen to be equivalent. We wi