ginal set S. The weight for each partition is the proportion of the elements in that partition. One can see that the more homogenous the partitions are, the greater is the information gain. This is illustrated in Figure 18.10. The algorithm tries out the different attributes available and picks the one that yields the greatest information gain. The process continues recursively on the partitions. Let us look at this process for the training set given in Table 18.3. There are 48 employees in the training set S of which there are 11 with the High (salary) label, 16 with the Low label, 8 with Medium, and 13 with the VeryHigh label, in alphabetical order of the labels. The entropy of the set is computed as follows. Entropy(S) (11 48) log,(11 48) (16 48) log,(16 48) (8 48) log,(8 48) (13 48) log,(13 48) 0.229 2.126 0.333 1.585 0.167 2.585 0.271 1.885 0.487 0.528 0.431 0.510 1.957 am (ie was Low Entropy f O 7 oO ( 1 o D AO reece ee ) FIGURE 18.10 Partitioning a set into subsets could result in information gain, if the subsets have lower entropy. The algorithm to construct decision trees pick that attribute to partition the set which results in highest information gain. In the figure, three illustrative choices are shown. The set S can be partitioned by the three attributes: Exp-Symbol (the nominal version of the numeric attribute Experience), Education and Hands-on. The first attribute Exp-Symb produces three partitions for the three values High, Low and Medium. Education also produces three partitions for the values Bachelor or B, Master or M, and PhD or P, while Hands-On produces two partitions. The possible partitions and the number of elements in them are shown in Table 18.4. Table 18.4 The distribution of elements by the three attributes Exp-Symb, Education, and HandOn s Class High 1 Low 16 Medium 6 VeryHigh 13 Total 48 For example, the Higheyp, partition induced by Exp-symb has 13 elements of which 6 have the label High (salary) and 7 have label VeryHigh. The entrop