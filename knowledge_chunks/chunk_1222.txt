cation along with the parameters. The meaning of will be clear from the context and we will state it explicitly whenever required. The following three problems are associated with HMM, out of which, two are inference problems and the remaining one is the learning problem. Learning Given a set of training examples, determine parameters 6 of the given HMM. Inference Given a Hidden Markov Model, 9, find the probability that a given observation sequence X is generated by it: P(X; )". Inference Given a Hidden Markov Model, 0, find the most probable sequence of states that generates a given sequence X. Formally, Y argmax PCX, Y: 0) Y An Example: Modelling a Casino An HMM can be used to model a sequence of coin tosses in a casino. The casino maintains two types of coins which show different sides with different probabilities. For instance, one type of coin lands up heads with probability 0.6, while the other type of coin lands up heads with probability 0.9. In practice, we do not have access to the model that the casino uses to select a particular type of a coin for the next throw. It is reasonable to assume that selection of the next type of coin depends only on the current type and is independent of all previously selected types. We only observe a sequence of outcomes after throwing a coin (heads or tails). Here, the sequence of outcome corresponds to an observation sequence (X), while the sequence of coin types corresponds to a hidden sequence (Y). For a throw of a coin, the probability of the outcome depends only on the coin that is chosen and is independent of the coins chosen earlier and the previous outcomes. The HMM has the following components. 1. Set of observation symbols S H, T , where H corresponds to heads and T corresponds to tails. 2. Set of labels L Y U B U E where Y 7 , To . 3. The topology of the HMM that models coin tosses from the casino is shown in Figure 18.2. 4. Transition matrix T4 4. 5. Emission matrix O2 9 PTB) P(EIT,) - PUTT) - PT T2) N (p PUTIB