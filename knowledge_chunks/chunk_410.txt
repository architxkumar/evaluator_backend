ion representing the mapping between the input output pairs. Neurogammon was very successful and won the Backgammon championship at the 1989 International Computer Olympiad quite easily (Tesauro, 1989). The second, and much better, program by Tesauro (1994, 1995, 2002), called TD-Gammon, learnt the evaluation function in an entirely different manner. While Neurogammon got its knowledge by being taught by experts, 7D-Gammon learnt the principles of Backgammon strategy by itself, playing 300, 000 games against itself and learning the good moves, or afterstates, as they are known in the research field of reinforcement learning (Sutton and Barto, 1998). The specific form of learning that the program did is called Temporal Difference (TD) learning, and hence its name. TD learning addresses the problem of credit assignment to moves when the reward is only available at the end of a sequence of actions. When a player plays a game, the result is known only when the game ends. The task is to decide which of the moves contributed by how much to the result. The algorithm used in TD-Gammon is the algorithm known as TD( ambda) (Sutton, 1988). The training, as described in (Tesauro, 1994), is as follows. The complete set of moves played by the program is fed to the feedforward neural network. The board positions are the input vectors x 1 , X 2 ... x f to the network. For each input pattern x t , there is a neural network output vector Y t indicating the neural network s estimate of expected outcome for pattern x t . Y t is a four-component vector corresponding to the four possible outcomes of either White or Black, winning either a normal win or a gammon. At each time step, the TD(lambda) algorithm is applied to change the network weights, using the following formula, Way Wy, (Yaa Y) Da AAV where a is a small constant, commonly thought of as a learning rate parameter, w is the vector of weights that parameterizes the network, and V,,Y; is the gradient of network output with respec