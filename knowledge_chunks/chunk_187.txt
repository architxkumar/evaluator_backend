will follow it. In the computational version of the algorithm, (see (Dorigo, 2004)), Ant Colony Optimization (ACO) combines this pheromone pull with the innate problem solving strategy of an agent. ACO thus refers to a set of agents working together in such a way that the agents have a tendency of following and reinforcing the trails of other agents, in addition to their own strategy. Given a choice of moves, the agent (an ant) will be influenced by two things. One is its own heuristic information that indicates a preference from the available moves. The second is the amount of pheromone left by other ants. Pheromone trails are simple semiotic means of sharing experience. The process works when the pheromone deposits reinforce good moves as opposed to bad ones. In the case of food finding activity, the ants travel at a constant speed and leave a constant amount of pheromone that evaporates at a constant rate. On the paths that lead to food, the ants will return quickly, adding more pheromone, and thus strengthening the shortest path to the food. In the example below, we look at the TSP problem that was used originally by Dorigo et al. (Colorni, 1991). In the ACO algorithm for TSP, a population of ants are used to construct a set of tours. After the tours have been constructed, each ant deposits an amount of pheromone on the edges, making up the tour that is inversely proportional to the cost of the complete tour. In this way, the ants finding shorter tours will leave stronger pheromone trails, which will then attract more ants on those segments. Let there be n cities in the TSP problem. Let 7; t) represent the amount of pheromone on segment from the " city to the j" city at time t, which is the start of a new tour construction. Then after n moves, each ant would have constructed a new tour and the pheromone trails will be updated as follows, T(t m) (1p) TO A t t. t m) (4.7) where 0 1 is the coefficient of pheromone evaporation. By choosing an appropriate value of p,