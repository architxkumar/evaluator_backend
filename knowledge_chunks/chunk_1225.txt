propriate discrete distributions such as binomial, multinomial, Poisson, etc. On the other hand, the continuous or real observations can be modelled using distributions like Gaussian. The parameter estimation can be performed via MLE or Bayesian estimation techniques. As an example, we explain parameter estimation when observations are discrete in nature. Let each observation s come from a set of finite symbols 2. Each label ye Y emits these symbols according to a fixed distribution with unknown parameters. Let us assume that Oyo Poe x yveLl) a Mult(9,) The estimates for O , can be obtained using MLE as follows. DS, o Kj r) Oy.0 ist Ze jal ij ee ast? length(X;) F Dap) ae Lvoex MX; 9. .5 7 Here, I(.) is an indicator function that returns 1 if that argument is true. The length (Xj) returns number of observations in sequence X;. The Bayesian estimates are obtained by using appropriate prior distribution. It is equivalent to adding a small pseudocount d to the actual counts. ET eP My, jso.K r1 5 jel - m wo length - Sveee DD OTL, j 0.Y,; yl 6 6 (18.18) Training Data Consisting only Observation Sequences Unlike the first case, here the label sequences are not available with observation sequences. Therefore we cannot directly apply the procedure described in the previous section. This is the case of missing information, where label sequences are not available. In such cases, the Expectation Maximization (EM) technique is used to counter the missing information problem. EM is a powerful technique that operates in two steps repetitively until convergence. 1. E-step: Assigns values to the missing variable. 2. M-step: Maximizes the objective function with respect to the assignments made in E-step. HMM training with EM algorithm can be performed via two different random initializations of either label sequence or parameters such as transition and emission matrix. These techniques are known as Viterbi and Baum-Welsch training respectively. Their algorithms are given in Figures 1