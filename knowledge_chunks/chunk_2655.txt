ion (see page 657),
using a simplified value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation,
the value iteration process can use the previous utility estimates as initial values and should
converge quitequickly.
The process of learning the model itself is easy, because the environment is fully ob-
servable. Thismeansthatwehaveasupervisedlearningtaskwheretheinputisastate action
pair and the output is the resulting state. In the simplest case, we can represent the tran-
sition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability P(s
(cid:2) s,a)
from the frequency with which s
(cid:2)
is reached when executing a in s. Forexample, in the three trials given on page 832, Right
is executed three times in (1,3) and two out of three times the resulting state is (2,3), so
P((2,3) (1,3),Right)isestimatedtobe2 3.
Section21.2. Passive Reinforcement Learning 835
1
0.8
0.6
0.4
0.2
0
0 20 40 60 80 100
setamitse
ytilit U
0.6
(4,3)
(3,3) 0.5
(1,3)
0.4
(1,1)
(3,2)
0.3
0.2
0.1
0
0 20 40 60 80 100
Number of trials
ytilitu
ni
rorre
SMR
Number of trials
(a) (b)
Figure21.3 Thepassive AD Plearningcurvesforthe4 3world,giventheoptimalpolicy
shownin Figure21.1. (a)Theutilityestimatesforaselectedsubsetofstates, asa function
ofthenumberoftrials. Noticethelargechangesoccurringaroundthe78thtrial thisisthe
first time that the agent falls into the 1 terminal state at (4,2). (b) The root-mean-square
error(see Appendix A)intheestimatefor U(1,1),averagedover20runsof100trialseach.
The full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor-
mance on the 4 3 world is shown in Figure 21.3. In terms of how quickly its value es-
timates improve, the ADP agent is limited only by its ability to learn the transition model.
In this sense, it provides a standard against which to measure other reinforcem