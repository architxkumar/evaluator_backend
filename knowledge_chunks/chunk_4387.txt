.1,,). Notice that the weighted summation function g(x) and the output function o(x) can be defined as: h go) = y Wik i=0 1 if g(x) >0 a(x) = . 0 if g(x) <0 Consider the case where we have only two inputs (as in Fig. 18.9), Then: B(x) = Wy + WX, + W2t D If g(x) is exactly zero, the perceptron cannot decide whether to fire. A slight change in inputs could cause the device to go either way. If we solve the equation g(x) = 0, we get the equation far a line: The location of the line is completely determined by the weights wy, w, , and w. If an input vector lies on one side of the line, the perceptron will output 1; if it lies on the other side, the perceptron will output 0. A line that correctly separates the training instances corresponds to a perfectly functioning perceptron. Such a line is called a decision surface. In perceptrons with many inputs, the decision surface will be a hyperplane through the multidimensional space of possible input vectors. The problem of learning is one of locating an appropriate decision surface. We present a formal learning algorithm later. For now, consider the informal rule: If the perceptron fires when it should not fire, make each w, smaller by an amount proportional to x,. If the perceptron fails to fire when it should fire, make each w,, larger by a similar amount. Suppose we want to train a three-input perceptron to fire only when its first input is on. If the perceptron fails to fire in the presence of an active x,, we will increase w, (and we may increase other weights). If the perceptron fires incorrectly, we will end up decreasing weights that are not w,. (We will never decrease w, because undesired firings only occur when x, is 0, which forces the proportional change in w, alsa to be 0.) In addition, wy will find a value based on the total number of incorrect firings versus incorrect misfirings. Soon, w, will become large enough to overpower wp, while w, and w; will not be powerful enough to fire the perceptron, even in the p