h this with a six-input, three-output backpropagation network. We simply present the network with an input, observe its output, and update its weights based on the errors it makes. Without a teacher, however, the error cannot be computed, so we must seek other methods. Our first problem is to ensure that only one of the three output units becomes active for any given input. One solution to this problem is to let the network settle, find the output unit with the highest level of activation, and set that unit to 1 and all other output units to 0. In other words, the output unit with the highest activation is the only one we consider to be active. A more neural-like solution is to have the output units fight among 9 One analogue of unsupervised learning in symbolic AI is discovery (Section 17.7). Connectionist Models 393 PASC ROTATE CRE T themselves for control of an input vector. The scheme is shown in Fig. 18.19. The input units are directly connected to the output units, as in the perceptron, but the output units are also connected to each other via prewired negative, or inhibitory, connections. The a) output unit with the most activation along its input lines initially will most strongly dampen its () competitors. As a result, the competitors will a, become weaker, losing their power of inhibition oe) @) QO output units over the stronger output unit. The stronger unit then becomes even stronger, and its inhibiting effect on the other output units becomes overwhelming. Soon, the other output units are all completely inactive. This type of mutual input units inhibition is called winner-take-all behavior. One 7 < popular unsupervised learning scheme based on _has-hair? / has-teathers?\, INesiwalere sp . woe sae i has-scales? flies? this behavior is known as competitive learning. . _. . In competitive learning, output units fight for Fig. 18.19 A Competitive Learning Network control over portions of the input space. A simple competitive learning algorithm is the follow