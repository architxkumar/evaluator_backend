 in increasing order of the attribute value then a split point is never placed between two values, corresponding to elements having the same class label. The algorithm in fact tries out different possible split points and chooses the one that maximizes information gain. Figure 18.13 shows the decision tree constructed for our example, from the original data set in Table 15.3. The set of attributes Att used here is Age, Education, Experience, Hands-On , with Age being the new attribute added. ID3 (Training Set: T, Target-attribute: C, Attributes: Att) 1 create node N with set T 2 if all instances of T in N have class label c values(C) 3 then return N with label c 4 if empty(Att) Ss then return N with majority label from T 6 A chooseAttribute(Att, T) with maximum information gain 7 N.decisionAttribute A 8 Att Att - A 9 for each value V of A 10 Ty subset of T with value V of elements 11 ereate node M, with set Ty 12 child(N,V) N, the child of N with value V 13 if empty(T,) 14 then 15 label Ny with majority label from T 16 else 17 Ny ID3(T,, C, Att) 18 return N FIGURE 18.12 The procedure D3 creates a tree structure with a decision point at each node. At every node, the attribute that maximizes information gain is chosen to partition the set based on the attribute values, and each partition is processed recursively. Function chooseAttribute(Att, T) chooses the maximum information gain attribute from the set Aff, given the training set T. cae in) 2 12 No es C Age Education ) P aes Se High 31 34 P Age ) 3 31 BY y NN A Low 4 S38 Xs i 2 Medium Very High (Biucation) C ie) ee ie wS ge ze, High Very High B MNP 26 26 No Low Medium Medium Low High FIGURE 18.13 The decision tree constructed by C4.5 for the date set with numerical values in Table 15.3. An additional numeric attribute Age has also been considered. It is interesting to note that with these numerical values too, Experience has been the attribute that is chosen first, though the algorithm is designed to make a two-way 