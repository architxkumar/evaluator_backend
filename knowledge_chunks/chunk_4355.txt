other complexities, including the way in which features interact with one another. For example, if the origin of a car is Japan, then the manufacturer cannot be Chrysler. The version space algorithm as described above makes no use of such information. Also in our example, it would be more natural to replace the decade slot with a continuously valued year field. We would have to change our procedures for updating the S and G sets to account for this kind of numerical data. 364 Artificial Intelligence 17.5.3 Decision Trees A third approach to concept learning is the induction of decision trees, as exemplified by the ID3 program origin? of Quinlan [1986]. [D3 uses a tree representation r T T T } for concepts, such as the one shown in Fig. 17.13. USA Germany Britain Italy Japan Jo classify a particular input, we start at the top of Oo o O o | the tree and answer questions until we reach a leaf, type? where the classification is stored. Fig. 17.13 aye Sports Economy Luxury represents the familiar concept Japanese economy -) (+) O car. [D3 is a program that builds decision trees automatically, given positive and negative instances of a concept.* TD3 uses an iterative method to build up decision trees, preferring simple trees over complex ones, on the theory that simple trees are more accurate classifiers of future inputs. It begins by choosing a random subset of the training examples. This subset is called the window. The algorithm builds a decision tree that correctly classifies all examples in the window. The tree is then tested on the training examples outside the window. If all the examples are classified correctly, the algorithm halts. Otherwise, it adds a number of training examples to the window and the process repeats. Empirical evidence indicates that the iterative strategy is more efficient than considering the whole training set at once. So how does ID3 actually construct decision trees? Building a node means choosing some attribute to test. Ata given point in 