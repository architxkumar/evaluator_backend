 to the conclusion we are interested in. Box 13.1 : The Frame Problem Consider a robot carrying out some task autonomously. Assume that it maintains its set of beliefs about its world as a set of propositions. Given that it is planning some actions to be done, how does it update its representation to account for the changes that happen as a result of its actions? Of course some changes are intended and are captured in its representation of actions. For example, double clicking on file icon in one s computer will open the file. Or turning the faucet will make the water flow from the tap. Or shooting with a gun will result in the death of the target. The question is what does not change as a consequence of a given action? Is there a succinct way of asserting that? (Hayes, 1987). For example, if one fired a gun a bird sitting on a nearby tree might take flight. Or turning the faucet on will (somehow) turn on a light as well. John McCarthy and Patrick Hayes (1969) called this the Frame Problem. To use the analogy of drawing cartoon animations by hand, what does not change from one frame to the next, and can be carried forward? They illustrate this with an example in which an agent looks up a telephone directory intending to call a friend over the phone. How is one sure that the action of looking up the directory has not (somehow) made the phone vanish? One can see that the solution to this problem of logical reasoning is to somehow assert that actions have no unstated effects, and also that no unstated actions have happened. Steven Hanks and Drew McDermott (1987) pose the Yale shooting problem in which a gun is loaded and fired after a time interval. How can one conclude that the target is dead as a consequence? From a philosopher's point of view, the question is how can an agent ever be sure that it has updated its beliefs to reflect all the changes that are a consequence of its actions? Drew McDermott s response to this is that even humans are unable to guarantee this