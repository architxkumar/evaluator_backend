G = {(VJapan, x , X4, X4, Economy)} We then generalize the S set to include the new example: S= ((/apan, X, X3, 44, Economy)} S and G are both singletons, so the algorithm has converged on the target concept. No more examples are needed. There are several things to note about the candidate elimination algorithm. First, it is a /east-commitment algorithm. The version space is pruned as little as possible at each step. Thus, even if all the positive training examples are Japanese cars, the algorithm will not reject the possibility that the target concept may include cars of other origin until it receives a negative example that forces the rejection. This means that if the training data are sparse, the S and G sets may never converge to a single description; the system may learn uily partially specified concepts. Second, the algorithm involves exhaustive, breadth-first search through the version space. We can see this in the algorithm for updating the G set. Contrast this with the depth-first behavior of Winston s learning program. Third, in our simple representation language, the S set always contains exactly one element, because any two positive examples always have exactly one generalization. Other representation languages may not share this property. 3 It could be the case that our target concept is not Chrysler, but we will ignore this possibility because our representation {anguage is not powerful enough to express negation and disjunction. 7 Learning 363 The version space approach can be applied to a wide variety of learning tasks and representation languages. The algorithm above can be extended to handle continuously valued features and hierarchical knowledge (see Exercises). However, version spaces have several deficiencies. One is the large space requirements of the exhaustive, breadth-first search mentioned above. Another is that inconsistent data, also called noise, can cause the candidate elimination algorithm to prune the target concept from the version s