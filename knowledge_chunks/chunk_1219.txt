 X4 1)? The calculations are shown below. P(X, 1,.X, ) P(X, ) LD x,.x, (Xs 1,X4 ) 7 D x, Xs P(X, ) Langa, PONG) PMs Ig. Ng PUR) PONS D Lx apt, PON) PX 4 INR. 5) Pg) PS) Lx, P(X, 1X. Xs 1) P(Xp) P(X5 ) Dixp a, PORy AUX Xs) PKR) PMS) P(Xs5 1 X, ) 0.9 0.8 0.1 14 0.2 0.1 0.9 0.8 0.1 1 0.2 0.14 0 0.8 0.9 1 0.2 0.9 0.3382 0.9 0.8 0.1 1 0.2 0.1 0.9 0.8 0.14 1 0.2 0.1 0 0.8 0.9 1 0.2 0.9 0.3382 18.3 Hidden Markov Models A class of models known as Hidden Markov Models (HMM) assumes that there are nodes in the model that are not observable. What is accessible is a set of observations, which are dependent upon the unobservable state of the system. HMMs are Markov models because one assumes that the state that the system is in is not dependent on the history of the past states, and a given state has well defined successor states, one of which will be next state with a given probability. HHMs are popular where temporal patterns are involved, for example in Speech Recognition. In many applications, there is a natural order in the data that imposes label dependency between points. The label of a point depends on points prior to it as specified by the label dependency. For instance, (a) part of speech (POS) tag of a word in English statement depends on the tag of the previous word; (b) the address segmentation problem that automatically tags locality, city, and state information in the address exploits relative placements of these entities in the address. Thus, there are two sequences per example: observation sequence and label sequence. The observation sequence refers to the sequence that we observe as the name suggests, while the label sequence refers to the sequence of states that are hidden. Each hidden state emits a symbol, thus forming an observation sequence. Let X be the sequence of n observations x,, X2, -, X, and let Y be the corresponding label sequence Y y4, Y2,..., n2, which is often unknown or hidden from the user. Note that there is an order information associated wi