represented as a lattice structure having 2" nodes, we see that this is an exponential search problem (Figure 18.1):. 18.4 GENERALIZATION AND SPECIALIZATION In this section we consider some techniques which are essential for the application of inductive learning algorithms. Concept learning requires lhat a guess or estimate of a larger class, the target concept, be made after having observed only some fraction of the objects belonging to that class. This is essentially a process of generalization, of formulating a description or a rule for a larger class but one which is still 26386 Learning by Induction Chap. 18 consistent with the observed positive examples. For example, given the three positive instances of objects (blue cube rigid large) (small flexible blue cube) (rigid small cube blue) a proper generalization which implies the three instances is blue cube. Each of the instances satisfies the general description. Specialization is the opposite of generalization. To specialize the concept blue cube, a more restrictive class of blue cubes is required such as small blue cube or flexible blue cube or any of the original instances given above. Specialization may be required if the learning algorithm over-generalizes in its search for the target concept. An over-generalized hypothesis is inconsistent since it will include some negative instances in addition to the positive ones. There are many ways toiorm generalizations. We shall describe the most commonly used rules below. They will be sufficient to describe all of the learning paradigms which follow. In describing the rules, we distinguish between two basic types of generalization, comparable to the corresponding types of induction (Section 18.2), selective generalization and constructive generalization (Michalski, 1983). Selective generalization rules build descriptions using only the descriptors (attributes and relations) that appear in the instances, whereas constructive generalization rules do not. These conce