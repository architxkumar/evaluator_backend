exploit certain basic properties of the constituent neurons and connections to drive the system towards a stable configuration that is dependent upon the input patterns shown to the network. These are described in Section 18.8.3. As observed in Section 4.5, the striking thing about the human brain, when seen as a neural network, is that a large, interconnected network of simple processing elements, or neurons, ends up becoming a complex information processing machine. The first paper to explore the idea exploring neural networks as computing machines was by McCulloch and Pitts (1943) (see also (Haykin, 2009)). We begin by studying what can be done by a single neuron, and then we will look at the need to combine multiple neurons and the corresponding increase in computational power. The model of the neuron adapted from Section 4.5 is, Y PZini n Wi x; 4) (18.53) where wWj,...,W, are weights for the n inputs x;,..., X, that the neuron receives, b is a bias and is some function that is applied to the summation. We begin below with the Perceptron, where is the signum function or the sign functions that returns either 1 or 1. 18.8.1 The Perceptron The simplest computing device based on a single neuron was first explored by the psychologist Frank Rosenblatt and named the Perceptron (Rosenblatt, 1958). The Perceptron can be visualized as shown on Figure 18.19. It consists of a single neuron that receives n inputs x,...xX, from other neurons as shown in the figure. In the Perceptron, these neurons are sensory neurons which sense real valued feature values from the external world. There is an additional input in the form of a bias b. The Perceptron does a two-stage processing on these inputs. First it computes a weighted sum of the inputs, where each weight wis the weight associated with the " input, or the connection from the neuron. One often views the bias as a constant 0" input with value 1 and weight Wo b. The output of the first stage is, 2 Ziny np WX O z. 0. nn Wi X; w