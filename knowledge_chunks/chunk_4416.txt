sented with an input from either cluster. In other words, it may oscillate between the two clusters. Normally, another output unit will win occasionally and move to claim one of the two clusters. However, if the other output units are completely unexcitable by the input vectors, they may never win the competition. One solution, called leaky learning, is to change the weights belonging to relatively inactive output units as well as the most active one. The weight update rule for losing output units is the same as in the algorithm above, except that they move their weights with a much smaller 7 (learning rate). An alternative solution is to adjust the sensitivity of an output unit through the use of a bias, or adjustable threshold. Recall that this bias mechanism was used in perceptrons and corresponded to the propensity of a unit to fire irrespective of its inputs. Output units that seldom win in the competitive learning process can be given larger biases. In effect, they are given control over a larger portion of the input space. In this way, units that consistently lose are eventually given a chance to win and adjust their weights in the direction of a particular cluster. 18.2.7 The Kohonen Neural Network Model The Kohonen neural network is a typical example of both self-organization and competitive learning. Selforganization is an unsupervised leaming quality which organizes a neural network and makes it learn some meaningful information. Categorized as an unsupervised leaming network, the Kohonen network is based on the concept of graded or reinforcement leaming. Graded learning is similar to grading a class of students by way of conducting quizes and using their scores obtained to reflect their performance. In our world several processes are learnt by grading while many others are learnt unsupervised. A typical example of the latter is the way in which an infant learns to recognize the objects and people around it. A Kohonen network is a non-recurrent (feedforwa