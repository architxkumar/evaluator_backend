 data, whereas the error correcting rule makes adjustments, looking at each individual example. Learning Boolean Functions Neural networks in general and Perceptrons in particular can also be employed for function approximation. We look at some Boolean logic functions which have a close relation to the two class classification problem. Given a set of Boolean variables X4, .... Xp, a Boolean function of the variables evaluates to either true or false, or equivalently 1 or 0. The learning problem here is that given the various tuples of Boolean variables and the function value for each of them, can a Perceptron learn the function. Since the functions are Boolean, we can divide the tuples into two subsets, or classes. One for which the function evaluates to true, and the other that evaluates to false. Can we find a hyperplane that would recognize all and only those tuples that should evaluate to true and return a value 1? Consider the example in Figure 18.24. There are two Boolean variables x; and x2 and each can take a value from the set 0, 1 . Given the four pairs along with their target values ( 0, O , 0), ( 0, 1 , 0), ( 1, O , 0), ( 1, 1 , 1) , the reader can recognize it as the Boolean AND function. We want to find a line in a two dimensional space such that the Perceptron outputs 1 only for the element 1, 1 . If so, then the Perceptron has learnt to represent the Boolean AND function. The figure shows one line represented by the equation x; x2 - (1 ) 0 that accomplishes this task. Here, e is some small positive value. It is left as an exercise for the reader to work out how the Perceptron training algorithm will find such a separator. There are 16 Boolean functions possible of two variables. It turns out that only 14 of them are linearly separable, and two are not. These two are the XOR function represented by ( 0, 0 , 1), ( 0, 1 , 1), ( 1, O , 1), ( 1, 1 , 0) and its negation the EQ or equivalence function represented by ( 0, O , 1), ( 0, 1 , 0), ( 1, O , 0), ( 