hat are not consistent with each training instance. The algorithm does not store the entire hypotheses space H, but considers a smaller set of hypotheses that form the boundary of a region in the space that contains all consistent hypotheses. A hypothesis is consistent with respect to a set of training examples, if it agrees with the target concept on all the examples. If T is the set of training examples and ha candidate hypothesis then, Consistent(h, T) Vxe T (h(x) c(x)) (18.40) A version space VS , 7 for a hypotheses space H with respect to a set of training instances seen, T is the set of all hypotheses in H that are consistent with T. VSy. he H Consistent(h, 21) (18.41) The candidate elimination works with a version space that is initialized to the entire hypotheses space. It then inspects all training instances, both positive and negative, and prunes the version space by removing those hypotheses that are inconsistent with any training example. This process continues till all training instances are inspected. Three outcomes are possible. 1. The procedure ends with a single hypothesis that is consistent with all training examples. The inductive learning hypothesis says that this hypothesis should predict the class label of the unseen instances as well. . The procedure ends with a set of hypotheses. This means that the training set is insufficient to discriminate between these hypotheses. Two options exist. One, to try and use the resulting set to classify the unseen examples. This could be done in a conservative (or sceptical) manner by choosing the most specific hypothesis, if there exists a unique one; in a iberal (or plausible) manner by using the most general hypothesis, if there exists one, or by combining the verdict of all the hypotheses, possibly in some weighted fashion. The other option would be to seek more training instances to np discriminate between the remaining candidate hypotheses. 3. At some point during the process of inspecting the training 