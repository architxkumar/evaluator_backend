words the and ir that could follow we can use the respective probabilities (of the and if) to guess that the word the is a better candidate. Note that the has higher probability. But things do not always work this way. If we consider the segment of a sentence A very wealthy... If we assume that the high frequency word the will follow the word wealthy it would lead to a syntactic error. The word man with a probability much less than the seems more appropriate. It thus seems that the next word is dependent on the previous one. Finding probabilities of all such words in the corpus that follow the word wealthy and then choosing the best of them may lead to the construction of a more appropriate sentence. Thus as we move through a sentence we could keep looking at a Avo-word window and predict the next or second word using probabilities of finding the second word, given the first word. This can be more formally written as max( P(X\wealthy)} or in plain English we find that word X which appears after wealthy and has the maximum probability of occurrence in this two-word sequence (viz. wealthy followed by X) among all other such words in the corpus. If there are n words in a sentence and assuming the occurrence of each word at their appropriate places to be independent events, the probability P(w,,..., w,,) can be expressed using the chain rule as P(W) = P(w,). Pow2| 1). Pl! (Ww, 2)... Pl, (Wy, Wz Way) Computing this probability is far from simple. Observe that as we move to rightwards, the terms become more complex. The last term would naturally be the most complex to compute. A more practical approach to such chaining of probabilities could be to look at only one prior word at any given moment of processing. In other words, given a word we look for only the previous word to compute the probabilities. Since we look at only word pairs (viz. a single word previous to the one we are searching) this model is called the bigram model. If we follow the bigram model of seeking th