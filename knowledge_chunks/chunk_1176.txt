re two policies. The definition of an optimal policy says that the expected cost over all possible histories must be optimal. Given two policies 77, and TIz we say that policy 77, is better than 772 if, vs S(V s) V" (s)) The basic idea behind Policy Iteration is to start with a random policy and find a better policy in each iteration. Let 779 be the initial policy. Then, in each iteration, the algorithm moves from a policy 77, ; to a new policy 1, which has a better action prescribed for some state. The quality of an action a in a state s is measured by a factor known as its Q-value defined as (Howard, 1960), O"(s,a) Yves P(s, a, s ) C(s, a, 5 ) Vis ) where V(s) is assumed to be the true expected cost of reaching the goal from state s. The Q-value of a state is the expected value of the state when a given action is taken in that state. Since one does not have access to the true expected cost, one goes through an iterative process which oscillates between two phases. In the policy evaluation phase, the algorithm evaluates the value function for a given policy. This can be done by solving the corresponding set of linear equations or an iterative procedure as described in the previous section. In the policy improvement phase, the current policy is refined to a new policy with a smaller value function. This is done by a process called greedy policy construction which is as follows. Given a value function V, a greedy policy mY selects a locally optimal action at each state by a process of one step look ahead. That is, w(s) argmin,. 4 O'(s,a) argmite 4 Lye s P(s, a, s) C(s, a, 5) V(s) That is, it selects an action that yields the lowest expected cost from state s. If, and only if, the new value of the state s becomes lower than the old value, then the action is incorporated into the policy. In this way, it moves to a new policy which has a strictly lower cost. The process continues till a better policy cannot be constructed. The algorithm in the Figure 17.34 below is adap