, given the evidence that litchis are available. They are 0.48, 0.16 and 0.36 for Bengaluru, Madurai and Dehradun respectively. These are also called the a posteriori probabilities, as compared the a priori probabilities we started with, 0.6, 0.2 and 0.2 respectively. The reader should verify that the three probabilities add up to 1 in both cases, signifying that the Sumedha has to be in one of the three cities. The prior probabilities were revised to the posterior probabilities after getting the evidence (of eating litchis). In machine learning literature, the City variable would define the hypothesis space and this approach computes the maximum a posteriori (MAP) hypothesis (Mitchell, 1997). Observe that we do not really need to compute P(Litchi), which is a common factor in all the likelihood estimates, and we could have chosen the maximum of P(Litchi X) P(X), with X taking the values of the three cities. If we did not have a priori information on where Sumedha is likely to be, we could have assumed that the probabilities are equal, and discarded them from our computation. Then we would have simply selected the hypothesis (city) based on where litchis are most likely to be available. This is known as the maximum likelihood hypothesis. The reader would have noticed that despite the fact that litchis are more likely to be available in Dehradun (0.9), one still concluded that Sumedha is likely to be in Bengaluru (even) after hearing that she ate litchis. This was influenced by the predominantly high a priori probability of her being in Bengaluru. Probability calculations can often lead to conclusions that are often counter intuitive. The author has had many an interesting time discussing problems like the Monty Hall problem, the three prisoners problem and the birthday clash problem (see Exercises 18, 19 and 20). 17.5.2 Diagnosis vs. Prediction We have seen above that the Bayes rule allows us to express conditional probabilities in a symmetric manner. That is, PY .)