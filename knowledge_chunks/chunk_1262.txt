s would be amenable to this form of learning. In fact, in the TD-Gammon program, it was the weights of a neural network that were being learnt. Reinforcement Learning happens following a process of trial and error in which the agent repeatedly tries actions and observes the response from the environment. In the game playing situation, this can be done by a program by playing games against itself. And being a computer", it is possible to play millions of games for the task of learning. This addresses the problem of needing a large number of learning experiences. We now address the learning task. In supervised learning, one is presented with training data in the form of a state, value from which the system learns. In the game playing environment, the feedback is available only at the end of the game, when the outcome of the game is known. The value is Large, if MAX wins the game, Large if MAX loses the game, and 0 if the game is drawn. The question of temporal credit assignment is that how does one assign the credit of the outcome to each state in which MAX has made a choice. Consider the situation in Figure 18.17 in which two games are depicted. In one, the outcome is a win for MAX and in the other it is a loss. If MAX played the same first three moves in both the games, then were those moves good or bad? Nod, O-T oe; L OQ. 7 : Q, - WIN FIGURE 18.17 Two games in which MAX makes the same first three moves result in different outcomes, a win in one case and a loss in the other. Were those moves good or bad? If the evaluation were perfect then each state would be labelled with the minimax value of the state, which would come from the set -Large, 0, Large . However, since the minimax value cannot be computed, the game tree being too large, one needs an approximation. This is done by playing as many games as possible. The evaluation function e(J) looks at a board position and returns a number. The task of learning is to update the weights used in the evaluation function t