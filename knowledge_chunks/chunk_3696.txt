tion resulting specialized representation. initial concept description. The next example is then matched against this description using a graph-matching algorithm. This produces a common subgraph and a list of nodes and links which differ. The unmatched nodes and links are tagged with comments which are used to determine how the current description should be modified. If the new example is positive, the description is generalized (Figure 19.9) by either dropping nodes or links or replacing them with more generalized ones obtained from a hierarchical generalization tree If the new example is a negative one, the description is specialized to exclude that example (Figure 19.10). The negative examples are called near misses, since they differ from a positive example in only a single detail. Note the form of specialization used in Figure 19.10. This is an example of specialization by taking exception. The network representation for these exceptions are must and must no, links to emphasize the fact that an arch must not have these features. 19.6 SUMMARY Examples of four different inductive learning paradigms were presented in this chapter. In the first paradigm, the 11)3 system, classifications were learned from a set of positive examples only. The examples were described as attribute values of ohjec;. The classifications were learned in the form of discrimination tree. Once created, the 1D3 system used the tree to classify new unknown objects. Attributes are selected 414 Examples of Other Inductive Learners Chap. 19 on the basis of the information Gain expected. This results in a minimal tree size. LEX, the second system described, learned heuristics to choose when certain operators should be used in symbolic integration problems. One of the interesting features of LEX is the use of a version space which bounds the Set of plausible heuristics that are applicable in a given problem state. LEX uses a syntactic 'form of bias, its grammar, to limit the size of the hypothesis