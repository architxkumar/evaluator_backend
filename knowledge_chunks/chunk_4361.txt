ftable(Object23) | open-vessel(Object23) Stable(Object23) is(Object23, Light) has-part(Object23, Concavity2) has-part(Object23, Handle 16) isa(Concavity12, Concavity) isa(Handie16, handie) isa(Concavity12, Upward-Pointing) has-part(Object23, Bottom19) isa(Bottom?9, Bottom) is(Bottom19, Flat) Fig. 17.15 An Explanation Learning 367 Se pee APY FY 2 PRED IROR SL ETE RATS OAT ERS isolates the relevant features of the training example; nowhere in the proof do the predicates owner and colcr appear. The proof also serves as a basis for a valid generalization. If we gather up all the assumptions and replace constants with variables, we get the following description of a cup: has-partlx, ) 4s isaly. Concavity) \ is(y, Upward-Pointing) /\ has-parttx, 5) isatz, Bottom) /\ is(z, Flat) /\ has-part(x, w) / isa(w, Handle) \ is(x, Light) This definition satisfies the operationality criterion and could be used by a robot to classify objects. Simply replacing constants by variables worked in this example, but in some cases it is necessary to retain certain constants. To catch these cases, we must reprove the goal. This process, which we saw earlier in our discussion of learning in STRIPS, is called goal regression. As we have seen, EBL depends strongly on a domain theory. Given such a theory, why are exampies needed ai all? We could have operationalized the goal concept Cup without reference to an example, since the domain theory contains all of the requisite information. The answer is that examples help to focus the learning on relevant operationalizations, Without an example cup, EBL is faced with the task of characterizing the entire range of objects that satisfy the goal concept. Most of these objects will never be encountered in the real world, and so the result will be overly general. Providing a tractable domain theory is a difficult task. There is evidence that humans do not learn with very primitive relations. Instead, they create incomplete and inconsistent domain theories. 