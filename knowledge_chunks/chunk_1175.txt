. The bottom of the graph plots the residual for the value of state S,. The value of the residual for s; drops to 0.153651 in the 20" iteration. This could be a reasonable point to stop the process. The values after 29 iterations are, "5G (45,2375, 26.8677, 52.8811, 52.8811, 28.2903, 60.2627, 0.0368 which are close to the values computed by solving the equations. Figure 17.33 shows the evolution of three states s,, s5 and Sg, starting from two sets of initial values for all the seven variables, 0 and 100. The plot shows that irrespective of the initial values, the algorithm converges to the same set of values. 17.6.2 Solving MDPs The task of planning with MDPs is to find the optimal policy for a given problem statement. Recall that the problem can be generalized to SSP in which the task is to minimize the overall expected costs of reaching a goal state. A brute force approach would evaluate all possible policies and pick the best one. However, the number of different policies can be very large. If there are N states in the system and a choice of K actions in each state, one would need to evaluate K policies each needing an order of N2 computations. 140 1205 1005 805 Hg ieee Sg ail 605 eweetoe 405 1 2' 3' 4 56 7-8 9 1011 12 1314 15 1617 18 19 20 21 2223 24 25 26 27 28 29 30 Interation Figure 17.33 The values of states S;, Ss and Sg starting with values 0 and 100 converge to the same values for the two initial values. There have been two approaches to finding optimal policies faster, policy iteration and value iteration. We look at them below. Policy Iteration Given that we can evaluate a policy, how does one search for an optimal policy. There are two issues here. One, how does one compare two policies, and two, how does one pick the best policy, given that you can compare two policies. The definition of an optimal policy says that the expected cost over all possible histories must be optimal. Given two policies 77, and TIz we say that policy 77, is better than 772 i