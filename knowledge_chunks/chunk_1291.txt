tes. The degree of influence is determined by the weight of the connection. Negative weights imply inhibitory influence. At any point, the state of a neuron is determined by the states of neurons connected to it. If the sum of the products of neighbour neuron activation times the connection weight is greater than a threshold, the neuron gets value 1, else it gets value 0. The two stable states in the figure assume a threshold of 0, and represent the two patterns that the network has stored. FIGURE 18.31 A tiny Hopfield network. Pointed arrowheads depict excitatory (positive) influences and rounded arrowheads represent inhibitory (negative) influences. Observe that the influences are symmetric. The process of relaxation may be deterministic (Hill Climbing) as in the Hopfield network, or it may be stochastic (Simulated Annealing) as in the Boltzmann machine (Hinton, 1986). The minimum energy states are defined during the training process in which a set of patterns are shown to the network, and a minimum energy state in what is known as a basin of attraction is formed for each pattern to be remembered. A detailed indepth discussion of these networks involves viewing the network as a neurodynamic system which is guaranteed to converge, and is beyond the scope of this book. Interested readers are referred to (Hassoun, 1998) and (Haykin, 2009). The training process employs a form Hebbian learning (Hebbs, 1949). The basic idea behind Hebbian learning is reinforcement of agreement. When two neighbouring neurons have their activation levels close to the desired activation levels, then the weights of the connection between them is increased. If a machine is trained to remember the following patterns, P, 101100111 Py 000111111 P3 111100000 and then shown the partial pattern 0 0 0 x x x x x x where an x represents missing or unspecified data, it would converge to the second pattern P2, and given a pattern like 10 111001 1 probably converge to P;. This can be seen as similarity 