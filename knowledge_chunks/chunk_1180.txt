red threshold for the residual. It then constructs and returns the greedy policy mV for the value function. As discussed above, the Value Iteration algorithm is similar to the iteration process used for policy evaluation. However, the Value Iteration algorithm achieves much more. It explores the policy space, choosing the action that yields the lowest Q-value at each step, since it does not have the benefit of access to a policy. Figure 17.36 shows the progress of the algorithm for the SSP problem depicted in Table 17.13. As the plot shows, the residual for the state s, has dropped to zero by the 31 iteration. The reader should observe that the value function is close to the value function obtained by solving the linear equations for policy 112g, reproduced below, and also shown in the figure. Our informal discussion had suggested that 772g, the better of the two policies we looked at, is the optimal policy, and that has been borne out by the Value Iteration algorithm. The values V 2 (s,), ..., VP2S (s7) computed by solving the linear equations, Ve2G (38.2075, 24.7908, 45.8741, 45.8741, 25.5211, 53.3027, 53.3027 60. SaSy da SSSA 153, 23667 50 adi 4 HHH MHI S 20 24.76805 10 Residuals S10 4 T a T r 123 45 6 7 8 91011 1213141516 171819 20 21 2223 24 25 2627 2829 3031 Iteration Figure 17.36 The progress of the Value Iteration starting with all states initialized to 0. After 31 iterations, the residual of S; becomes 0. Observe that the Value function approaches the Value function of policy 112g which is the optimal policy. The plot in Figure 17.36 has been obtained by initializing all values of the value function to 0 for the sake of illustration. In practice, one would use more informed initial values. For example, one could use the costs of reaching the goal for deterministic actions. The above plot appears similar to the plot in Figure 17.32, but one must keep in mind that the Value Iteration algorithm has to consider all possible actions at each step, as compared to 