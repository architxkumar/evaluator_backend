sify them. In such a situation, an alternate weight update rule discussed below is more useful. FIGURE 18.21 The Perceptron finds a linear discriminator for the two classes. As long as the training data is linearly separable, the Perceptron can learn to distinguish between the two classes. Here, in a two dimensional space, if 2-92 w;x; 0, shown by the arrow, an element belongs to Class-2. When the training data is linearly separable then the Perceptron training algorithm will find a hyperplane that separates the two classes. The initial weight vector defines some hyperplane in the feature space. In the training phase, every time the algorithm misclassifies a training instance, the hyperplane is shifted in a direction towards the misclassified example. The quantum of the move depends upon the learning rate parameter h. The particular hyperplane that is found would depend upon the order in which the data is presented to the training algorithm and the initial weight vector that is chosen. Consider the two gene expression clusters of Figure 18.15 found by the K-means clustering algorithm. If the data were presented to the Perceptron training algorithm, then it would find one of the many possible lines shown in Figure 18.22, reproduced with class labels shown in the figure. 1 e1 Cc: cj a 3d 0 my FIGURE 18.22 Given the labelled training data from Figure 18.15, the Perceptron training algorithm will find a straight line that separates the two classes or clusters. The actual line found would depend upon the initial set of weights. The Delta Rule for Weight Adjustment If the training examples are not linearly separable then the errorcorrecting rule fails to converge. One can then use an alternative criterion of minimizing the total error in classification. The delta rule can be understood by considering the error in classification in the first stage of the Perceptron, before the Signum function has been applied (Mitchell, 1997). Equation (18.54), which is the linear weighted