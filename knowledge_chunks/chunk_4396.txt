eural nets seem to be able to do is classification. Hard Al problems such as planning, natural language parsing, and theorem proving are not simply classification tasks, so how do connectionist models address these problems? Most of the problems we see in this chapter are indeed classification problems, because these are the problems that neural networks are best suited to handle at present. A major limitation of current network formalisms is how they deal with phenomena that involve time. This limitation is lifted to some degree in work on recurrent networks (see Section 18.4), but the problems are still severe. Hence, we concentrate on classification problems for now. Let s now return to backpropagation networks. The unit in a backpropagation network requires a slightly 1.0 1.0 different activation function from the perceptron. Both functions are shown in Fig. 18.16. A backpropagation 0.5 0.5 unit still sums up its weighted inputs, but unlike the perceptron, it produces a real value between 0 and | as 0 0 output, based on a sigmoid (or S-shaped) function, which Fig. 18.16 The Stepwise Activation Function of is continuous and differentiable, as required by the the Perceptron (left), and the Sigmoid backpropagation algorithm. Let sum be the weighted sum Activation Function of the of the inputs to a unit. The equation for the unit s output Backpropagation Unit (right) is given by: output = l+e Notice that if the sum is 0, the output is 0.5 (in contrast to the perceptron, where it must be either 0 or 1). As the sum gets larger, the output approaches 1. As the sum gets smaller, on the other hand, the output approaches 0. sum Connectionist Models 387 CE EL Like a perceptron, a backpropagation network typically starts out with a random set of weights. The network adjusts its weights each time it sees an input-output pair. Each pair requires two stages: a forward pass and a backward pass. The forward pass involves presenting a sample input to the network and letting activ